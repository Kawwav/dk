2025-05-12 15:48:12,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:48:12,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:48:12,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:48:12,000:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:51:09,318:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:51:09,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:51:09,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:51:09,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 15:51:10,516:INFO:PyCaret ClassificationExperiment
2025-05-12 15:51:10,516:INFO:Logging name: clf-default-name
2025-05-12 15:51:10,516:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-12 15:51:10,516:INFO:version 3.3.2
2025-05-12 15:51:10,516:INFO:Initializing setup()
2025-05-12 15:51:10,516:INFO:self.USI: f163
2025-05-12 15:51:10,516:INFO:self._variable_keys: {'fold_generator', 'fix_imbalance', 'data', 'fold_shuffle_param', 'exp_id', '_ml_usecase', '_available_plots', 'y', 'target_param', 'X_test', 'log_plots_param', 'n_jobs_param', 'y_test', 'fold_groups_param', 'X', 'gpu_param', 'X_train', 'logging_param', 'idx', 'seed', 'memory', 'html_param', 'y_train', 'gpu_n_jobs_param', 'exp_name_log', 'USI', 'pipeline', 'is_multiclass'}
2025-05-12 15:51:10,516:INFO:Checking environment
2025-05-12 15:51:10,516:INFO:python_version: 3.10.16
2025-05-12 15:51:10,516:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-05-12 15:51:10,516:INFO:machine: AMD64
2025-05-12 15:51:10,533:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-12 15:51:10,538:INFO:Memory: svmem(total=17009004544, available=4436070400, percent=73.9, used=12572934144, free=4436070400)
2025-05-12 15:51:10,538:INFO:Physical Core: 6
2025-05-12 15:51:10,538:INFO:Logical Core: 12
2025-05-12 15:51:10,538:INFO:Checking libraries
2025-05-12 15:51:10,538:INFO:System:
2025-05-12 15:51:10,538:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-05-12 15:51:10,538:INFO:executable: C:\Users\Thayse\.conda\envs\mlpipeline\python.exe
2025-05-12 15:51:10,538:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-12 15:51:10,538:INFO:PyCaret required dependencies:
2025-05-12 15:51:10,748:INFO:                 pip: 25.0
2025-05-12 15:51:10,748:INFO:          setuptools: 75.8.0
2025-05-12 15:51:10,748:INFO:             pycaret: 3.3.2
2025-05-12 15:51:10,748:INFO:             IPython: 8.34.0
2025-05-12 15:51:10,748:INFO:          ipywidgets: 8.1.5
2025-05-12 15:51:10,748:INFO:                tqdm: 4.67.1
2025-05-12 15:51:10,748:INFO:               numpy: 1.26.4
2025-05-12 15:51:10,748:INFO:              pandas: 2.1.4
2025-05-12 15:51:10,748:INFO:              jinja2: 3.1.6
2025-05-12 15:51:10,748:INFO:               scipy: 1.11.4
2025-05-12 15:51:10,748:INFO:              joblib: 1.3.2
2025-05-12 15:51:10,748:INFO:             sklearn: 1.4.2
2025-05-12 15:51:10,748:INFO:                pyod: 2.0.4
2025-05-12 15:51:10,748:INFO:            imblearn: 0.13.0
2025-05-12 15:51:10,748:INFO:   category_encoders: 2.7.0
2025-05-12 15:51:10,748:INFO:            lightgbm: 4.6.0
2025-05-12 15:51:10,748:INFO:               numba: 0.61.0
2025-05-12 15:51:10,748:INFO:            requests: 2.32.3
2025-05-12 15:51:10,749:INFO:          matplotlib: 3.7.5
2025-05-12 15:51:10,749:INFO:          scikitplot: 0.3.7
2025-05-12 15:51:10,749:INFO:         yellowbrick: 1.5
2025-05-12 15:51:10,749:INFO:              plotly: 5.24.1
2025-05-12 15:51:10,749:INFO:    plotly-resampler: Not installed
2025-05-12 15:51:10,749:INFO:             kaleido: 0.2.1
2025-05-12 15:51:10,749:INFO:           schemdraw: 0.15
2025-05-12 15:51:10,749:INFO:         statsmodels: 0.14.4
2025-05-12 15:51:10,749:INFO:              sktime: 0.26.0
2025-05-12 15:51:10,749:INFO:               tbats: 1.1.3
2025-05-12 15:51:10,749:INFO:            pmdarima: 2.0.4
2025-05-12 15:51:10,749:INFO:              psutil: 7.0.0
2025-05-12 15:51:10,749:INFO:          markupsafe: 3.0.2
2025-05-12 15:51:10,749:INFO:             pickle5: Not installed
2025-05-12 15:51:10,749:INFO:         cloudpickle: 3.1.1
2025-05-12 15:51:10,749:INFO:         deprecation: 2.1.0
2025-05-12 15:51:10,749:INFO:              xxhash: 3.5.0
2025-05-12 15:51:10,749:INFO:           wurlitzer: Not installed
2025-05-12 15:51:10,749:INFO:PyCaret optional dependencies:
2025-05-12 15:51:11,270:INFO:                shap: Not installed
2025-05-12 15:51:11,270:INFO:           interpret: Not installed
2025-05-12 15:51:11,270:INFO:                umap: Not installed
2025-05-12 15:51:11,270:INFO:     ydata_profiling: 4.16.1
2025-05-12 15:51:11,270:INFO:  explainerdashboard: Not installed
2025-05-12 15:51:11,270:INFO:             autoviz: Not installed
2025-05-12 15:51:11,270:INFO:           fairlearn: Not installed
2025-05-12 15:51:11,270:INFO:          deepchecks: Not installed
2025-05-12 15:51:11,270:INFO:             xgboost: Not installed
2025-05-12 15:51:11,270:INFO:            catboost: Not installed
2025-05-12 15:51:11,270:INFO:              kmodes: Not installed
2025-05-12 15:51:11,270:INFO:             mlxtend: Not installed
2025-05-12 15:51:11,270:INFO:       statsforecast: Not installed
2025-05-12 15:51:11,270:INFO:        tune_sklearn: Not installed
2025-05-12 15:51:11,270:INFO:                 ray: Not installed
2025-05-12 15:51:11,270:INFO:            hyperopt: Not installed
2025-05-12 15:51:11,270:INFO:              optuna: 4.2.1
2025-05-12 15:51:11,270:INFO:               skopt: Not installed
2025-05-12 15:51:11,270:INFO:              mlflow: Not installed
2025-05-12 15:51:11,270:INFO:              gradio: Not installed
2025-05-12 15:51:11,270:INFO:             fastapi: 0.115.12
2025-05-12 15:51:11,270:INFO:             uvicorn: 0.34.2
2025-05-12 15:51:11,270:INFO:              m2cgen: Not installed
2025-05-12 15:51:11,270:INFO:           evidently: Not installed
2025-05-12 15:51:11,270:INFO:               fugue: Not installed
2025-05-12 15:51:11,270:INFO:           streamlit: 1.44.1
2025-05-12 15:51:11,270:INFO:             prophet: 1.1.6
2025-05-12 15:51:11,270:INFO:None
2025-05-12 15:51:11,270:INFO:Set up data.
2025-05-12 15:51:11,282:INFO:Set up folding strategy.
2025-05-12 15:51:11,282:INFO:Set up train/test split.
2025-05-12 15:51:11,282:INFO:Set up index.
2025-05-12 15:51:11,282:INFO:Assigning column types.
2025-05-12 15:51:11,282:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-12 15:51:11,313:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-12 15:51:11,313:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 15:51:11,350:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,350:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,398:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-12 15:51:11,399:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 15:51:11,419:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,419:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,419:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-12 15:51:11,450:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 15:51:11,482:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,482:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,514:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 15:51:11,544:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,545:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,545:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-12 15:51:11,600:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,600:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,667:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,667:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,667:INFO:Preparing preprocessing pipeline...
2025-05-12 15:51:11,667:INFO:Set up simple imputation.
2025-05-12 15:51:11,667:INFO:Set up column name cleaning.
2025-05-12 15:51:11,715:INFO:Finished creating preprocessing pipeline.
2025-05-12 15:51:11,720:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Thayse\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-12 15:51:11,720:INFO:Creating final display dataframe.
2025-05-12 15:51:11,800:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (150, 5)
4        Transformed data shape          (150, 5)
5   Transformed train set shape          (105, 5)
6    Transformed test set shape           (45, 5)
7              Numeric features                 4
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              f163
2025-05-12 15:51:11,874:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,874:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,935:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,935:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:51:11,935:INFO:setup() successfully completed in 1.45s...............
2025-05-12 15:51:11,935:INFO:Initializing compare_models()
2025-05-12 15:51:11,935:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-05-12 15:51:11,935:INFO:Checking exceptions
2025-05-12 15:51:11,951:INFO:Preparing display monitor
2025-05-12 15:51:11,951:INFO:Initializing Logistic Regression
2025-05-12 15:51:11,951:INFO:Total runtime is 0.0 minutes
2025-05-12 15:51:11,951:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:11,951:INFO:Initializing create_model()
2025-05-12 15:51:11,951:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:11,951:INFO:Checking exceptions
2025-05-12 15:51:11,951:INFO:Importing libraries
2025-05-12 15:51:11,951:INFO:Copying training dataset
2025-05-12 15:51:11,951:INFO:Defining folds
2025-05-12 15:51:11,951:INFO:Declaring metric variables
2025-05-12 15:51:11,951:INFO:Importing untrained model
2025-05-12 15:51:11,951:INFO:Logistic Regression Imported successfully
2025-05-12 15:51:11,951:INFO:Starting cross validation
2025-05-12 15:51:11,951:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:19,086:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:19,086:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:19,086:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:19,086:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:19,086:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:19,135:INFO:Calculating mean and std
2025-05-12 15:51:19,143:INFO:Creating metrics dataframe
2025-05-12 15:51:19,157:INFO:Uploading results into container
2025-05-12 15:51:19,159:INFO:Uploading model into container now
2025-05-12 15:51:19,162:INFO:_master_model_container: 1
2025-05-12 15:51:19,162:INFO:_display_container: 2
2025-05-12 15:51:19,164:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 15:51:19,164:INFO:create_model() successfully completed......................................
2025-05-12 15:51:19,380:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:19,381:INFO:Creating metrics dataframe
2025-05-12 15:51:19,384:INFO:Initializing K Neighbors Classifier
2025-05-12 15:51:19,384:INFO:Total runtime is 0.12388087113698323 minutes
2025-05-12 15:51:19,384:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:19,385:INFO:Initializing create_model()
2025-05-12 15:51:19,385:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:19,385:INFO:Checking exceptions
2025-05-12 15:51:19,385:INFO:Importing libraries
2025-05-12 15:51:19,385:INFO:Copying training dataset
2025-05-12 15:51:19,388:INFO:Defining folds
2025-05-12 15:51:19,389:INFO:Declaring metric variables
2025-05-12 15:51:19,389:INFO:Importing untrained model
2025-05-12 15:51:19,389:INFO:K Neighbors Classifier Imported successfully
2025-05-12 15:51:19,389:INFO:Starting cross validation
2025-05-12 15:51:19,390:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:22,183:INFO:Calculating mean and std
2025-05-12 15:51:22,183:INFO:Creating metrics dataframe
2025-05-12 15:51:22,183:INFO:Uploading results into container
2025-05-12 15:51:22,183:INFO:Uploading model into container now
2025-05-12 15:51:22,183:INFO:_master_model_container: 2
2025-05-12 15:51:22,183:INFO:_display_container: 2
2025-05-12 15:51:22,183:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-05-12 15:51:22,183:INFO:create_model() successfully completed......................................
2025-05-12 15:51:22,257:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:22,257:INFO:Creating metrics dataframe
2025-05-12 15:51:22,273:INFO:Initializing Naive Bayes
2025-05-12 15:51:22,273:INFO:Total runtime is 0.17203384637832642 minutes
2025-05-12 15:51:22,273:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:22,273:INFO:Initializing create_model()
2025-05-12 15:51:22,273:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:22,273:INFO:Checking exceptions
2025-05-12 15:51:22,273:INFO:Importing libraries
2025-05-12 15:51:22,273:INFO:Copying training dataset
2025-05-12 15:51:22,273:INFO:Defining folds
2025-05-12 15:51:22,273:INFO:Declaring metric variables
2025-05-12 15:51:22,273:INFO:Importing untrained model
2025-05-12 15:51:22,273:INFO:Naive Bayes Imported successfully
2025-05-12 15:51:22,273:INFO:Starting cross validation
2025-05-12 15:51:22,273:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:22,373:INFO:Calculating mean and std
2025-05-12 15:51:22,375:INFO:Creating metrics dataframe
2025-05-12 15:51:22,378:INFO:Uploading results into container
2025-05-12 15:51:22,378:INFO:Uploading model into container now
2025-05-12 15:51:22,378:INFO:_master_model_container: 3
2025-05-12 15:51:22,378:INFO:_display_container: 2
2025-05-12 15:51:22,378:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-05-12 15:51:22,378:INFO:create_model() successfully completed......................................
2025-05-12 15:51:22,448:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:22,448:INFO:Creating metrics dataframe
2025-05-12 15:51:22,451:INFO:Initializing Decision Tree Classifier
2025-05-12 15:51:22,451:INFO:Total runtime is 0.17499743700027465 minutes
2025-05-12 15:51:22,452:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:22,452:INFO:Initializing create_model()
2025-05-12 15:51:22,452:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:22,452:INFO:Checking exceptions
2025-05-12 15:51:22,452:INFO:Importing libraries
2025-05-12 15:51:22,452:INFO:Copying training dataset
2025-05-12 15:51:22,456:INFO:Defining folds
2025-05-12 15:51:22,456:INFO:Declaring metric variables
2025-05-12 15:51:22,456:INFO:Importing untrained model
2025-05-12 15:51:22,456:INFO:Decision Tree Classifier Imported successfully
2025-05-12 15:51:22,457:INFO:Starting cross validation
2025-05-12 15:51:22,457:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:22,539:INFO:Calculating mean and std
2025-05-12 15:51:22,540:INFO:Creating metrics dataframe
2025-05-12 15:51:22,544:INFO:Uploading results into container
2025-05-12 15:51:22,545:INFO:Uploading model into container now
2025-05-12 15:51:22,545:INFO:_master_model_container: 4
2025-05-12 15:51:22,546:INFO:_display_container: 2
2025-05-12 15:51:22,546:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-05-12 15:51:22,547:INFO:create_model() successfully completed......................................
2025-05-12 15:51:22,614:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:22,614:INFO:Creating metrics dataframe
2025-05-12 15:51:22,617:INFO:Initializing SVM - Linear Kernel
2025-05-12 15:51:22,617:INFO:Total runtime is 0.1777648131052653 minutes
2025-05-12 15:51:22,617:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:22,618:INFO:Initializing create_model()
2025-05-12 15:51:22,618:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:22,618:INFO:Checking exceptions
2025-05-12 15:51:22,618:INFO:Importing libraries
2025-05-12 15:51:22,618:INFO:Copying training dataset
2025-05-12 15:51:22,621:INFO:Defining folds
2025-05-12 15:51:22,621:INFO:Declaring metric variables
2025-05-12 15:51:22,622:INFO:Importing untrained model
2025-05-12 15:51:22,622:INFO:SVM - Linear Kernel Imported successfully
2025-05-12 15:51:22,622:INFO:Starting cross validation
2025-05-12 15:51:22,623:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:22,684:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,686:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,686:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,687:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,703:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,703:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,703:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,705:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,719:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,719:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:22,719:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:22,719:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:22,719:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:22,719:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:22,719:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:22,751:INFO:Calculating mean and std
2025-05-12 15:51:22,751:INFO:Creating metrics dataframe
2025-05-12 15:51:22,751:INFO:Uploading results into container
2025-05-12 15:51:22,751:INFO:Uploading model into container now
2025-05-12 15:51:22,751:INFO:_master_model_container: 5
2025-05-12 15:51:22,751:INFO:_display_container: 2
2025-05-12 15:51:22,751:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-05-12 15:51:22,751:INFO:create_model() successfully completed......................................
2025-05-12 15:51:22,824:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:22,824:INFO:Creating metrics dataframe
2025-05-12 15:51:22,824:INFO:Initializing Ridge Classifier
2025-05-12 15:51:22,824:INFO:Total runtime is 0.18121232589085895 minutes
2025-05-12 15:51:22,824:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:22,824:INFO:Initializing create_model()
2025-05-12 15:51:22,824:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:22,824:INFO:Checking exceptions
2025-05-12 15:51:22,824:INFO:Importing libraries
2025-05-12 15:51:22,824:INFO:Copying training dataset
2025-05-12 15:51:22,824:INFO:Defining folds
2025-05-12 15:51:22,824:INFO:Declaring metric variables
2025-05-12 15:51:22,824:INFO:Importing untrained model
2025-05-12 15:51:22,824:INFO:Ridge Classifier Imported successfully
2025-05-12 15:51:22,840:INFO:Starting cross validation
2025-05-12 15:51:22,840:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:22,888:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,888:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,890:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,890:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,896:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,898:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,900:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,900:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,900:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:22,923:INFO:Calculating mean and std
2025-05-12 15:51:22,923:INFO:Creating metrics dataframe
2025-05-12 15:51:22,923:INFO:Uploading results into container
2025-05-12 15:51:22,923:INFO:Uploading model into container now
2025-05-12 15:51:22,923:INFO:_master_model_container: 6
2025-05-12 15:51:22,923:INFO:_display_container: 2
2025-05-12 15:51:22,923:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-05-12 15:51:22,923:INFO:create_model() successfully completed......................................
2025-05-12 15:51:22,991:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:22,991:INFO:Creating metrics dataframe
2025-05-12 15:51:22,994:INFO:Initializing Random Forest Classifier
2025-05-12 15:51:22,994:INFO:Total runtime is 0.18404722611109414 minutes
2025-05-12 15:51:22,994:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:22,995:INFO:Initializing create_model()
2025-05-12 15:51:22,995:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:22,995:INFO:Checking exceptions
2025-05-12 15:51:22,995:INFO:Importing libraries
2025-05-12 15:51:22,995:INFO:Copying training dataset
2025-05-12 15:51:22,998:INFO:Defining folds
2025-05-12 15:51:22,998:INFO:Declaring metric variables
2025-05-12 15:51:22,999:INFO:Importing untrained model
2025-05-12 15:51:22,999:INFO:Random Forest Classifier Imported successfully
2025-05-12 15:51:22,999:INFO:Starting cross validation
2025-05-12 15:51:23,000:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:23,518:INFO:Calculating mean and std
2025-05-12 15:51:23,518:INFO:Creating metrics dataframe
2025-05-12 15:51:23,518:INFO:Uploading results into container
2025-05-12 15:51:23,518:INFO:Uploading model into container now
2025-05-12 15:51:23,518:INFO:_master_model_container: 7
2025-05-12 15:51:23,518:INFO:_display_container: 2
2025-05-12 15:51:23,518:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-05-12 15:51:23,518:INFO:create_model() successfully completed......................................
2025-05-12 15:51:23,608:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:23,608:INFO:Creating metrics dataframe
2025-05-12 15:51:23,608:INFO:Initializing Quadratic Discriminant Analysis
2025-05-12 15:51:23,608:INFO:Total runtime is 0.19427679379781085 minutes
2025-05-12 15:51:23,608:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:23,624:INFO:Initializing create_model()
2025-05-12 15:51:23,624:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:23,624:INFO:Checking exceptions
2025-05-12 15:51:23,624:INFO:Importing libraries
2025-05-12 15:51:23,624:INFO:Copying training dataset
2025-05-12 15:51:23,624:INFO:Defining folds
2025-05-12 15:51:23,624:INFO:Declaring metric variables
2025-05-12 15:51:23,624:INFO:Importing untrained model
2025-05-12 15:51:23,624:INFO:Quadratic Discriminant Analysis Imported successfully
2025-05-12 15:51:23,624:INFO:Starting cross validation
2025-05-12 15:51:23,624:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:23,672:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,674:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,676:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,679:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,679:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,683:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,684:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,684:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,689:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,692:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:23,723:INFO:Calculating mean and std
2025-05-12 15:51:23,723:INFO:Creating metrics dataframe
2025-05-12 15:51:23,723:INFO:Uploading results into container
2025-05-12 15:51:23,723:INFO:Uploading model into container now
2025-05-12 15:51:23,723:INFO:_master_model_container: 8
2025-05-12 15:51:23,723:INFO:_display_container: 2
2025-05-12 15:51:23,723:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-05-12 15:51:23,723:INFO:create_model() successfully completed......................................
2025-05-12 15:51:23,803:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:23,803:INFO:Creating metrics dataframe
2025-05-12 15:51:23,818:INFO:Initializing Ada Boost Classifier
2025-05-12 15:51:23,818:INFO:Total runtime is 0.19777875343958534 minutes
2025-05-12 15:51:23,818:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:23,818:INFO:Initializing create_model()
2025-05-12 15:51:23,818:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:23,818:INFO:Checking exceptions
2025-05-12 15:51:23,818:INFO:Importing libraries
2025-05-12 15:51:23,818:INFO:Copying training dataset
2025-05-12 15:51:23,822:INFO:Defining folds
2025-05-12 15:51:23,822:INFO:Declaring metric variables
2025-05-12 15:51:23,822:INFO:Importing untrained model
2025-05-12 15:51:23,822:INFO:Ada Boost Classifier Imported successfully
2025-05-12 15:51:23,822:INFO:Starting cross validation
2025-05-12 15:51:23,823:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:23,873:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,874:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,874:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,874:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,874:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,874:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,874:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,874:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:23,878:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 15:51:24,053:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,063:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,067:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,071:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,073:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,075:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,079:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,079:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,081:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,083:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,104:INFO:Calculating mean and std
2025-05-12 15:51:24,104:INFO:Creating metrics dataframe
2025-05-12 15:51:24,104:INFO:Uploading results into container
2025-05-12 15:51:24,104:INFO:Uploading model into container now
2025-05-12 15:51:24,104:INFO:_master_model_container: 9
2025-05-12 15:51:24,104:INFO:_display_container: 2
2025-05-12 15:51:24,104:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-05-12 15:51:24,104:INFO:create_model() successfully completed......................................
2025-05-12 15:51:24,170:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:24,170:INFO:Creating metrics dataframe
2025-05-12 15:51:24,173:INFO:Initializing Gradient Boosting Classifier
2025-05-12 15:51:24,173:INFO:Total runtime is 0.20370457967122393 minutes
2025-05-12 15:51:24,173:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:24,173:INFO:Initializing create_model()
2025-05-12 15:51:24,173:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:24,173:INFO:Checking exceptions
2025-05-12 15:51:24,173:INFO:Importing libraries
2025-05-12 15:51:24,173:INFO:Copying training dataset
2025-05-12 15:51:24,174:INFO:Defining folds
2025-05-12 15:51:24,174:INFO:Declaring metric variables
2025-05-12 15:51:24,174:INFO:Importing untrained model
2025-05-12 15:51:24,174:INFO:Gradient Boosting Classifier Imported successfully
2025-05-12 15:51:24,174:INFO:Starting cross validation
2025-05-12 15:51:24,174:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:24,620:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,700:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,700:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,732:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,748:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,748:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,748:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,748:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,764:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,764:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,796:INFO:Calculating mean and std
2025-05-12 15:51:24,796:INFO:Creating metrics dataframe
2025-05-12 15:51:24,796:INFO:Uploading results into container
2025-05-12 15:51:24,796:INFO:Uploading model into container now
2025-05-12 15:51:24,796:INFO:_master_model_container: 10
2025-05-12 15:51:24,796:INFO:_display_container: 2
2025-05-12 15:51:24,796:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-05-12 15:51:24,796:INFO:create_model() successfully completed......................................
2025-05-12 15:51:24,891:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:24,891:INFO:Creating metrics dataframe
2025-05-12 15:51:24,895:INFO:Initializing Linear Discriminant Analysis
2025-05-12 15:51:24,895:INFO:Total runtime is 0.2157281716664632 minutes
2025-05-12 15:51:24,895:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:24,895:INFO:Initializing create_model()
2025-05-12 15:51:24,895:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:24,895:INFO:Checking exceptions
2025-05-12 15:51:24,895:INFO:Importing libraries
2025-05-12 15:51:24,896:INFO:Copying training dataset
2025-05-12 15:51:24,899:INFO:Defining folds
2025-05-12 15:51:24,899:INFO:Declaring metric variables
2025-05-12 15:51:24,900:INFO:Importing untrained model
2025-05-12 15:51:24,900:INFO:Linear Discriminant Analysis Imported successfully
2025-05-12 15:51:24,900:INFO:Starting cross validation
2025-05-12 15:51:24,901:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:24,942:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,948:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,952:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,952:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,954:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,956:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,957:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,958:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,961:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,964:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 15:51:24,990:INFO:Calculating mean and std
2025-05-12 15:51:24,992:INFO:Creating metrics dataframe
2025-05-12 15:51:24,992:INFO:Uploading results into container
2025-05-12 15:51:24,992:INFO:Uploading model into container now
2025-05-12 15:51:24,992:INFO:_master_model_container: 11
2025-05-12 15:51:24,992:INFO:_display_container: 2
2025-05-12 15:51:24,992:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-05-12 15:51:24,992:INFO:create_model() successfully completed......................................
2025-05-12 15:51:25,075:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:25,075:INFO:Creating metrics dataframe
2025-05-12 15:51:25,075:INFO:Initializing Extra Trees Classifier
2025-05-12 15:51:25,075:INFO:Total runtime is 0.21873811483383176 minutes
2025-05-12 15:51:25,075:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:25,075:INFO:Initializing create_model()
2025-05-12 15:51:25,075:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:25,075:INFO:Checking exceptions
2025-05-12 15:51:25,075:INFO:Importing libraries
2025-05-12 15:51:25,075:INFO:Copying training dataset
2025-05-12 15:51:25,075:INFO:Defining folds
2025-05-12 15:51:25,075:INFO:Declaring metric variables
2025-05-12 15:51:25,075:INFO:Importing untrained model
2025-05-12 15:51:25,075:INFO:Extra Trees Classifier Imported successfully
2025-05-12 15:51:25,075:INFO:Starting cross validation
2025-05-12 15:51:25,075:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:25,525:INFO:Calculating mean and std
2025-05-12 15:51:25,525:INFO:Creating metrics dataframe
2025-05-12 15:51:25,525:INFO:Uploading results into container
2025-05-12 15:51:25,525:INFO:Uploading model into container now
2025-05-12 15:51:25,525:INFO:_master_model_container: 12
2025-05-12 15:51:25,525:INFO:_display_container: 2
2025-05-12 15:51:25,525:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-05-12 15:51:25,525:INFO:create_model() successfully completed......................................
2025-05-12 15:51:25,625:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:25,625:INFO:Creating metrics dataframe
2025-05-12 15:51:25,625:INFO:Initializing Light Gradient Boosting Machine
2025-05-12 15:51:25,625:INFO:Total runtime is 0.22790206273396807 minutes
2025-05-12 15:51:25,625:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:25,625:INFO:Initializing create_model()
2025-05-12 15:51:25,625:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:25,625:INFO:Checking exceptions
2025-05-12 15:51:25,625:INFO:Importing libraries
2025-05-12 15:51:25,625:INFO:Copying training dataset
2025-05-12 15:51:25,641:INFO:Defining folds
2025-05-12 15:51:25,641:INFO:Declaring metric variables
2025-05-12 15:51:25,641:INFO:Importing untrained model
2025-05-12 15:51:25,641:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-12 15:51:25,641:INFO:Starting cross validation
2025-05-12 15:51:25,641:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:27,475:INFO:Calculating mean and std
2025-05-12 15:51:27,477:INFO:Creating metrics dataframe
2025-05-12 15:51:27,481:INFO:Uploading results into container
2025-05-12 15:51:27,482:INFO:Uploading model into container now
2025-05-12 15:51:27,483:INFO:_master_model_container: 13
2025-05-12 15:51:27,483:INFO:_display_container: 2
2025-05-12 15:51:27,485:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-12 15:51:27,485:INFO:create_model() successfully completed......................................
2025-05-12 15:51:27,558:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:27,558:INFO:Creating metrics dataframe
2025-05-12 15:51:27,558:INFO:Initializing Dummy Classifier
2025-05-12 15:51:27,558:INFO:Total runtime is 0.260117773214976 minutes
2025-05-12 15:51:27,558:INFO:SubProcess create_model() called ==================================
2025-05-12 15:51:27,558:INFO:Initializing create_model()
2025-05-12 15:51:27,558:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000024D200BCCA0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:27,558:INFO:Checking exceptions
2025-05-12 15:51:27,558:INFO:Importing libraries
2025-05-12 15:51:27,558:INFO:Copying training dataset
2025-05-12 15:51:27,574:INFO:Defining folds
2025-05-12 15:51:27,574:INFO:Declaring metric variables
2025-05-12 15:51:27,574:INFO:Importing untrained model
2025-05-12 15:51:27,574:INFO:Dummy Classifier Imported successfully
2025-05-12 15:51:27,574:INFO:Starting cross validation
2025-05-12 15:51:27,574:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 15:51:27,622:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,625:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 15:51:27,657:INFO:Calculating mean and std
2025-05-12 15:51:27,657:INFO:Creating metrics dataframe
2025-05-12 15:51:27,657:INFO:Uploading results into container
2025-05-12 15:51:27,657:INFO:Uploading model into container now
2025-05-12 15:51:27,657:INFO:_master_model_container: 14
2025-05-12 15:51:27,657:INFO:_display_container: 2
2025-05-12 15:51:27,657:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-05-12 15:51:27,657:INFO:create_model() successfully completed......................................
2025-05-12 15:51:27,758:INFO:SubProcess create_model() end ==================================
2025-05-12 15:51:27,758:INFO:Creating metrics dataframe
2025-05-12 15:51:27,758:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-05-12 15:51:27,758:INFO:Initializing create_model()
2025-05-12 15:51:27,758:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000024D1E726E60>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:51:27,758:INFO:Checking exceptions
2025-05-12 15:51:27,758:INFO:Importing libraries
2025-05-12 15:51:27,758:INFO:Copying training dataset
2025-05-12 15:51:27,758:INFO:Defining folds
2025-05-12 15:51:27,758:INFO:Declaring metric variables
2025-05-12 15:51:27,758:INFO:Importing untrained model
2025-05-12 15:51:27,758:INFO:Declaring custom model
2025-05-12 15:51:27,758:INFO:Logistic Regression Imported successfully
2025-05-12 15:51:27,758:INFO:Cross validation set to False
2025-05-12 15:51:27,758:INFO:Fitting Model
2025-05-12 15:51:27,780:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 15:51:27,780:INFO:create_model() successfully completed......................................
2025-05-12 15:51:27,859:INFO:_master_model_container: 14
2025-05-12 15:51:27,859:INFO:_display_container: 2
2025-05-12 15:51:27,859:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 15:51:27,859:INFO:compare_models() successfully completed......................................
2025-05-12 15:51:27,875:INFO:Initializing save_model()
2025-05-12 15:51:27,875:INFO:save_model(model=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), model_name=iris_classifier, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\Thayse\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-12 15:51:27,875:INFO:Adding model into prep_pipe
2025-05-12 15:51:27,883:INFO:iris_classifier.pkl saved in current working directory
2025-05-12 15:51:27,883:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('trained_model',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-05-12 15:51:27,883:INFO:save_model() successfully completed......................................
2025-05-12 15:55:44,796:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:49,465:INFO:PyCaret TSForecastingExperiment
2025-05-12 15:55:49,465:INFO:Logging name: ts-default-name
2025-05-12 15:55:49,465:INFO:ML Usecase: MLUsecase.TIME_SERIES
2025-05-12 15:55:49,465:INFO:version 3.3.2
2025-05-12 15:55:49,465:INFO:Initializing setup()
2025-05-12 15:55:49,465:INFO:self.USI: f525
2025-05-12 15:55:49,465:INFO:self._variable_keys: {'seed', 'enforce_exogenous', 'USI', 'approach_type', 'X_transformed', 'significant_sps_no_harmonics', 'fold_param', 'fold_generator', 'X_test_transformed', 'all_sps_to_use', 'strictly_positive', 'y', '_available_plots', 'gpu_n_jobs_param', 'X', 'candidate_sps', 'n_jobs_param', 'idx', 'exogenous_present', 'exp_id', 'y_train_transformed', 'y_train', 'fh', 'enforce_pi', 'y_transformed', 'primary_sp_to_use', 'X_test', 'data', 'pipeline', '_ml_usecase', 'log_plots_param', 'html_param', 'gpu_param', 'memory', 'index_type', 'X_train_transformed', 'logging_param', 'model_engines', 'seasonality_present', 'y_test', 'y_test_transformed', 'X_train', 'significant_sps', 'exp_name_log'}
2025-05-12 15:55:49,465:INFO:Checking environment
2025-05-12 15:55:49,465:INFO:python_version: 3.10.16
2025-05-12 15:55:49,465:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-05-12 15:55:49,465:INFO:machine: AMD64
2025-05-12 15:55:49,480:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-12 15:55:49,480:INFO:Memory: svmem(total=17009004544, available=3990896640, percent=76.5, used=13018107904, free=3990896640)
2025-05-12 15:55:49,480:INFO:Physical Core: 6
2025-05-12 15:55:49,480:INFO:Logical Core: 12
2025-05-12 15:55:49,480:INFO:Checking libraries
2025-05-12 15:55:49,480:INFO:System:
2025-05-12 15:55:49,480:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-05-12 15:55:49,480:INFO:executable: C:\Users\Thayse\.conda\envs\mlpipeline\python.exe
2025-05-12 15:55:49,480:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-12 15:55:49,480:INFO:PyCaret required dependencies:
2025-05-12 15:55:49,572:INFO:                 pip: 25.0
2025-05-12 15:55:49,572:INFO:          setuptools: 75.8.0
2025-05-12 15:55:49,572:INFO:             pycaret: 3.3.2
2025-05-12 15:55:49,572:INFO:             IPython: 8.34.0
2025-05-12 15:55:49,572:INFO:          ipywidgets: 8.1.5
2025-05-12 15:55:49,572:INFO:                tqdm: 4.67.1
2025-05-12 15:55:49,572:INFO:               numpy: 1.26.4
2025-05-12 15:55:49,572:INFO:              pandas: 2.1.4
2025-05-12 15:55:49,574:INFO:              jinja2: 3.1.6
2025-05-12 15:55:49,574:INFO:               scipy: 1.11.4
2025-05-12 15:55:49,574:INFO:              joblib: 1.3.2
2025-05-12 15:55:49,574:INFO:             sklearn: 1.4.2
2025-05-12 15:55:49,574:INFO:                pyod: 2.0.4
2025-05-12 15:55:49,574:INFO:            imblearn: 0.13.0
2025-05-12 15:55:49,574:INFO:   category_encoders: 2.7.0
2025-05-12 15:55:49,574:INFO:            lightgbm: 4.6.0
2025-05-12 15:55:49,574:INFO:               numba: 0.61.0
2025-05-12 15:55:49,574:INFO:            requests: 2.32.3
2025-05-12 15:55:49,574:INFO:          matplotlib: 3.7.5
2025-05-12 15:55:49,574:INFO:          scikitplot: 0.3.7
2025-05-12 15:55:49,574:INFO:         yellowbrick: 1.5
2025-05-12 15:55:49,574:INFO:              plotly: 5.24.1
2025-05-12 15:55:49,574:INFO:    plotly-resampler: Not installed
2025-05-12 15:55:49,574:INFO:             kaleido: 0.2.1
2025-05-12 15:55:49,574:INFO:           schemdraw: 0.15
2025-05-12 15:55:49,574:INFO:         statsmodels: 0.14.4
2025-05-12 15:55:49,574:INFO:              sktime: 0.26.0
2025-05-12 15:55:49,574:INFO:               tbats: 1.1.3
2025-05-12 15:55:49,574:INFO:            pmdarima: 2.0.4
2025-05-12 15:55:49,574:INFO:              psutil: 7.0.0
2025-05-12 15:55:49,574:INFO:          markupsafe: 3.0.2
2025-05-12 15:55:49,574:INFO:             pickle5: Not installed
2025-05-12 15:55:49,574:INFO:         cloudpickle: 3.1.1
2025-05-12 15:55:49,574:INFO:         deprecation: 2.1.0
2025-05-12 15:55:49,574:INFO:              xxhash: 3.5.0
2025-05-12 15:55:49,574:INFO:           wurlitzer: Not installed
2025-05-12 15:55:49,574:INFO:PyCaret optional dependencies:
2025-05-12 15:55:49,996:INFO:                shap: Not installed
2025-05-12 15:55:49,996:INFO:           interpret: Not installed
2025-05-12 15:55:49,996:INFO:                umap: Not installed
2025-05-12 15:55:49,996:INFO:     ydata_profiling: 4.16.1
2025-05-12 15:55:49,996:INFO:  explainerdashboard: Not installed
2025-05-12 15:55:49,996:INFO:             autoviz: Not installed
2025-05-12 15:55:49,996:INFO:           fairlearn: Not installed
2025-05-12 15:55:49,996:INFO:          deepchecks: Not installed
2025-05-12 15:55:49,996:INFO:             xgboost: Not installed
2025-05-12 15:55:49,996:INFO:            catboost: Not installed
2025-05-12 15:55:49,996:INFO:              kmodes: Not installed
2025-05-12 15:55:49,996:INFO:             mlxtend: Not installed
2025-05-12 15:55:49,996:INFO:       statsforecast: Not installed
2025-05-12 15:55:49,996:INFO:        tune_sklearn: Not installed
2025-05-12 15:55:49,996:INFO:                 ray: Not installed
2025-05-12 15:55:49,996:INFO:            hyperopt: Not installed
2025-05-12 15:55:49,996:INFO:              optuna: 4.2.1
2025-05-12 15:55:49,996:INFO:               skopt: Not installed
2025-05-12 15:55:49,996:INFO:              mlflow: Not installed
2025-05-12 15:55:49,997:INFO:              gradio: Not installed
2025-05-12 15:55:49,997:INFO:             fastapi: 0.115.12
2025-05-12 15:55:49,997:INFO:             uvicorn: 0.34.2
2025-05-12 15:55:49,997:INFO:              m2cgen: Not installed
2025-05-12 15:55:49,997:INFO:           evidently: Not installed
2025-05-12 15:55:49,997:INFO:               fugue: Not installed
2025-05-12 15:55:49,997:INFO:           streamlit: 1.44.1
2025-05-12 15:55:49,997:INFO:             prophet: 1.1.6
2025-05-12 15:55:49,997:INFO:None
2025-05-12 15:55:49,997:INFO:Set Forecast Horizon.
2025-05-12 15:55:49,998:INFO:Set up Train-Test Splits.
2025-05-12 15:55:50,080:INFO:Finished creating preprocessing pipeline.
2025-05-12 15:55:50,085:INFO:Pipeline: ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                DummyForecaster())]))])
2025-05-12 15:55:50,085:INFO:Set up Seasonal Period.
2025-05-12 15:55:50,095:INFO:Setting the seasonal component type - 'add' or 'mul'.
2025-05-12 15:55:50,095:INFO:Checking if data is strictly positive.
2025-05-12 15:55:50,111:INFO:Creating final display dataframe.
2025-05-12 15:55:50,125:INFO:Setup Display Container:                                          Description                    Value
0                                         session_id                      123
1                                             Target                    Close
2                                           Approach               Univariate
3                                Exogenous Variables                  Present
4                                Original data shape                 (501, 2)
5                             Transformed data shape                 (501, 2)
6                        Transformed train set shape                 (500, 2)
7                         Transformed test set shape                   (1, 2)
8                           Rows with missing values                     0.0%
9                                     Fold Generator  ExpandingWindowSplitter
10                                       Fold Number                        3
11                       Enforce Prediction Interval                    False
12                   Splits used for hyperparameters                      all
13                   User Defined Seasonal Period(s)                     None
14                           Ignore Seasonality Test                    False
15                        Seasonality Detection Algo                     auto
16                            Max Period to Consider                       60
17                         Seasonal Period(s) Tested                       []
18                    Significant Seasonal Period(s)                      [1]
19  Significant Seasonal Period(s) without Harmonics                      [1]
20                                  Remove Harmonics                    False
21                            Harmonics Order Method             harmonic_max
22                          Num Seasonalities to Use                        1
23                          All Seasonalities to Use                      [1]
24                               Primary Seasonality                        1
25                               Seasonality Present                    False
26                                  Seasonality Type                     None
27                          Target Strictly Positive                     True
28                                Target White Noise                       No
29                                     Recommended d                        1
30                            Recommended Seasonal D                        0
31                                        Preprocess                    False
32                                          CPU Jobs                       -1
33                                           Use GPU                    False
34                                    Log Experiment                    False
35                                   Experiment Name          ts-default-name
36                                               USI                     f525
2025-05-12 15:55:50,125:INFO:Engine successfully changes for model 'auto_arima' to 'pmdarima'.
2025-05-12 15:55:50,190:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,282:INFO:Engine for model 'lr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,282:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,282:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,282:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,282:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,282:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,282:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,287:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,289:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,289:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,289:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,289:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,289:INFO:Engine for model 'lr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,289:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,305:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,305:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,305:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,305:INFO:Engine successfully changes for model 'lr_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,305:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,305:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,305:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,321:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,321:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,322:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,322:INFO:Engine successfully changes for model 'en_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,339:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,339:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,339:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,339:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,339:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,339:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,355:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,355:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,355:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,355:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,355:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,355:INFO:Engine successfully changes for model 'ridge_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,355:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,355:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,371:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,371:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,371:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,372:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,372:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,372:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,372:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,372:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,372:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,389:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,389:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,389:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,389:INFO:Engine successfully changes for model 'lasso_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,389:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,389:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,389:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,405:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,405:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,405:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,405:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,405:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,405:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,421:INFO:Engine successfully changes for model 'lar_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,422:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,422:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,422:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,422:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,422:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,437:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,438:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,438:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,438:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,438:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,438:INFO:Engine successfully changes for model 'llar_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,438:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,438:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,438:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,455:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,455:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,455:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,455:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,455:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,455:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,471:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,472:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,472:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,472:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,472:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,476:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,476:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,476:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,476:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,476:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,476:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,476:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,476:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,476:INFO:Engine successfully changes for model 'br_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,476:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,476:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,476:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,488:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,490:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,490:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,490:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,490:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,490:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,490:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,505:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,505:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,506:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,506:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,506:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,506:INFO:Engine successfully changes for model 'huber_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,506:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,506:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,506:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,506:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,506:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,506:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,522:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,522:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,522:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,522:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,522:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,522:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,522:INFO:Engine successfully changes for model 'par_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,522:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,539:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,539:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,539:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,539:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,555:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,557:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,557:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,557:INFO:Engine successfully changes for model 'omp_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,557:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,571:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,571:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,572:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,573:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,573:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,573:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,573:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,588:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,588:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,589:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,589:INFO:Engine successfully changes for model 'knn_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,591:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,601:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,601:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,601:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,601:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,601:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,606:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,606:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,606:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,606:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,606:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,606:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,606:INFO:Engine successfully changes for model 'dt_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,622:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,622:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,622:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,622:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,622:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,622:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,622:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,622:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,622:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,622:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,622:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,622:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,622:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,639:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,639:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,639:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,639:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,639:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,639:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,639:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,639:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,639:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,639:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,639:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,639:INFO:Engine successfully changes for model 'rf_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,639:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,655:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,655:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,655:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,655:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,655:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,655:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,655:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,655:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,655:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,655:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,655:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,671:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,671:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,672:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,674:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,674:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,674:INFO:Engine successfully changes for model 'et_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,674:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,674:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,674:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,674:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,674:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,689:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,689:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,689:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,689:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,689:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,689:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,689:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,689:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,689:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,689:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,689:INFO:Engine successfully changes for model 'gbr_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,705:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,707:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,707:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,707:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,707:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,707:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,707:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,707:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,707:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,707:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,722:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,722:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,722:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,722:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,722:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,722:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,722:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,722:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,722:INFO:Engine successfully changes for model 'ada_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,722:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,739:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,739:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,739:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,739:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,739:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,739:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,739:INFO:Engine successfully changes for model 'xgboost_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,756:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,756:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,756:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,756:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,756:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,756:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,756:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,772:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,772:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,772:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,772:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,772:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,772:INFO:Engine successfully changes for model 'lightgbm_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,772:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,789:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,789:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,789:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,789:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,789:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,789:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,789:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,789:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,804:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:55:50,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,804:INFO:Engine successfully changes for model 'catboost_cds_dt' to 'sklearn'.
2025-05-12 15:55:50,805:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,805:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,805:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,805:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,805:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,805:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,822:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,822:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,822:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,822:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,822:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,839:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,839:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,839:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,839:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,839:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:55:50,855:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,855:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,855:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,855:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:55:50,855:INFO:setup() successfully completed in 1.4s...............
2025-05-12 15:55:50,855:INFO:Initializing compare_models()
2025-05-12 15:55:50,855:INFO:compare_models(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, include=None, fold=None, round=4, cross_validation=True, sort=MAE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.time_series.forecasting.oop.TSForecastingExperiment'>}, exclude=None)
2025-05-12 15:55:50,855:INFO:Checking exceptions
2025-05-12 15:55:50,855:INFO:Preparing display monitor
2025-05-12 15:55:50,855:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:713: UserWarning: Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library
  warnings.warn(

2025-05-12 15:55:50,855:INFO:Initializing ARIMA
2025-05-12 15:55:50,855:INFO:Total runtime is 0.0 minutes
2025-05-12 15:55:50,855:INFO:SubProcess create_model() called ==================================
2025-05-12 15:55:50,855:INFO:Initializing create_model()
2025-05-12 15:55:50,855:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:55:50,855:INFO:Checking exceptions
2025-05-12 15:55:50,855:INFO:Importing libraries
2025-05-12 15:55:50,855:INFO:Copying training dataset
2025-05-12 15:55:50,855:INFO:Defining folds
2025-05-12 15:55:50,855:INFO:Declaring metric variables
2025-05-12 15:55:50,855:INFO:Importing untrained model
2025-05-12 15:55:50,855:INFO:ARIMA Imported successfully
2025-05-12 15:55:50,855:INFO:Starting cross validation
2025-05-12 15:55:50,855:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:55:55,006:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:55:55,006:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:55:55,022:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:55:55,022:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:55:55,022:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:55:55,022:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:55:55,037:INFO:Calculating mean and std
2025-05-12 15:55:55,037:INFO:Creating metrics dataframe
2025-05-12 15:55:55,043:INFO:Uploading results into container
2025-05-12 15:55:55,043:INFO:Uploading model into container now
2025-05-12 15:55:55,043:INFO:_master_model_container: 1
2025-05-12 15:55:55,043:INFO:_display_container: 2
2025-05-12 15:55:55,045:INFO:ARIMA()
2025-05-12 15:55:55,045:INFO:create_model() successfully completed......................................
2025-05-12 15:55:55,156:WARNING:create_model() for ARIMA() raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:55:55,156:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:55:55,156:INFO:Initializing create_model()
2025-05-12 15:55:55,156:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:55:55,156:INFO:Checking exceptions
2025-05-12 15:55:55,156:INFO:Importing libraries
2025-05-12 15:55:55,156:INFO:Copying training dataset
2025-05-12 15:55:55,156:INFO:Defining folds
2025-05-12 15:55:55,156:INFO:Declaring metric variables
2025-05-12 15:55:55,156:INFO:Importing untrained model
2025-05-12 15:55:55,156:INFO:ARIMA Imported successfully
2025-05-12 15:55:55,156:INFO:Starting cross validation
2025-05-12 15:55:55,156:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:55:58,191:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:55:58,191:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:55:58,191:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:55:58,191:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:55:58,191:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:55:58,191:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:55:58,205:INFO:Calculating mean and std
2025-05-12 15:55:58,205:INFO:Creating metrics dataframe
2025-05-12 15:55:58,205:INFO:Uploading results into container
2025-05-12 15:55:58,205:INFO:Uploading model into container now
2025-05-12 15:55:58,205:INFO:_master_model_container: 2
2025-05-12 15:55:58,205:INFO:_display_container: 2
2025-05-12 15:55:58,205:INFO:ARIMA()
2025-05-12 15:55:58,205:INFO:create_model() successfully completed......................................
2025-05-12 15:55:58,323:ERROR:create_model() for ARIMA() raised an exception or returned all 0.0:
2025-05-12 15:55:58,323:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:55:58,323:INFO:Initializing Auto ARIMA
2025-05-12 15:55:58,323:INFO:Total runtime is 0.12445165713628133 minutes
2025-05-12 15:55:58,323:INFO:SubProcess create_model() called ==================================
2025-05-12 15:55:58,323:INFO:Initializing create_model()
2025-05-12 15:55:58,323:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=auto_arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:55:58,323:INFO:Checking exceptions
2025-05-12 15:55:58,323:INFO:Importing libraries
2025-05-12 15:55:58,323:INFO:Copying training dataset
2025-05-12 15:55:58,323:INFO:Defining folds
2025-05-12 15:55:58,323:INFO:Declaring metric variables
2025-05-12 15:55:58,323:INFO:Importing untrained model
2025-05-12 15:55:58,323:INFO:Auto ARIMA Imported successfully
2025-05-12 15:55:58,338:INFO:Starting cross validation
2025-05-12 15:55:58,338:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:01,627:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:56:01,627:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:56:01,627:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:56:01,627:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:56:01,627:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:56:01,627:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:56:01,643:INFO:Calculating mean and std
2025-05-12 15:56:01,643:INFO:Creating metrics dataframe
2025-05-12 15:56:01,643:INFO:Uploading results into container
2025-05-12 15:56:01,643:INFO:Uploading model into container now
2025-05-12 15:56:01,643:INFO:_master_model_container: 3
2025-05-12 15:56:01,643:INFO:_display_container: 2
2025-05-12 15:56:01,643:INFO:AutoARIMA(random_state=123, suppress_warnings=True)
2025-05-12 15:56:01,643:INFO:create_model() successfully completed......................................
2025-05-12 15:56:01,757:WARNING:create_model() for AutoARIMA(random_state=123, suppress_warnings=True) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:01,757:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:01,757:INFO:Initializing create_model()
2025-05-12 15:56:01,757:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=auto_arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:01,757:INFO:Checking exceptions
2025-05-12 15:56:01,757:INFO:Importing libraries
2025-05-12 15:56:01,757:INFO:Copying training dataset
2025-05-12 15:56:01,769:INFO:Defining folds
2025-05-12 15:56:01,769:INFO:Declaring metric variables
2025-05-12 15:56:01,769:INFO:Importing untrained model
2025-05-12 15:56:01,773:INFO:Auto ARIMA Imported successfully
2025-05-12 15:56:01,773:INFO:Starting cross validation
2025-05-12 15:56:01,773:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:05,028:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:56:05,028:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:56:05,028:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:56:05,028:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:56:05,028:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:56:05,028:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:56:05,042:INFO:Calculating mean and std
2025-05-12 15:56:05,043:INFO:Creating metrics dataframe
2025-05-12 15:56:05,044:INFO:Uploading results into container
2025-05-12 15:56:05,044:INFO:Uploading model into container now
2025-05-12 15:56:05,044:INFO:_master_model_container: 4
2025-05-12 15:56:05,044:INFO:_display_container: 2
2025-05-12 15:56:05,044:INFO:AutoARIMA(random_state=123, suppress_warnings=True)
2025-05-12 15:56:05,044:INFO:create_model() successfully completed......................................
2025-05-12 15:56:05,161:ERROR:create_model() for AutoARIMA(random_state=123, suppress_warnings=True) raised an exception or returned all 0.0:
2025-05-12 15:56:05,161:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:05,161:INFO:Initializing Croston
2025-05-12 15:56:05,161:INFO:Total runtime is 0.23842465082804362 minutes
2025-05-12 15:56:05,161:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:05,161:INFO:Initializing create_model()
2025-05-12 15:56:05,161:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=croston, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:05,161:INFO:Checking exceptions
2025-05-12 15:56:05,161:INFO:Importing libraries
2025-05-12 15:56:05,161:INFO:Copying training dataset
2025-05-12 15:56:05,176:INFO:Defining folds
2025-05-12 15:56:05,177:INFO:Declaring metric variables
2025-05-12 15:56:05,177:INFO:Importing untrained model
2025-05-12 15:56:05,177:INFO:Croston Imported successfully
2025-05-12 15:56:05,179:INFO:Starting cross validation
2025-05-12 15:56:05,180:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:05,258:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:56:05,258:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:56:05,258:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:56:05,259:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:56:05,259:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:56:05,259:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:56:05,276:INFO:Calculating mean and std
2025-05-12 15:56:05,276:INFO:Creating metrics dataframe
2025-05-12 15:56:05,276:INFO:Uploading results into container
2025-05-12 15:56:05,276:INFO:Uploading model into container now
2025-05-12 15:56:05,276:INFO:_master_model_container: 5
2025-05-12 15:56:05,276:INFO:_display_container: 2
2025-05-12 15:56:05,276:INFO:Croston()
2025-05-12 15:56:05,276:INFO:create_model() successfully completed......................................
2025-05-12 15:56:05,360:INFO:SubProcess create_model() end ==================================
2025-05-12 15:56:05,360:INFO:Creating metrics dataframe
2025-05-12 15:56:05,360:INFO:Initializing Linear w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:05,360:INFO:Total runtime is 0.2417484919230143 minutes
2025-05-12 15:56:05,360:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:05,360:INFO:Initializing create_model()
2025-05-12 15:56:05,360:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=lr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:05,360:INFO:Checking exceptions
2025-05-12 15:56:05,360:INFO:Importing libraries
2025-05-12 15:56:05,360:INFO:Copying training dataset
2025-05-12 15:56:05,377:INFO:Defining folds
2025-05-12 15:56:05,377:INFO:Declaring metric variables
2025-05-12 15:56:05,377:INFO:Importing untrained model
2025-05-12 15:56:05,377:INFO:Linear w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:05,379:INFO:Starting cross validation
2025-05-12 15:56:05,381:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:06,130:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:06,130:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:06,130:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:06,211:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:06,216:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:06,217:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:06,217:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:06,217:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:06,217:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:06,228:INFO:Calculating mean and std
2025-05-12 15:56:06,230:INFO:Creating metrics dataframe
2025-05-12 15:56:06,236:INFO:Uploading results into container
2025-05-12 15:56:06,236:INFO:Uploading model into container now
2025-05-12 15:56:06,237:INFO:_master_model_container: 6
2025-05-12 15:56:06,237:INFO:_display_container: 2
2025-05-12 15:56:06,242:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1)
2025-05-12 15:56:06,242:INFO:create_model() successfully completed......................................
2025-05-12 15:56:06,328:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:06,329:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:06,329:INFO:Initializing create_model()
2025-05-12 15:56:06,329:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=lr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:06,329:INFO:Checking exceptions
2025-05-12 15:56:06,329:INFO:Importing libraries
2025-05-12 15:56:06,329:INFO:Copying training dataset
2025-05-12 15:56:06,332:INFO:Defining folds
2025-05-12 15:56:06,332:INFO:Declaring metric variables
2025-05-12 15:56:06,332:INFO:Importing untrained model
2025-05-12 15:56:06,334:INFO:Linear w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:06,335:INFO:Starting cross validation
2025-05-12 15:56:06,336:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:07,055:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:07,055:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:07,055:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:07,134:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:07,134:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:07,134:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:07,134:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:07,134:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:07,134:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:07,150:INFO:Calculating mean and std
2025-05-12 15:56:07,150:INFO:Creating metrics dataframe
2025-05-12 15:56:07,150:INFO:Uploading results into container
2025-05-12 15:56:07,150:INFO:Uploading model into container now
2025-05-12 15:56:07,150:INFO:_master_model_container: 7
2025-05-12 15:56:07,150:INFO:_display_container: 2
2025-05-12 15:56:07,150:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1)
2025-05-12 15:56:07,150:INFO:create_model() successfully completed......................................
2025-05-12 15:56:07,238:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:07,238:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:07,238:INFO:Initializing Elastic Net w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:07,238:INFO:Total runtime is 0.273050590356191 minutes
2025-05-12 15:56:07,238:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:07,238:INFO:Initializing create_model()
2025-05-12 15:56:07,238:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=en_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:07,238:INFO:Checking exceptions
2025-05-12 15:56:07,238:INFO:Importing libraries
2025-05-12 15:56:07,238:INFO:Copying training dataset
2025-05-12 15:56:07,240:INFO:Defining folds
2025-05-12 15:56:07,240:INFO:Declaring metric variables
2025-05-12 15:56:07,240:INFO:Importing untrained model
2025-05-12 15:56:07,241:INFO:Elastic Net w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:07,243:INFO:Starting cross validation
2025-05-12 15:56:07,244:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:08,044:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:08,048:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:08,051:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:08,136:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:08,136:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:08,139:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:08,139:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:08,140:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:08,140:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:08,151:INFO:Calculating mean and std
2025-05-12 15:56:08,151:INFO:Creating metrics dataframe
2025-05-12 15:56:08,154:INFO:Uploading results into container
2025-05-12 15:56:08,154:INFO:Uploading model into container now
2025-05-12 15:56:08,154:INFO:_master_model_container: 8
2025-05-12 15:56:08,155:INFO:_display_container: 2
2025-05-12 15:56:08,156:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1)
2025-05-12 15:56:08,156:INFO:create_model() successfully completed......................................
2025-05-12 15:56:08,239:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:08,239:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:08,239:INFO:Initializing create_model()
2025-05-12 15:56:08,239:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=en_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:08,239:INFO:Checking exceptions
2025-05-12 15:56:08,239:INFO:Importing libraries
2025-05-12 15:56:08,239:INFO:Copying training dataset
2025-05-12 15:56:08,242:INFO:Defining folds
2025-05-12 15:56:08,242:INFO:Declaring metric variables
2025-05-12 15:56:08,242:INFO:Importing untrained model
2025-05-12 15:56:08,243:INFO:Elastic Net w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:08,246:INFO:Starting cross validation
2025-05-12 15:56:08,246:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:09,006:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:09,012:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:09,013:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:56:09,098:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,098:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,099:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,099:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,101:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,101:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,110:INFO:Calculating mean and std
2025-05-12 15:56:09,112:INFO:Creating metrics dataframe
2025-05-12 15:56:09,118:INFO:Uploading results into container
2025-05-12 15:56:09,118:INFO:Uploading model into container now
2025-05-12 15:56:09,120:INFO:_master_model_container: 9
2025-05-12 15:56:09,120:INFO:_display_container: 2
2025-05-12 15:56:09,123:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1)
2025-05-12 15:56:09,124:INFO:create_model() successfully completed......................................
2025-05-12 15:56:09,210:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:09,210:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:09,211:INFO:Initializing Ridge w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:09,211:INFO:Total runtime is 0.3059205174446106 minutes
2025-05-12 15:56:09,211:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:09,211:INFO:Initializing create_model()
2025-05-12 15:56:09,211:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=ridge_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:09,211:INFO:Checking exceptions
2025-05-12 15:56:09,211:INFO:Importing libraries
2025-05-12 15:56:09,211:INFO:Copying training dataset
2025-05-12 15:56:09,214:INFO:Defining folds
2025-05-12 15:56:09,214:INFO:Declaring metric variables
2025-05-12 15:56:09,214:INFO:Importing untrained model
2025-05-12 15:56:09,216:INFO:Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:09,218:INFO:Starting cross validation
2025-05-12 15:56:09,218:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:09,311:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,311:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,311:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,311:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,311:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,311:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,327:INFO:Calculating mean and std
2025-05-12 15:56:09,327:INFO:Creating metrics dataframe
2025-05-12 15:56:09,330:INFO:Uploading results into container
2025-05-12 15:56:09,330:INFO:Uploading model into container now
2025-05-12 15:56:09,330:INFO:_master_model_container: 10
2025-05-12 15:56:09,330:INFO:_display_container: 2
2025-05-12 15:56:09,332:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1)
2025-05-12 15:56:09,332:INFO:create_model() successfully completed......................................
2025-05-12 15:56:09,421:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:09,422:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:09,422:INFO:Initializing create_model()
2025-05-12 15:56:09,422:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=ridge_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:09,422:INFO:Checking exceptions
2025-05-12 15:56:09,422:INFO:Importing libraries
2025-05-12 15:56:09,422:INFO:Copying training dataset
2025-05-12 15:56:09,424:INFO:Defining folds
2025-05-12 15:56:09,424:INFO:Declaring metric variables
2025-05-12 15:56:09,424:INFO:Importing untrained model
2025-05-12 15:56:09,425:INFO:Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:09,427:INFO:Starting cross validation
2025-05-12 15:56:09,427:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:09,524:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,524:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,524:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,524:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,540:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,540:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,556:INFO:Calculating mean and std
2025-05-12 15:56:09,556:INFO:Creating metrics dataframe
2025-05-12 15:56:09,556:INFO:Uploading results into container
2025-05-12 15:56:09,556:INFO:Uploading model into container now
2025-05-12 15:56:09,556:INFO:_master_model_container: 11
2025-05-12 15:56:09,556:INFO:_display_container: 2
2025-05-12 15:56:09,556:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1)
2025-05-12 15:56:09,556:INFO:create_model() successfully completed......................................
2025-05-12 15:56:09,645:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:09,645:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:09,645:INFO:Initializing Lasso w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:09,645:INFO:Total runtime is 0.31315828164418535 minutes
2025-05-12 15:56:09,645:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:09,645:INFO:Initializing create_model()
2025-05-12 15:56:09,645:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=lasso_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:09,645:INFO:Checking exceptions
2025-05-12 15:56:09,645:INFO:Importing libraries
2025-05-12 15:56:09,645:INFO:Copying training dataset
2025-05-12 15:56:09,645:INFO:Defining folds
2025-05-12 15:56:09,645:INFO:Declaring metric variables
2025-05-12 15:56:09,645:INFO:Importing untrained model
2025-05-12 15:56:09,645:INFO:Lasso w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:09,645:INFO:Starting cross validation
2025-05-12 15:56:09,645:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:09,744:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,744:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,744:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,744:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,744:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,744:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,760:INFO:Calculating mean and std
2025-05-12 15:56:09,760:INFO:Creating metrics dataframe
2025-05-12 15:56:09,763:INFO:Uploading results into container
2025-05-12 15:56:09,763:INFO:Uploading model into container now
2025-05-12 15:56:09,763:INFO:_master_model_container: 12
2025-05-12 15:56:09,763:INFO:_display_container: 2
2025-05-12 15:56:09,765:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1)
2025-05-12 15:56:09,765:INFO:create_model() successfully completed......................................
2025-05-12 15:56:09,846:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:09,846:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:09,846:INFO:Initializing create_model()
2025-05-12 15:56:09,846:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=lasso_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:09,846:INFO:Checking exceptions
2025-05-12 15:56:09,846:INFO:Importing libraries
2025-05-12 15:56:09,846:INFO:Copying training dataset
2025-05-12 15:56:09,846:INFO:Defining folds
2025-05-12 15:56:09,846:INFO:Declaring metric variables
2025-05-12 15:56:09,846:INFO:Importing untrained model
2025-05-12 15:56:09,846:INFO:Lasso w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:09,846:INFO:Starting cross validation
2025-05-12 15:56:09,846:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:09,977:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,977:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,977:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,977:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:09,987:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:09,987:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,003:INFO:Calculating mean and std
2025-05-12 15:56:10,005:INFO:Creating metrics dataframe
2025-05-12 15:56:10,014:INFO:Uploading results into container
2025-05-12 15:56:10,014:INFO:Uploading model into container now
2025-05-12 15:56:10,016:INFO:_master_model_container: 13
2025-05-12 15:56:10,016:INFO:_display_container: 2
2025-05-12 15:56:10,020:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1)
2025-05-12 15:56:10,020:INFO:create_model() successfully completed......................................
2025-05-12 15:56:10,096:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:10,096:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:10,096:INFO:Initializing Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:10,096:INFO:Total runtime is 0.3206762711207072 minutes
2025-05-12 15:56:10,096:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:10,096:INFO:Initializing create_model()
2025-05-12 15:56:10,096:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=llar_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:10,096:INFO:Checking exceptions
2025-05-12 15:56:10,096:INFO:Importing libraries
2025-05-12 15:56:10,096:INFO:Copying training dataset
2025-05-12 15:56:10,112:INFO:Defining folds
2025-05-12 15:56:10,112:INFO:Declaring metric variables
2025-05-12 15:56:10,112:INFO:Importing untrained model
2025-05-12 15:56:10,115:INFO:Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:10,115:INFO:Starting cross validation
2025-05-12 15:56:10,117:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:10,215:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,215:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,215:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,215:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,215:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,215:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,227:INFO:Calculating mean and std
2025-05-12 15:56:10,229:INFO:Creating metrics dataframe
2025-05-12 15:56:10,229:INFO:Uploading results into container
2025-05-12 15:56:10,229:INFO:Uploading model into container now
2025-05-12 15:56:10,229:INFO:_master_model_container: 14
2025-05-12 15:56:10,229:INFO:_display_container: 2
2025-05-12 15:56:10,229:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1)
2025-05-12 15:56:10,229:INFO:create_model() successfully completed......................................
2025-05-12 15:56:10,326:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:10,326:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:10,326:INFO:Initializing create_model()
2025-05-12 15:56:10,326:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=llar_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:10,326:INFO:Checking exceptions
2025-05-12 15:56:10,326:INFO:Importing libraries
2025-05-12 15:56:10,326:INFO:Copying training dataset
2025-05-12 15:56:10,329:INFO:Defining folds
2025-05-12 15:56:10,329:INFO:Declaring metric variables
2025-05-12 15:56:10,329:INFO:Importing untrained model
2025-05-12 15:56:10,329:INFO:Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:10,331:INFO:Starting cross validation
2025-05-12 15:56:10,334:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:10,428:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,428:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,428:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,428:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,428:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,428:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,444:INFO:Calculating mean and std
2025-05-12 15:56:10,444:INFO:Creating metrics dataframe
2025-05-12 15:56:10,447:INFO:Uploading results into container
2025-05-12 15:56:10,447:INFO:Uploading model into container now
2025-05-12 15:56:10,447:INFO:_master_model_container: 15
2025-05-12 15:56:10,447:INFO:_display_container: 2
2025-05-12 15:56:10,449:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1)
2025-05-12 15:56:10,449:INFO:create_model() successfully completed......................................
2025-05-12 15:56:10,536:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:10,536:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:10,536:INFO:Initializing Bayesian Ridge w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:10,536:INFO:Total runtime is 0.3280066728591919 minutes
2025-05-12 15:56:10,537:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:10,537:INFO:Initializing create_model()
2025-05-12 15:56:10,537:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=br_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:10,537:INFO:Checking exceptions
2025-05-12 15:56:10,537:INFO:Importing libraries
2025-05-12 15:56:10,537:INFO:Copying training dataset
2025-05-12 15:56:10,539:INFO:Defining folds
2025-05-12 15:56:10,539:INFO:Declaring metric variables
2025-05-12 15:56:10,539:INFO:Importing untrained model
2025-05-12 15:56:10,540:INFO:Bayesian Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:10,542:INFO:Starting cross validation
2025-05-12 15:56:10,543:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:10,644:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,644:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,644:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,644:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,649:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,649:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,662:INFO:Calculating mean and std
2025-05-12 15:56:10,664:INFO:Creating metrics dataframe
2025-05-12 15:56:10,670:INFO:Uploading results into container
2025-05-12 15:56:10,670:INFO:Uploading model into container now
2025-05-12 15:56:10,671:INFO:_master_model_container: 16
2025-05-12 15:56:10,671:INFO:_display_container: 2
2025-05-12 15:56:10,674:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1)
2025-05-12 15:56:10,674:INFO:create_model() successfully completed......................................
2025-05-12 15:56:10,760:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:10,760:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:10,760:INFO:Initializing create_model()
2025-05-12 15:56:10,760:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=br_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:10,760:INFO:Checking exceptions
2025-05-12 15:56:10,761:INFO:Importing libraries
2025-05-12 15:56:10,761:INFO:Copying training dataset
2025-05-12 15:56:10,763:INFO:Defining folds
2025-05-12 15:56:10,763:INFO:Declaring metric variables
2025-05-12 15:56:10,763:INFO:Importing untrained model
2025-05-12 15:56:10,765:INFO:Bayesian Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:10,767:INFO:Starting cross validation
2025-05-12 15:56:10,768:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:10,865:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,865:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,866:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,866:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,867:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:10,867:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:10,877:INFO:Calculating mean and std
2025-05-12 15:56:10,878:INFO:Creating metrics dataframe
2025-05-12 15:56:10,879:INFO:Uploading results into container
2025-05-12 15:56:10,879:INFO:Uploading model into container now
2025-05-12 15:56:10,879:INFO:_master_model_container: 17
2025-05-12 15:56:10,879:INFO:_display_container: 2
2025-05-12 15:56:10,879:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1)
2025-05-12 15:56:10,879:INFO:create_model() successfully completed......................................
2025-05-12 15:56:10,974:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:10,974:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:10,974:INFO:Initializing Huber w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:10,974:INFO:Total runtime is 0.3353069265683492 minutes
2025-05-12 15:56:10,974:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:10,974:INFO:Initializing create_model()
2025-05-12 15:56:10,974:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=huber_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:10,974:INFO:Checking exceptions
2025-05-12 15:56:10,974:INFO:Importing libraries
2025-05-12 15:56:10,974:INFO:Copying training dataset
2025-05-12 15:56:10,974:INFO:Defining folds
2025-05-12 15:56:10,974:INFO:Declaring metric variables
2025-05-12 15:56:10,974:INFO:Importing untrained model
2025-05-12 15:56:10,974:INFO:Huber w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:10,980:INFO:Starting cross validation
2025-05-12 15:56:10,982:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:11,063:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,063:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,063:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,063:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,063:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,063:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,079:INFO:Calculating mean and std
2025-05-12 15:56:11,079:INFO:Creating metrics dataframe
2025-05-12 15:56:11,079:INFO:Uploading results into container
2025-05-12 15:56:11,079:INFO:Uploading model into container now
2025-05-12 15:56:11,079:INFO:_master_model_container: 18
2025-05-12 15:56:11,079:INFO:_display_container: 2
2025-05-12 15:56:11,079:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1)
2025-05-12 15:56:11,079:INFO:create_model() successfully completed......................................
2025-05-12 15:56:11,163:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:11,163:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:11,163:INFO:Initializing create_model()
2025-05-12 15:56:11,163:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=huber_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:11,163:INFO:Checking exceptions
2025-05-12 15:56:11,163:INFO:Importing libraries
2025-05-12 15:56:11,163:INFO:Copying training dataset
2025-05-12 15:56:11,163:INFO:Defining folds
2025-05-12 15:56:11,163:INFO:Declaring metric variables
2025-05-12 15:56:11,163:INFO:Importing untrained model
2025-05-12 15:56:11,163:INFO:Huber w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:11,163:INFO:Starting cross validation
2025-05-12 15:56:11,163:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:11,261:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,261:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,261:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,261:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,261:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,261:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,277:INFO:Calculating mean and std
2025-05-12 15:56:11,277:INFO:Creating metrics dataframe
2025-05-12 15:56:11,277:INFO:Uploading results into container
2025-05-12 15:56:11,277:INFO:Uploading model into container now
2025-05-12 15:56:11,277:INFO:_master_model_container: 19
2025-05-12 15:56:11,277:INFO:_display_container: 2
2025-05-12 15:56:11,277:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1)
2025-05-12 15:56:11,277:INFO:create_model() successfully completed......................................
2025-05-12 15:56:11,363:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:11,363:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:11,363:INFO:Initializing Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:11,363:INFO:Total runtime is 0.3417949676513672 minutes
2025-05-12 15:56:11,363:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:11,363:INFO:Initializing create_model()
2025-05-12 15:56:11,363:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=omp_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:11,363:INFO:Checking exceptions
2025-05-12 15:56:11,363:INFO:Importing libraries
2025-05-12 15:56:11,363:INFO:Copying training dataset
2025-05-12 15:56:11,363:INFO:Defining folds
2025-05-12 15:56:11,363:INFO:Declaring metric variables
2025-05-12 15:56:11,363:INFO:Importing untrained model
2025-05-12 15:56:11,363:INFO:Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:11,379:INFO:Starting cross validation
2025-05-12 15:56:11,379:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:11,471:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,471:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,471:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,471:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,473:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,473:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,489:INFO:Calculating mean and std
2025-05-12 15:56:11,491:INFO:Creating metrics dataframe
2025-05-12 15:56:11,491:INFO:Uploading results into container
2025-05-12 15:56:11,491:INFO:Uploading model into container now
2025-05-12 15:56:11,491:INFO:_master_model_container: 20
2025-05-12 15:56:11,491:INFO:_display_container: 2
2025-05-12 15:56:11,491:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1)
2025-05-12 15:56:11,491:INFO:create_model() successfully completed......................................
2025-05-12 15:56:11,586:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:11,587:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:11,587:INFO:Initializing create_model()
2025-05-12 15:56:11,587:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=omp_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:11,587:INFO:Checking exceptions
2025-05-12 15:56:11,587:INFO:Importing libraries
2025-05-12 15:56:11,587:INFO:Copying training dataset
2025-05-12 15:56:11,590:INFO:Defining folds
2025-05-12 15:56:11,590:INFO:Declaring metric variables
2025-05-12 15:56:11,591:INFO:Importing untrained model
2025-05-12 15:56:11,592:INFO:Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:11,593:INFO:Starting cross validation
2025-05-12 15:56:11,593:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:11,685:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,685:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,693:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,693:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,695:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,695:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,707:INFO:Calculating mean and std
2025-05-12 15:56:11,707:INFO:Creating metrics dataframe
2025-05-12 15:56:11,707:INFO:Uploading results into container
2025-05-12 15:56:11,707:INFO:Uploading model into container now
2025-05-12 15:56:11,707:INFO:_master_model_container: 21
2025-05-12 15:56:11,707:INFO:_display_container: 2
2025-05-12 15:56:11,707:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1)
2025-05-12 15:56:11,707:INFO:create_model() successfully completed......................................
2025-05-12 15:56:11,794:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:11,794:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:11,794:INFO:Initializing K Neighbors w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:11,794:INFO:Total runtime is 0.3489704608917236 minutes
2025-05-12 15:56:11,794:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:11,794:INFO:Initializing create_model()
2025-05-12 15:56:11,794:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=knn_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:11,794:INFO:Checking exceptions
2025-05-12 15:56:11,794:INFO:Importing libraries
2025-05-12 15:56:11,794:INFO:Copying training dataset
2025-05-12 15:56:11,794:INFO:Defining folds
2025-05-12 15:56:11,794:INFO:Declaring metric variables
2025-05-12 15:56:11,794:INFO:Importing untrained model
2025-05-12 15:56:11,794:INFO:K Neighbors w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:11,794:INFO:Starting cross validation
2025-05-12 15:56:11,810:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:11,901:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,901:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,903:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,903:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,907:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:11,907:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:11,921:INFO:Calculating mean and std
2025-05-12 15:56:11,921:INFO:Creating metrics dataframe
2025-05-12 15:56:11,921:INFO:Uploading results into container
2025-05-12 15:56:11,921:INFO:Uploading model into container now
2025-05-12 15:56:11,921:INFO:_master_model_container: 22
2025-05-12 15:56:11,921:INFO:_display_container: 2
2025-05-12 15:56:11,921:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1)
2025-05-12 15:56:11,921:INFO:create_model() successfully completed......................................
2025-05-12 15:56:12,016:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:12,016:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:12,016:INFO:Initializing create_model()
2025-05-12 15:56:12,016:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=knn_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:12,016:INFO:Checking exceptions
2025-05-12 15:56:12,016:INFO:Importing libraries
2025-05-12 15:56:12,016:INFO:Copying training dataset
2025-05-12 15:56:12,016:INFO:Defining folds
2025-05-12 15:56:12,016:INFO:Declaring metric variables
2025-05-12 15:56:12,016:INFO:Importing untrained model
2025-05-12 15:56:12,016:INFO:K Neighbors w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:12,016:INFO:Starting cross validation
2025-05-12 15:56:12,016:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:12,112:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,112:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,127:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,127:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,127:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,127:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,144:INFO:Calculating mean and std
2025-05-12 15:56:12,144:INFO:Creating metrics dataframe
2025-05-12 15:56:12,144:INFO:Uploading results into container
2025-05-12 15:56:12,144:INFO:Uploading model into container now
2025-05-12 15:56:12,144:INFO:_master_model_container: 23
2025-05-12 15:56:12,144:INFO:_display_container: 2
2025-05-12 15:56:12,144:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1)
2025-05-12 15:56:12,144:INFO:create_model() successfully completed......................................
2025-05-12 15:56:12,238:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:12,238:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:12,238:INFO:Initializing Decision Tree w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:12,238:INFO:Total runtime is 0.35637008349100746 minutes
2025-05-12 15:56:12,238:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:12,238:INFO:Initializing create_model()
2025-05-12 15:56:12,238:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=dt_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:12,238:INFO:Checking exceptions
2025-05-12 15:56:12,238:INFO:Importing libraries
2025-05-12 15:56:12,238:INFO:Copying training dataset
2025-05-12 15:56:12,238:INFO:Defining folds
2025-05-12 15:56:12,238:INFO:Declaring metric variables
2025-05-12 15:56:12,238:INFO:Importing untrained model
2025-05-12 15:56:12,238:INFO:Decision Tree w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:12,238:INFO:Starting cross validation
2025-05-12 15:56:12,238:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:12,332:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,332:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,332:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,332:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,332:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,332:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,348:INFO:Calculating mean and std
2025-05-12 15:56:12,348:INFO:Creating metrics dataframe
2025-05-12 15:56:12,348:INFO:Uploading results into container
2025-05-12 15:56:12,348:INFO:Uploading model into container now
2025-05-12 15:56:12,348:INFO:_master_model_container: 24
2025-05-12 15:56:12,348:INFO:_display_container: 2
2025-05-12 15:56:12,348:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:56:12,348:INFO:create_model() successfully completed......................................
2025-05-12 15:56:12,444:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:12,444:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:12,444:INFO:Initializing create_model()
2025-05-12 15:56:12,444:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=dt_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:12,444:INFO:Checking exceptions
2025-05-12 15:56:12,444:INFO:Importing libraries
2025-05-12 15:56:12,444:INFO:Copying training dataset
2025-05-12 15:56:12,444:INFO:Defining folds
2025-05-12 15:56:12,444:INFO:Declaring metric variables
2025-05-12 15:56:12,444:INFO:Importing untrained model
2025-05-12 15:56:12,444:INFO:Decision Tree w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:12,444:INFO:Starting cross validation
2025-05-12 15:56:12,444:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:12,545:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,545:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,547:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,547:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,547:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,547:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,553:INFO:Calculating mean and std
2025-05-12 15:56:12,553:INFO:Creating metrics dataframe
2025-05-12 15:56:12,553:INFO:Uploading results into container
2025-05-12 15:56:12,553:INFO:Uploading model into container now
2025-05-12 15:56:12,553:INFO:_master_model_container: 25
2025-05-12 15:56:12,553:INFO:_display_container: 2
2025-05-12 15:56:12,553:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:56:12,553:INFO:create_model() successfully completed......................................
2025-05-12 15:56:12,648:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:12,648:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:12,648:INFO:Initializing Random Forest w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:12,648:INFO:Total runtime is 0.36320872306823726 minutes
2025-05-12 15:56:12,648:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:12,648:INFO:Initializing create_model()
2025-05-12 15:56:12,648:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=rf_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:12,648:INFO:Checking exceptions
2025-05-12 15:56:12,648:INFO:Importing libraries
2025-05-12 15:56:12,648:INFO:Copying training dataset
2025-05-12 15:56:12,648:INFO:Defining folds
2025-05-12 15:56:12,648:INFO:Declaring metric variables
2025-05-12 15:56:12,648:INFO:Importing untrained model
2025-05-12 15:56:12,648:INFO:Random Forest w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:12,648:INFO:Starting cross validation
2025-05-12 15:56:12,648:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:12,755:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,755:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,755:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,755:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,757:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,757:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,759:INFO:Calculating mean and std
2025-05-12 15:56:12,759:INFO:Creating metrics dataframe
2025-05-12 15:56:12,759:INFO:Uploading results into container
2025-05-12 15:56:12,759:INFO:Uploading model into container now
2025-05-12 15:56:12,759:INFO:_master_model_container: 26
2025-05-12 15:56:12,759:INFO:_display_container: 2
2025-05-12 15:56:12,759:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:56:12,759:INFO:create_model() successfully completed......................................
2025-05-12 15:56:12,848:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:12,848:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:12,848:INFO:Initializing create_model()
2025-05-12 15:56:12,848:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=rf_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:12,848:INFO:Checking exceptions
2025-05-12 15:56:12,848:INFO:Importing libraries
2025-05-12 15:56:12,849:INFO:Copying training dataset
2025-05-12 15:56:12,851:INFO:Defining folds
2025-05-12 15:56:12,851:INFO:Declaring metric variables
2025-05-12 15:56:12,851:INFO:Importing untrained model
2025-05-12 15:56:12,852:INFO:Random Forest w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:12,855:INFO:Starting cross validation
2025-05-12 15:56:12,855:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:12,956:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,957:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,957:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,957:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,959:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:12,959:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:12,966:INFO:Calculating mean and std
2025-05-12 15:56:12,966:INFO:Creating metrics dataframe
2025-05-12 15:56:12,970:INFO:Uploading results into container
2025-05-12 15:56:12,970:INFO:Uploading model into container now
2025-05-12 15:56:12,970:INFO:_master_model_container: 27
2025-05-12 15:56:12,970:INFO:_display_container: 2
2025-05-12 15:56:12,972:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:56:12,972:INFO:create_model() successfully completed......................................
2025-05-12 15:56:13,048:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:13,048:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:13,048:INFO:Initializing Extra Trees w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:13,048:INFO:Total runtime is 0.36988141536712643 minutes
2025-05-12 15:56:13,048:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:13,048:INFO:Initializing create_model()
2025-05-12 15:56:13,048:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=et_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:13,048:INFO:Checking exceptions
2025-05-12 15:56:13,048:INFO:Importing libraries
2025-05-12 15:56:13,048:INFO:Copying training dataset
2025-05-12 15:56:13,063:INFO:Defining folds
2025-05-12 15:56:13,063:INFO:Declaring metric variables
2025-05-12 15:56:13,063:INFO:Importing untrained model
2025-05-12 15:56:13,065:INFO:Extra Trees w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:13,065:INFO:Starting cross validation
2025-05-12 15:56:13,065:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:13,165:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,165:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,165:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,165:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,165:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,165:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,181:INFO:Calculating mean and std
2025-05-12 15:56:13,183:INFO:Creating metrics dataframe
2025-05-12 15:56:13,195:INFO:Uploading results into container
2025-05-12 15:56:13,195:INFO:Uploading model into container now
2025-05-12 15:56:13,196:INFO:_master_model_container: 28
2025-05-12 15:56:13,196:INFO:_display_container: 2
2025-05-12 15:56:13,196:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:56:13,196:INFO:create_model() successfully completed......................................
2025-05-12 15:56:13,287:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:13,287:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:13,287:INFO:Initializing create_model()
2025-05-12 15:56:13,287:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=et_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:13,287:INFO:Checking exceptions
2025-05-12 15:56:13,287:INFO:Importing libraries
2025-05-12 15:56:13,287:INFO:Copying training dataset
2025-05-12 15:56:13,287:INFO:Defining folds
2025-05-12 15:56:13,287:INFO:Declaring metric variables
2025-05-12 15:56:13,287:INFO:Importing untrained model
2025-05-12 15:56:13,287:INFO:Extra Trees w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:13,287:INFO:Starting cross validation
2025-05-12 15:56:13,287:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:13,386:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,386:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,386:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,386:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,386:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,386:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,396:INFO:Calculating mean and std
2025-05-12 15:56:13,396:INFO:Creating metrics dataframe
2025-05-12 15:56:13,399:INFO:Uploading results into container
2025-05-12 15:56:13,399:INFO:Uploading model into container now
2025-05-12 15:56:13,399:INFO:_master_model_container: 29
2025-05-12 15:56:13,399:INFO:_display_container: 2
2025-05-12 15:56:13,401:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:56:13,401:INFO:create_model() successfully completed......................................
2025-05-12 15:56:13,487:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:13,487:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:13,487:INFO:Initializing Gradient Boosting w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:13,487:INFO:Total runtime is 0.37720127900441486 minutes
2025-05-12 15:56:13,487:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:13,487:INFO:Initializing create_model()
2025-05-12 15:56:13,487:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=gbr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:13,487:INFO:Checking exceptions
2025-05-12 15:56:13,487:INFO:Importing libraries
2025-05-12 15:56:13,487:INFO:Copying training dataset
2025-05-12 15:56:13,487:INFO:Defining folds
2025-05-12 15:56:13,487:INFO:Declaring metric variables
2025-05-12 15:56:13,487:INFO:Importing untrained model
2025-05-12 15:56:13,487:INFO:Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:13,496:INFO:Starting cross validation
2025-05-12 15:56:13,498:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:13,586:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,586:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,598:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,599:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,600:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,600:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,614:INFO:Calculating mean and std
2025-05-12 15:56:13,615:INFO:Creating metrics dataframe
2025-05-12 15:56:13,620:INFO:Uploading results into container
2025-05-12 15:56:13,620:INFO:Uploading model into container now
2025-05-12 15:56:13,620:INFO:_master_model_container: 30
2025-05-12 15:56:13,620:INFO:_display_container: 2
2025-05-12 15:56:13,620:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:56:13,620:INFO:create_model() successfully completed......................................
2025-05-12 15:56:13,720:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:13,720:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:13,720:INFO:Initializing create_model()
2025-05-12 15:56:13,720:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=gbr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:13,720:INFO:Checking exceptions
2025-05-12 15:56:13,720:INFO:Importing libraries
2025-05-12 15:56:13,720:INFO:Copying training dataset
2025-05-12 15:56:13,720:INFO:Defining folds
2025-05-12 15:56:13,720:INFO:Declaring metric variables
2025-05-12 15:56:13,720:INFO:Importing untrained model
2025-05-12 15:56:13,720:INFO:Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:13,720:INFO:Starting cross validation
2025-05-12 15:56:13,720:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:13,820:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,820:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,820:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,820:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,820:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:13,820:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:13,830:INFO:Calculating mean and std
2025-05-12 15:56:13,830:INFO:Creating metrics dataframe
2025-05-12 15:56:13,833:INFO:Uploading results into container
2025-05-12 15:56:13,833:INFO:Uploading model into container now
2025-05-12 15:56:13,833:INFO:_master_model_container: 31
2025-05-12 15:56:13,833:INFO:_display_container: 2
2025-05-12 15:56:13,835:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:56:13,835:INFO:create_model() successfully completed......................................
2025-05-12 15:56:13,920:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:13,920:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:13,920:INFO:Initializing AdaBoost w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:13,920:INFO:Total runtime is 0.3844082991282145 minutes
2025-05-12 15:56:13,920:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:13,920:INFO:Initializing create_model()
2025-05-12 15:56:13,920:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=ada_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:13,920:INFO:Checking exceptions
2025-05-12 15:56:13,920:INFO:Importing libraries
2025-05-12 15:56:13,920:INFO:Copying training dataset
2025-05-12 15:56:13,920:INFO:Defining folds
2025-05-12 15:56:13,920:INFO:Declaring metric variables
2025-05-12 15:56:13,920:INFO:Importing untrained model
2025-05-12 15:56:13,920:INFO:AdaBoost w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:13,920:INFO:Starting cross validation
2025-05-12 15:56:13,920:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:14,013:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,013:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:14,013:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,013:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:14,013:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,013:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:14,029:INFO:Calculating mean and std
2025-05-12 15:56:14,029:INFO:Creating metrics dataframe
2025-05-12 15:56:14,029:INFO:Uploading results into container
2025-05-12 15:56:14,029:INFO:Uploading model into container now
2025-05-12 15:56:14,029:INFO:_master_model_container: 32
2025-05-12 15:56:14,029:INFO:_display_container: 2
2025-05-12 15:56:14,029:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:56:14,029:INFO:create_model() successfully completed......................................
2025-05-12 15:56:14,117:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:14,119:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:14,119:INFO:Initializing create_model()
2025-05-12 15:56:14,119:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=ada_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:14,119:INFO:Checking exceptions
2025-05-12 15:56:14,119:INFO:Importing libraries
2025-05-12 15:56:14,119:INFO:Copying training dataset
2025-05-12 15:56:14,119:INFO:Defining folds
2025-05-12 15:56:14,119:INFO:Declaring metric variables
2025-05-12 15:56:14,119:INFO:Importing untrained model
2025-05-12 15:56:14,119:INFO:AdaBoost w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:14,119:INFO:Starting cross validation
2025-05-12 15:56:14,119:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:14,195:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,195:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
                           n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,195:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:14,210:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,210:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:56:14,226:INFO:Calculating mean and std
2025-05-12 15:56:14,226:INFO:Creating metrics dataframe
2025-05-12 15:56:14,226:INFO:Uploading results into container
2025-05-12 15:56:14,226:INFO:Uploading model into container now
2025-05-12 15:56:14,226:INFO:_master_model_container: 33
2025-05-12 15:56:14,226:INFO:_display_container: 2
2025-05-12 15:56:14,226:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:56:14,226:INFO:create_model() successfully completed......................................
2025-05-12 15:56:14,312:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:14,312:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:14,312:INFO:Initializing Light Gradient Boosting w/ Cond. Deseasonalize & Detrending
2025-05-12 15:56:14,312:INFO:Total runtime is 0.3909511804580688 minutes
2025-05-12 15:56:14,312:INFO:SubProcess create_model() called ==================================
2025-05-12 15:56:14,312:INFO:Initializing create_model()
2025-05-12 15:56:14,312:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=lightgbm_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:14,312:INFO:Checking exceptions
2025-05-12 15:56:14,312:INFO:Importing libraries
2025-05-12 15:56:14,312:INFO:Copying training dataset
2025-05-12 15:56:14,312:INFO:Defining folds
2025-05-12 15:56:14,312:INFO:Declaring metric variables
2025-05-12 15:56:14,312:INFO:Importing untrained model
2025-05-12 15:56:14,312:INFO:Light Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:14,328:INFO:Starting cross validation
2025-05-12 15:56:14,328:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:14,521:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,521:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:56:14,521:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,521:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:56:14,522:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,522:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:56:14,530:INFO:Calculating mean and std
2025-05-12 15:56:14,531:INFO:Creating metrics dataframe
2025-05-12 15:56:14,538:INFO:Uploading results into container
2025-05-12 15:56:14,538:INFO:Uploading model into container now
2025-05-12 15:56:14,539:INFO:_master_model_container: 34
2025-05-12 15:56:14,539:INFO:_display_container: 2
2025-05-12 15:56:14,543:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:56:14,543:INFO:create_model() successfully completed......................................
2025-05-12 15:56:14,630:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:56:14,630:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:14,630:INFO:Initializing create_model()
2025-05-12 15:56:14,630:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=lightgbm_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000238E9C40760>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:14,630:INFO:Checking exceptions
2025-05-12 15:56:14,630:INFO:Importing libraries
2025-05-12 15:56:14,630:INFO:Copying training dataset
2025-05-12 15:56:14,632:INFO:Defining folds
2025-05-12 15:56:14,632:INFO:Declaring metric variables
2025-05-12 15:56:14,632:INFO:Importing untrained model
2025-05-12 15:56:14,632:INFO:Light Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:56:14,632:INFO:Starting cross validation
2025-05-12 15:56:14,632:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:56:14,817:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,817:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:56:14,817:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:56:14,817:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:56:14,817:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:56:14,833:INFO:Calculating mean and std
2025-05-12 15:56:14,834:INFO:Creating metrics dataframe
2025-05-12 15:56:14,838:INFO:Uploading results into container
2025-05-12 15:56:14,838:INFO:Uploading model into container now
2025-05-12 15:56:14,838:INFO:_master_model_container: 35
2025-05-12 15:56:14,838:INFO:_display_container: 2
2025-05-12 15:56:14,838:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:56:14,838:INFO:create_model() successfully completed......................................
2025-05-12 15:56:14,931:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:56:14,931:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:14,931:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-05-12 15:56:14,931:INFO:Initializing create_model()
2025-05-12 15:56:14,931:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=Croston(), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:14,931:INFO:Checking exceptions
2025-05-12 15:56:14,931:INFO:Importing libraries
2025-05-12 15:56:14,931:INFO:Copying training dataset
2025-05-12 15:56:14,931:INFO:Defining folds
2025-05-12 15:56:14,931:INFO:Declaring metric variables
2025-05-12 15:56:14,931:INFO:Importing untrained model
2025-05-12 15:56:14,931:INFO:Declaring custom model
2025-05-12 15:56:14,931:INFO:Croston Imported successfully
2025-05-12 15:56:14,931:INFO:Cross validation set to False
2025-05-12 15:56:14,931:INFO:Fitting Model
2025-05-12 15:56:14,948:INFO:Croston()
2025-05-12 15:56:14,948:INFO:create_model() successfully completed......................................
2025-05-12 15:56:15,032:ERROR:create_model() for Croston() raised an exception or returned all 0.0:
2025-05-12 15:56:15,032:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 954, in compare_models
    np.sum(
AssertionError

2025-05-12 15:56:15,032:INFO:_master_model_container: 35
2025-05-12 15:56:15,032:INFO:_display_container: 2
2025-05-12 15:56:15,032:INFO:Croston()
2025-05-12 15:56:15,032:INFO:compare_models() successfully completed......................................
2025-05-12 15:56:15,032:INFO:Initializing finalize_model()
2025-05-12 15:56:15,032:INFO:finalize_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=Croston(), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-05-12 15:56:15,032:INFO:Finalizing Croston()
2025-05-12 15:56:15,032:INFO:Initializing create_model()
2025-05-12 15:56:15,032:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x00000238ACD2A050>, estimator=Croston(), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:56:15,032:INFO:Checking exceptions
2025-05-12 15:56:15,032:INFO:Importing libraries
2025-05-12 15:56:15,032:INFO:Copying training dataset
2025-05-12 15:56:15,032:INFO:Defining folds
2025-05-12 15:56:15,032:INFO:Declaring metric variables
2025-05-12 15:56:15,032:INFO:Importing untrained model
2025-05-12 15:56:15,032:INFO:Declaring custom model
2025-05-12 15:56:15,047:INFO:Croston Imported successfully
2025-05-12 15:56:15,047:INFO:Cross validation set to False
2025-05-12 15:56:15,047:INFO:Fitting Model
2025-05-12 15:56:15,054:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))])
2025-05-12 15:56:15,054:INFO:create_model() successfully completed......................................
2025-05-12 15:56:15,147:INFO:_master_model_container: 35
2025-05-12 15:56:15,147:INFO:_display_container: 2
2025-05-12 15:56:15,147:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))])
2025-05-12 15:56:15,147:INFO:finalize_model() successfully completed......................................
2025-05-12 15:56:15,232:INFO:Initializing save_model()
2025-05-12 15:56:15,232:INFO:save_model(model=ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))]), model_name=aapl_best_model, prep_pipe_=ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                DummyForecaster())]))]), verbose=True, use_case=MLUsecase.TIME_SERIES, kwargs={})
2025-05-12 15:56:15,232:INFO:Adding model into prep_pipe
2025-05-12 15:56:15,248:INFO:aapl_best_model.pkl saved in current working directory
2025-05-12 15:56:15,248:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ForecastingPipeline(steps=[('forecaster',
                                                                                            TransformedTargetForecaster(steps=[('model',
                                                                                                                                Croston())]))]))]))])
2025-05-12 15:56:15,248:INFO:save_model() successfully completed......................................
2025-05-12 15:58:03,764:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:04,845:INFO:Initializing load_model()
2025-05-12 15:58:04,845:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 15:58:17,846:INFO:Initializing load_model()
2025-05-12 15:58:17,846:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 15:58:53,189:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:54,519:INFO:PyCaret TSForecastingExperiment
2025-05-12 15:58:54,519:INFO:Logging name: ts-default-name
2025-05-12 15:58:54,519:INFO:ML Usecase: MLUsecase.TIME_SERIES
2025-05-12 15:58:54,519:INFO:version 3.3.2
2025-05-12 15:58:54,519:INFO:Initializing setup()
2025-05-12 15:58:54,519:INFO:self.USI: 728f
2025-05-12 15:58:54,519:INFO:self._variable_keys: {'seasonality_present', 'fold_param', 'primary_sp_to_use', 'data', 'X_train_transformed', 'logging_param', 'pipeline', 'X_test', 'memory', 'fh', 'n_jobs_param', 'html_param', 'log_plots_param', 'y_train', 'y', 'X_transformed', 'enforce_pi', 'gpu_param', 'y_train_transformed', '_available_plots', 'y_transformed', 'significant_sps', 'model_engines', 'index_type', 'enforce_exogenous', 'all_sps_to_use', 'seed', '_ml_usecase', 'significant_sps_no_harmonics', 'gpu_n_jobs_param', 'USI', 'exp_id', 'X_train', 'approach_type', 'X_test_transformed', 'exogenous_present', 'y_test', 'candidate_sps', 'exp_name_log', 'X', 'y_test_transformed', 'fold_generator', 'idx', 'strictly_positive'}
2025-05-12 15:58:54,519:INFO:Checking environment
2025-05-12 15:58:54,519:INFO:python_version: 3.10.16
2025-05-12 15:58:54,525:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-05-12 15:58:54,525:INFO:machine: AMD64
2025-05-12 15:58:54,540:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-12 15:58:54,540:INFO:Memory: svmem(total=17009004544, available=3908530176, percent=77.0, used=13100474368, free=3908530176)
2025-05-12 15:58:54,540:INFO:Physical Core: 6
2025-05-12 15:58:54,540:INFO:Logical Core: 12
2025-05-12 15:58:54,540:INFO:Checking libraries
2025-05-12 15:58:54,540:INFO:System:
2025-05-12 15:58:54,540:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-05-12 15:58:54,540:INFO:executable: C:\Users\Thayse\.conda\envs\mlpipeline\python.exe
2025-05-12 15:58:54,540:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-12 15:58:54,540:INFO:PyCaret required dependencies:
2025-05-12 15:58:54,636:INFO:                 pip: 25.0
2025-05-12 15:58:54,636:INFO:          setuptools: 75.8.0
2025-05-12 15:58:54,636:INFO:             pycaret: 3.3.2
2025-05-12 15:58:54,636:INFO:             IPython: 8.34.0
2025-05-12 15:58:54,636:INFO:          ipywidgets: 8.1.5
2025-05-12 15:58:54,636:INFO:                tqdm: 4.67.1
2025-05-12 15:58:54,636:INFO:               numpy: 1.26.4
2025-05-12 15:58:54,636:INFO:              pandas: 2.1.4
2025-05-12 15:58:54,636:INFO:              jinja2: 3.1.6
2025-05-12 15:58:54,636:INFO:               scipy: 1.11.4
2025-05-12 15:58:54,636:INFO:              joblib: 1.3.2
2025-05-12 15:58:54,636:INFO:             sklearn: 1.4.2
2025-05-12 15:58:54,636:INFO:                pyod: 2.0.4
2025-05-12 15:58:54,636:INFO:            imblearn: 0.13.0
2025-05-12 15:58:54,636:INFO:   category_encoders: 2.7.0
2025-05-12 15:58:54,636:INFO:            lightgbm: 4.6.0
2025-05-12 15:58:54,636:INFO:               numba: 0.61.0
2025-05-12 15:58:54,636:INFO:            requests: 2.32.3
2025-05-12 15:58:54,636:INFO:          matplotlib: 3.7.5
2025-05-12 15:58:54,636:INFO:          scikitplot: 0.3.7
2025-05-12 15:58:54,636:INFO:         yellowbrick: 1.5
2025-05-12 15:58:54,636:INFO:              plotly: 5.24.1
2025-05-12 15:58:54,636:INFO:    plotly-resampler: Not installed
2025-05-12 15:58:54,636:INFO:             kaleido: 0.2.1
2025-05-12 15:58:54,636:INFO:           schemdraw: 0.15
2025-05-12 15:58:54,636:INFO:         statsmodels: 0.14.4
2025-05-12 15:58:54,636:INFO:              sktime: 0.26.0
2025-05-12 15:58:54,636:INFO:               tbats: 1.1.3
2025-05-12 15:58:54,636:INFO:            pmdarima: 2.0.4
2025-05-12 15:58:54,636:INFO:              psutil: 7.0.0
2025-05-12 15:58:54,636:INFO:          markupsafe: 3.0.2
2025-05-12 15:58:54,636:INFO:             pickle5: Not installed
2025-05-12 15:58:54,636:INFO:         cloudpickle: 3.1.1
2025-05-12 15:58:54,636:INFO:         deprecation: 2.1.0
2025-05-12 15:58:54,636:INFO:              xxhash: 3.5.0
2025-05-12 15:58:54,636:INFO:           wurlitzer: Not installed
2025-05-12 15:58:54,636:INFO:PyCaret optional dependencies:
2025-05-12 15:58:55,030:INFO:                shap: Not installed
2025-05-12 15:58:55,030:INFO:           interpret: Not installed
2025-05-12 15:58:55,030:INFO:                umap: Not installed
2025-05-12 15:58:55,030:INFO:     ydata_profiling: 4.16.1
2025-05-12 15:58:55,030:INFO:  explainerdashboard: Not installed
2025-05-12 15:58:55,030:INFO:             autoviz: Not installed
2025-05-12 15:58:55,030:INFO:           fairlearn: Not installed
2025-05-12 15:58:55,030:INFO:          deepchecks: Not installed
2025-05-12 15:58:55,031:INFO:             xgboost: Not installed
2025-05-12 15:58:55,031:INFO:            catboost: Not installed
2025-05-12 15:58:55,031:INFO:              kmodes: Not installed
2025-05-12 15:58:55,031:INFO:             mlxtend: Not installed
2025-05-12 15:58:55,031:INFO:       statsforecast: Not installed
2025-05-12 15:58:55,031:INFO:        tune_sklearn: Not installed
2025-05-12 15:58:55,031:INFO:                 ray: Not installed
2025-05-12 15:58:55,031:INFO:            hyperopt: Not installed
2025-05-12 15:58:55,031:INFO:              optuna: 4.2.1
2025-05-12 15:58:55,031:INFO:               skopt: Not installed
2025-05-12 15:58:55,031:INFO:              mlflow: Not installed
2025-05-12 15:58:55,031:INFO:              gradio: Not installed
2025-05-12 15:58:55,031:INFO:             fastapi: 0.115.12
2025-05-12 15:58:55,031:INFO:             uvicorn: 0.34.2
2025-05-12 15:58:55,031:INFO:              m2cgen: Not installed
2025-05-12 15:58:55,031:INFO:           evidently: Not installed
2025-05-12 15:58:55,031:INFO:               fugue: Not installed
2025-05-12 15:58:55,031:INFO:           streamlit: 1.44.1
2025-05-12 15:58:55,031:INFO:             prophet: 1.1.6
2025-05-12 15:58:55,031:INFO:None
2025-05-12 15:58:55,032:INFO:Set Forecast Horizon.
2025-05-12 15:58:55,032:INFO:Set up Train-Test Splits.
2025-05-12 15:58:55,100:INFO:Finished creating preprocessing pipeline.
2025-05-12 15:58:55,104:INFO:Pipeline: ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                DummyForecaster())]))])
2025-05-12 15:58:55,104:INFO:Set up Seasonal Period.
2025-05-12 15:58:55,117:INFO:Setting the seasonal component type - 'add' or 'mul'.
2025-05-12 15:58:55,117:INFO:Checking if data is strictly positive.
2025-05-12 15:58:55,143:INFO:Creating final display dataframe.
2025-05-12 15:58:55,159:INFO:Setup Display Container:                                          Description                    Value
0                                         session_id                      123
1                                             Target                    Close
2                                           Approach               Univariate
3                                Exogenous Variables                  Present
4                                Original data shape                 (501, 2)
5                             Transformed data shape                 (501, 2)
6                        Transformed train set shape                 (500, 2)
7                         Transformed test set shape                   (1, 2)
8                           Rows with missing values                     0.0%
9                                     Fold Generator  ExpandingWindowSplitter
10                                       Fold Number                        3
11                       Enforce Prediction Interval                    False
12                   Splits used for hyperparameters                      all
13                   User Defined Seasonal Period(s)                     None
14                           Ignore Seasonality Test                    False
15                        Seasonality Detection Algo                     auto
16                            Max Period to Consider                       60
17                         Seasonal Period(s) Tested                       []
18                    Significant Seasonal Period(s)                      [1]
19  Significant Seasonal Period(s) without Harmonics                      [1]
20                                  Remove Harmonics                    False
21                            Harmonics Order Method             harmonic_max
22                          Num Seasonalities to Use                        1
23                          All Seasonalities to Use                      [1]
24                               Primary Seasonality                        1
25                               Seasonality Present                    False
26                                  Seasonality Type                     None
27                          Target Strictly Positive                     True
28                                Target White Noise                       No
29                                     Recommended d                        1
30                            Recommended Seasonal D                        0
31                                        Preprocess                    False
32                                          CPU Jobs                       -1
33                                           Use GPU                    False
34                                    Log Experiment                    False
35                                   Experiment Name          ts-default-name
36                                               USI                     728f
2025-05-12 15:58:55,167:INFO:Engine successfully changes for model 'auto_arima' to 'pmdarima'.
2025-05-12 15:58:55,228:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,281:INFO:Engine for model 'lr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,281:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,282:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,282:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,283:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,283:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,283:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,286:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,288:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,289:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,289:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,290:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,290:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,291:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,291:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,291:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,291:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,291:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,292:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,292:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,292:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,296:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,296:INFO:Engine for model 'lr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,297:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,297:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,297:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,298:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,298:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,298:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,301:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,303:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,303:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,304:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,304:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,305:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,305:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,305:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,305:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,306:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,306:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,306:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,306:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,307:INFO:Engine successfully changes for model 'lr_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,311:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,312:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,312:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,313:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,313:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,314:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,314:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,317:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,321:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,321:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,322:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,322:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,323:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,323:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,324:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,324:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,324:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,324:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,325:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,325:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,325:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,329:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,330:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,331:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,331:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,332:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,332:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,332:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,335:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,337:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,338:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,338:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,338:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,339:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,340:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,340:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,340:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,340:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,340:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,341:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,341:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,341:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,341:INFO:Engine successfully changes for model 'en_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,345:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,346:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,347:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,347:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,348:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,348:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,350:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,352:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,353:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,353:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,354:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,354:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,354:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,355:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,355:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,355:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,355:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,355:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,356:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,356:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,360:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,361:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,361:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,362:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,363:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,363:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,366:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,368:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,369:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,369:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,370:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,370:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,370:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,371:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,371:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,371:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,371:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,371:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,372:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,372:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,372:INFO:Engine successfully changes for model 'ridge_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,376:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,378:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,378:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,379:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,379:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,382:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,384:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,385:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,385:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,386:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,386:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,386:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,387:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,387:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,387:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,387:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,387:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,387:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,389:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,393:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,394:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,395:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,395:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,396:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,399:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,401:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,402:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,402:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,403:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,403:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,404:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,404:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,404:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,404:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,404:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,405:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,405:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,405:INFO:Engine successfully changes for model 'lasso_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,409:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,411:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,411:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,412:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,414:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,417:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,418:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,418:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,419:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,419:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,420:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,420:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,420:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,420:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,420:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,421:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,421:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,421:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,425:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,427:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,427:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,428:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,430:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,432:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,433:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,433:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,434:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,434:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,434:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,435:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,435:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,435:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,435:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,435:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,436:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,436:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,436:INFO:Engine successfully changes for model 'lar_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,440:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,442:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,443:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,443:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,448:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,450:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,451:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,451:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,451:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,452:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,452:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,453:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,453:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,453:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,453:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,454:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,454:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,454:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,459:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,461:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,462:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,463:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,466:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,468:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,469:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,469:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,469:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,470:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,470:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,470:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,471:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,471:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,471:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,471:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,471:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,471:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,472:INFO:Engine successfully changes for model 'llar_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,476:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,478:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,479:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,483:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,485:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,485:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,486:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,486:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,487:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,487:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,488:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,488:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,488:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,488:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,488:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,488:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,488:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,492:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,495:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,495:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,499:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,501:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,502:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,502:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,503:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,503:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,503:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,504:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,504:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,504:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,504:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,504:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,505:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,505:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,505:INFO:Engine successfully changes for model 'br_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,509:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,512:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,515:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,517:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,518:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,518:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,519:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,519:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,520:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,520:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,520:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,520:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,520:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,521:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,521:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,521:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,525:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,528:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,534:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,536:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,536:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,537:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,537:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,537:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,538:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,538:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,538:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,538:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,538:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,539:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,539:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,540:INFO:Engine successfully changes for model 'huber_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,544:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,550:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,553:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,553:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,553:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,554:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,554:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,555:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,555:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,555:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,555:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,555:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,556:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,556:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,556:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,560:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,567:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,569:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,570:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,570:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,571:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,571:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,572:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,572:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,572:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,572:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,572:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,573:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,573:INFO:Engine successfully changes for model 'par_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,578:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,583:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,585:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,586:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,586:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,587:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,587:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,587:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,588:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,588:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,588:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,588:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,588:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,589:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,589:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,593:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,597:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,599:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,600:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,600:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,601:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,601:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,601:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,602:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,602:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,602:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,602:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,602:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,603:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,603:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,603:INFO:Engine successfully changes for model 'omp_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,607:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,613:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,613:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,615:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,615:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,616:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,616:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,616:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,616:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,616:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,616:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,617:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,617:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,617:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,621:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,628:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,629:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,629:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,630:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,630:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,631:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,631:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,631:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,631:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,631:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,632:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,632:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,632:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,632:INFO:Engine successfully changes for model 'knn_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,635:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,642:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,643:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,643:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,643:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,644:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,644:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,644:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,644:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,644:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,645:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,645:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,649:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,656:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,656:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,656:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,657:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,657:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,658:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,658:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,658:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,658:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,658:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,658:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,659:INFO:Engine successfully changes for model 'dt_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,662:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,675:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,675:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,676:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,677:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,677:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,677:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,677:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,678:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,679:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,679:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,679:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,683:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,691:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,692:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,692:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,692:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,693:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,693:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,693:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,693:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,694:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,694:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,694:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,694:INFO:Engine successfully changes for model 'rf_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,698:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,706:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,707:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,708:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,708:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,709:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,709:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,709:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,709:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,709:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,709:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,714:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,723:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,723:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,724:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,724:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,724:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,724:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,724:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,725:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,725:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,725:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,725:INFO:Engine successfully changes for model 'et_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,729:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,737:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,737:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,737:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,738:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,738:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,738:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,738:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,738:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,738:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,742:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,750:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,750:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,751:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,751:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,751:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,751:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,751:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,751:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,751:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,752:INFO:Engine successfully changes for model 'gbr_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,755:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,763:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,763:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,763:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,763:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,764:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,764:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,764:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,764:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,768:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,776:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,776:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,776:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,776:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,776:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,776:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,776:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,776:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,776:INFO:Engine successfully changes for model 'ada_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,781:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,791:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,791:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,792:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,792:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,792:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,793:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,793:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,797:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,805:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,805:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,805:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,805:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,806:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,806:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,806:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,806:INFO:Engine successfully changes for model 'xgboost_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,810:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,819:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,819:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,819:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,820:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,820:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,820:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,824:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,835:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,836:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,836:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,836:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,836:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,836:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,836:INFO:Engine successfully changes for model 'lightgbm_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,841:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,851:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,851:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,852:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,852:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,852:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,856:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,866:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,867:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,867:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 15:58:55,867:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,867:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,867:INFO:Engine successfully changes for model 'catboost_cds_dt' to 'sklearn'.
2025-05-12 15:58:55,872:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,882:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,882:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,882:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,882:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,887:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,897:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,897:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,897:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,898:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,902:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,912:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,912:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,913:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,913:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,919:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:58:55,929:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,929:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,930:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,930:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 15:58:55,931:INFO:setup() successfully completed in 1.42s...............
2025-05-12 15:58:55,932:INFO:Initializing compare_models()
2025-05-12 15:58:55,932:INFO:compare_models(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, include=None, fold=None, round=4, cross_validation=True, sort=MAE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.time_series.forecasting.oop.TSForecastingExperiment'>}, exclude=None)
2025-05-12 15:58:55,932:INFO:Checking exceptions
2025-05-12 15:58:55,933:INFO:Preparing display monitor
2025-05-12 15:58:55,938:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:713: UserWarning: Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library
  warnings.warn(

2025-05-12 15:58:55,938:INFO:Initializing ARIMA
2025-05-12 15:58:55,938:INFO:Total runtime is 0.0 minutes
2025-05-12 15:58:55,938:INFO:SubProcess create_model() called ==================================
2025-05-12 15:58:55,938:INFO:Initializing create_model()
2025-05-12 15:58:55,938:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:58:55,938:INFO:Checking exceptions
2025-05-12 15:58:55,938:INFO:Importing libraries
2025-05-12 15:58:55,938:INFO:Copying training dataset
2025-05-12 15:58:55,941:INFO:Defining folds
2025-05-12 15:58:55,941:INFO:Declaring metric variables
2025-05-12 15:58:55,941:INFO:Importing untrained model
2025-05-12 15:58:55,943:INFO:ARIMA Imported successfully
2025-05-12 15:58:55,944:INFO:Starting cross validation
2025-05-12 15:58:55,945:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:00,018:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:59:00,020:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:59:00,020:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:59:00,020:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:59:00,020:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:59:00,020:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:59:00,026:INFO:Calculating mean and std
2025-05-12 15:59:00,026:INFO:Creating metrics dataframe
2025-05-12 15:59:00,032:INFO:Uploading results into container
2025-05-12 15:59:00,032:INFO:Uploading model into container now
2025-05-12 15:59:00,032:INFO:_master_model_container: 1
2025-05-12 15:59:00,032:INFO:_display_container: 2
2025-05-12 15:59:00,032:INFO:ARIMA()
2025-05-12 15:59:00,032:INFO:create_model() successfully completed......................................
2025-05-12 15:59:00,172:WARNING:create_model() for ARIMA() raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:00,172:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:00,172:INFO:Initializing create_model()
2025-05-12 15:59:00,172:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:00,172:INFO:Checking exceptions
2025-05-12 15:59:00,172:INFO:Importing libraries
2025-05-12 15:59:00,172:INFO:Copying training dataset
2025-05-12 15:59:00,173:INFO:Defining folds
2025-05-12 15:59:00,173:INFO:Declaring metric variables
2025-05-12 15:59:00,173:INFO:Importing untrained model
2025-05-12 15:59:00,173:INFO:ARIMA Imported successfully
2025-05-12 15:59:00,173:INFO:Starting cross validation
2025-05-12 15:59:00,173:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:03,167:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:59:03,167:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:59:03,167:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:59:03,167:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:59:03,167:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 15:59:03,167:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 15:59:03,182:INFO:Calculating mean and std
2025-05-12 15:59:03,182:INFO:Creating metrics dataframe
2025-05-12 15:59:03,188:INFO:Uploading results into container
2025-05-12 15:59:03,188:INFO:Uploading model into container now
2025-05-12 15:59:03,188:INFO:_master_model_container: 2
2025-05-12 15:59:03,188:INFO:_display_container: 2
2025-05-12 15:59:03,190:INFO:ARIMA()
2025-05-12 15:59:03,190:INFO:create_model() successfully completed......................................
2025-05-12 15:59:03,293:ERROR:create_model() for ARIMA() raised an exception or returned all 0.0:
2025-05-12 15:59:03,293:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:03,293:INFO:Initializing Auto ARIMA
2025-05-12 15:59:03,293:INFO:Total runtime is 0.12258155345916748 minutes
2025-05-12 15:59:03,293:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:03,293:INFO:Initializing create_model()
2025-05-12 15:59:03,293:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=auto_arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:03,293:INFO:Checking exceptions
2025-05-12 15:59:03,293:INFO:Importing libraries
2025-05-12 15:59:03,293:INFO:Copying training dataset
2025-05-12 15:59:03,306:INFO:Defining folds
2025-05-12 15:59:03,306:INFO:Declaring metric variables
2025-05-12 15:59:03,306:INFO:Importing untrained model
2025-05-12 15:59:03,309:INFO:Auto ARIMA Imported successfully
2025-05-12 15:59:03,309:INFO:Starting cross validation
2025-05-12 15:59:03,309:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:06,709:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:59:06,709:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:59:06,799:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:59:06,799:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:59:06,833:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:59:06,833:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:59:06,835:INFO:Calculating mean and std
2025-05-12 15:59:06,835:INFO:Creating metrics dataframe
2025-05-12 15:59:06,839:INFO:Uploading results into container
2025-05-12 15:59:06,839:INFO:Uploading model into container now
2025-05-12 15:59:06,839:INFO:_master_model_container: 3
2025-05-12 15:59:06,839:INFO:_display_container: 2
2025-05-12 15:59:06,841:INFO:AutoARIMA(random_state=123, suppress_warnings=True)
2025-05-12 15:59:06,841:INFO:create_model() successfully completed......................................
2025-05-12 15:59:06,930:WARNING:create_model() for AutoARIMA(random_state=123, suppress_warnings=True) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:06,930:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:06,930:INFO:Initializing create_model()
2025-05-12 15:59:06,930:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=auto_arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:06,930:INFO:Checking exceptions
2025-05-12 15:59:06,930:INFO:Importing libraries
2025-05-12 15:59:06,930:INFO:Copying training dataset
2025-05-12 15:59:06,930:INFO:Defining folds
2025-05-12 15:59:06,930:INFO:Declaring metric variables
2025-05-12 15:59:06,930:INFO:Importing untrained model
2025-05-12 15:59:06,930:INFO:Auto ARIMA Imported successfully
2025-05-12 15:59:06,930:INFO:Starting cross validation
2025-05-12 15:59:06,930:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:10,531:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:59:10,531:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:59:10,574:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:59:10,574:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:59:10,584:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 15:59:10,584:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 15:59:10,600:INFO:Calculating mean and std
2025-05-12 15:59:10,600:INFO:Creating metrics dataframe
2025-05-12 15:59:10,600:INFO:Uploading results into container
2025-05-12 15:59:10,600:INFO:Uploading model into container now
2025-05-12 15:59:10,600:INFO:_master_model_container: 4
2025-05-12 15:59:10,600:INFO:_display_container: 2
2025-05-12 15:59:10,600:INFO:AutoARIMA(random_state=123, suppress_warnings=True)
2025-05-12 15:59:10,600:INFO:create_model() successfully completed......................................
2025-05-12 15:59:10,695:ERROR:create_model() for AutoARIMA(random_state=123, suppress_warnings=True) raised an exception or returned all 0.0:
2025-05-12 15:59:10,695:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:10,695:INFO:Initializing Croston
2025-05-12 15:59:10,695:INFO:Total runtime is 0.2459560434023539 minutes
2025-05-12 15:59:10,695:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:10,695:INFO:Initializing create_model()
2025-05-12 15:59:10,695:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=croston, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:10,695:INFO:Checking exceptions
2025-05-12 15:59:10,695:INFO:Importing libraries
2025-05-12 15:59:10,695:INFO:Copying training dataset
2025-05-12 15:59:10,695:INFO:Defining folds
2025-05-12 15:59:10,695:INFO:Declaring metric variables
2025-05-12 15:59:10,695:INFO:Importing untrained model
2025-05-12 15:59:10,695:INFO:Croston Imported successfully
2025-05-12 15:59:10,695:INFO:Starting cross validation
2025-05-12 15:59:10,711:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:10,743:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:59:10,743:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:59:10,758:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:59:10,758:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:59:10,758:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 15:59:10,775:INFO:Calculating mean and std
2025-05-12 15:59:10,775:INFO:Creating metrics dataframe
2025-05-12 15:59:10,775:INFO:Uploading results into container
2025-05-12 15:59:10,775:INFO:Uploading model into container now
2025-05-12 15:59:10,775:INFO:_master_model_container: 5
2025-05-12 15:59:10,775:INFO:_display_container: 2
2025-05-12 15:59:10,775:INFO:Croston()
2025-05-12 15:59:10,775:INFO:create_model() successfully completed......................................
2025-05-12 15:59:10,870:INFO:SubProcess create_model() end ==================================
2025-05-12 15:59:10,870:INFO:Creating metrics dataframe
2025-05-12 15:59:10,870:INFO:Initializing Linear w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:10,870:INFO:Total runtime is 0.24886207183202108 minutes
2025-05-12 15:59:10,870:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:10,870:INFO:Initializing create_model()
2025-05-12 15:59:10,870:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=lr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:10,870:INFO:Checking exceptions
2025-05-12 15:59:10,870:INFO:Importing libraries
2025-05-12 15:59:10,870:INFO:Copying training dataset
2025-05-12 15:59:10,870:INFO:Defining folds
2025-05-12 15:59:10,870:INFO:Declaring metric variables
2025-05-12 15:59:10,870:INFO:Importing untrained model
2025-05-12 15:59:10,870:INFO:Linear w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:10,870:INFO:Starting cross validation
2025-05-12 15:59:10,870:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:11,801:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:11,814:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:11,823:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:11,894:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:11,894:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:11,912:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:11,912:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:11,928:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:11,928:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:11,943:INFO:Calculating mean and std
2025-05-12 15:59:11,943:INFO:Creating metrics dataframe
2025-05-12 15:59:11,946:INFO:Uploading results into container
2025-05-12 15:59:11,946:INFO:Uploading model into container now
2025-05-12 15:59:11,946:INFO:_master_model_container: 6
2025-05-12 15:59:11,946:INFO:_display_container: 2
2025-05-12 15:59:11,948:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1)
2025-05-12 15:59:11,948:INFO:create_model() successfully completed......................................
2025-05-12 15:59:12,032:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:12,032:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:12,032:INFO:Initializing create_model()
2025-05-12 15:59:12,032:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=lr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:12,032:INFO:Checking exceptions
2025-05-12 15:59:12,034:INFO:Importing libraries
2025-05-12 15:59:12,034:INFO:Copying training dataset
2025-05-12 15:59:12,036:INFO:Defining folds
2025-05-12 15:59:12,036:INFO:Declaring metric variables
2025-05-12 15:59:12,036:INFO:Importing untrained model
2025-05-12 15:59:12,038:INFO:Linear w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:12,040:INFO:Starting cross validation
2025-05-12 15:59:12,040:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:12,893:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:12,929:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:12,941:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:13,004:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:13,004:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:13,020:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:13,020:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:13,052:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:13,052:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:13,068:INFO:Calculating mean and std
2025-05-12 15:59:13,068:INFO:Creating metrics dataframe
2025-05-12 15:59:13,076:INFO:Uploading results into container
2025-05-12 15:59:13,076:INFO:Uploading model into container now
2025-05-12 15:59:13,076:INFO:_master_model_container: 7
2025-05-12 15:59:13,076:INFO:_display_container: 2
2025-05-12 15:59:13,078:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1)
2025-05-12 15:59:13,078:INFO:create_model() successfully completed......................................
2025-05-12 15:59:13,167:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:13,167:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:13,167:INFO:Initializing Elastic Net w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:13,167:INFO:Total runtime is 0.28714956442515055 minutes
2025-05-12 15:59:13,167:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:13,167:INFO:Initializing create_model()
2025-05-12 15:59:13,167:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=en_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:13,167:INFO:Checking exceptions
2025-05-12 15:59:13,169:INFO:Importing libraries
2025-05-12 15:59:13,169:INFO:Copying training dataset
2025-05-12 15:59:13,171:INFO:Defining folds
2025-05-12 15:59:13,171:INFO:Declaring metric variables
2025-05-12 15:59:13,171:INFO:Importing untrained model
2025-05-12 15:59:13,171:INFO:Elastic Net w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:13,171:INFO:Starting cross validation
2025-05-12 15:59:13,171:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:14,112:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:14,112:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:14,128:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:14,194:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:14,194:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:14,236:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:14,236:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:14,252:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:14,252:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:14,268:INFO:Calculating mean and std
2025-05-12 15:59:14,268:INFO:Creating metrics dataframe
2025-05-12 15:59:14,286:INFO:Uploading results into container
2025-05-12 15:59:14,286:INFO:Uploading model into container now
2025-05-12 15:59:14,288:INFO:_master_model_container: 8
2025-05-12 15:59:14,288:INFO:_display_container: 2
2025-05-12 15:59:14,292:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1)
2025-05-12 15:59:14,292:INFO:create_model() successfully completed......................................
2025-05-12 15:59:14,379:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:14,379:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:14,379:INFO:Initializing create_model()
2025-05-12 15:59:14,379:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=en_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:14,379:INFO:Checking exceptions
2025-05-12 15:59:14,379:INFO:Importing libraries
2025-05-12 15:59:14,379:INFO:Copying training dataset
2025-05-12 15:59:14,379:INFO:Defining folds
2025-05-12 15:59:14,379:INFO:Declaring metric variables
2025-05-12 15:59:14,379:INFO:Importing untrained model
2025-05-12 15:59:14,379:INFO:Elastic Net w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:14,386:INFO:Starting cross validation
2025-05-12 15:59:14,388:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:15,265:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:15,283:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:15,298:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 15:59:15,352:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,352:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,408:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,408:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,408:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,408:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,424:INFO:Calculating mean and std
2025-05-12 15:59:15,424:INFO:Creating metrics dataframe
2025-05-12 15:59:15,424:INFO:Uploading results into container
2025-05-12 15:59:15,424:INFO:Uploading model into container now
2025-05-12 15:59:15,424:INFO:_master_model_container: 9
2025-05-12 15:59:15,424:INFO:_display_container: 2
2025-05-12 15:59:15,424:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1)
2025-05-12 15:59:15,424:INFO:create_model() successfully completed......................................
2025-05-12 15:59:15,502:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:15,502:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:15,502:INFO:Initializing Ridge w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:15,502:INFO:Total runtime is 0.32607853412628174 minutes
2025-05-12 15:59:15,502:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:15,502:INFO:Initializing create_model()
2025-05-12 15:59:15,502:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=ridge_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:15,502:INFO:Checking exceptions
2025-05-12 15:59:15,502:INFO:Importing libraries
2025-05-12 15:59:15,502:INFO:Copying training dataset
2025-05-12 15:59:15,502:INFO:Defining folds
2025-05-12 15:59:15,502:INFO:Declaring metric variables
2025-05-12 15:59:15,502:INFO:Importing untrained model
2025-05-12 15:59:15,502:INFO:Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:15,518:INFO:Starting cross validation
2025-05-12 15:59:15,518:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:15,649:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,649:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,655:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,655:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,661:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,661:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,677:INFO:Calculating mean and std
2025-05-12 15:59:15,677:INFO:Creating metrics dataframe
2025-05-12 15:59:15,689:INFO:Uploading results into container
2025-05-12 15:59:15,689:INFO:Uploading model into container now
2025-05-12 15:59:15,689:INFO:_master_model_container: 10
2025-05-12 15:59:15,689:INFO:_display_container: 2
2025-05-12 15:59:15,693:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1)
2025-05-12 15:59:15,693:INFO:create_model() successfully completed......................................
2025-05-12 15:59:15,782:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:15,782:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:15,782:INFO:Initializing create_model()
2025-05-12 15:59:15,782:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=ridge_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:15,782:INFO:Checking exceptions
2025-05-12 15:59:15,782:INFO:Importing libraries
2025-05-12 15:59:15,782:INFO:Copying training dataset
2025-05-12 15:59:15,784:INFO:Defining folds
2025-05-12 15:59:15,784:INFO:Declaring metric variables
2025-05-12 15:59:15,784:INFO:Importing untrained model
2025-05-12 15:59:15,784:INFO:Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:15,788:INFO:Starting cross validation
2025-05-12 15:59:15,790:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:15,885:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,885:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,915:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,915:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,915:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:15,915:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:15,931:INFO:Calculating mean and std
2025-05-12 15:59:15,933:INFO:Creating metrics dataframe
2025-05-12 15:59:15,933:INFO:Uploading results into container
2025-05-12 15:59:15,933:INFO:Uploading model into container now
2025-05-12 15:59:15,933:INFO:_master_model_container: 11
2025-05-12 15:59:15,933:INFO:_display_container: 2
2025-05-12 15:59:15,933:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1)
2025-05-12 15:59:15,933:INFO:create_model() successfully completed......................................
2025-05-12 15:59:16,011:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:16,011:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:16,011:INFO:Initializing Lasso w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:16,011:INFO:Total runtime is 0.33454737265904744 minutes
2025-05-12 15:59:16,011:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:16,011:INFO:Initializing create_model()
2025-05-12 15:59:16,027:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=lasso_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:16,027:INFO:Checking exceptions
2025-05-12 15:59:16,027:INFO:Importing libraries
2025-05-12 15:59:16,027:INFO:Copying training dataset
2025-05-12 15:59:16,027:INFO:Defining folds
2025-05-12 15:59:16,027:INFO:Declaring metric variables
2025-05-12 15:59:16,027:INFO:Importing untrained model
2025-05-12 15:59:16,027:INFO:Lasso w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:16,027:INFO:Starting cross validation
2025-05-12 15:59:16,027:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:16,124:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,124:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,144:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,144:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,169:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,169:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,185:INFO:Calculating mean and std
2025-05-12 15:59:16,185:INFO:Creating metrics dataframe
2025-05-12 15:59:16,185:INFO:Uploading results into container
2025-05-12 15:59:16,185:INFO:Uploading model into container now
2025-05-12 15:59:16,185:INFO:_master_model_container: 12
2025-05-12 15:59:16,185:INFO:_display_container: 2
2025-05-12 15:59:16,185:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1)
2025-05-12 15:59:16,185:INFO:create_model() successfully completed......................................
2025-05-12 15:59:16,281:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:16,281:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:16,281:INFO:Initializing create_model()
2025-05-12 15:59:16,281:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=lasso_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:16,281:INFO:Checking exceptions
2025-05-12 15:59:16,281:INFO:Importing libraries
2025-05-12 15:59:16,281:INFO:Copying training dataset
2025-05-12 15:59:16,281:INFO:Defining folds
2025-05-12 15:59:16,281:INFO:Declaring metric variables
2025-05-12 15:59:16,281:INFO:Importing untrained model
2025-05-12 15:59:16,281:INFO:Lasso w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:16,281:INFO:Starting cross validation
2025-05-12 15:59:16,281:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:16,391:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,391:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,423:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,423:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,423:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,423:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,438:INFO:Calculating mean and std
2025-05-12 15:59:16,438:INFO:Creating metrics dataframe
2025-05-12 15:59:16,438:INFO:Uploading results into container
2025-05-12 15:59:16,438:INFO:Uploading model into container now
2025-05-12 15:59:16,438:INFO:_master_model_container: 13
2025-05-12 15:59:16,438:INFO:_display_container: 2
2025-05-12 15:59:16,438:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1)
2025-05-12 15:59:16,438:INFO:create_model() successfully completed......................................
2025-05-12 15:59:16,533:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:16,533:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:16,533:INFO:Initializing Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:16,533:INFO:Total runtime is 0.3432546655337016 minutes
2025-05-12 15:59:16,533:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:16,533:INFO:Initializing create_model()
2025-05-12 15:59:16,533:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=llar_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:16,533:INFO:Checking exceptions
2025-05-12 15:59:16,533:INFO:Importing libraries
2025-05-12 15:59:16,533:INFO:Copying training dataset
2025-05-12 15:59:16,533:INFO:Defining folds
2025-05-12 15:59:16,533:INFO:Declaring metric variables
2025-05-12 15:59:16,533:INFO:Importing untrained model
2025-05-12 15:59:16,533:INFO:Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:16,533:INFO:Starting cross validation
2025-05-12 15:59:16,533:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:16,628:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,628:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,664:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,664:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,668:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,668:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,676:INFO:Calculating mean and std
2025-05-12 15:59:16,678:INFO:Creating metrics dataframe
2025-05-12 15:59:16,678:INFO:Uploading results into container
2025-05-12 15:59:16,678:INFO:Uploading model into container now
2025-05-12 15:59:16,678:INFO:_master_model_container: 14
2025-05-12 15:59:16,678:INFO:_display_container: 2
2025-05-12 15:59:16,678:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1)
2025-05-12 15:59:16,678:INFO:create_model() successfully completed......................................
2025-05-12 15:59:16,770:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:16,770:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:16,770:INFO:Initializing create_model()
2025-05-12 15:59:16,770:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=llar_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:16,770:INFO:Checking exceptions
2025-05-12 15:59:16,770:INFO:Importing libraries
2025-05-12 15:59:16,770:INFO:Copying training dataset
2025-05-12 15:59:16,770:INFO:Defining folds
2025-05-12 15:59:16,770:INFO:Declaring metric variables
2025-05-12 15:59:16,770:INFO:Importing untrained model
2025-05-12 15:59:16,770:INFO:Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:16,770:INFO:Starting cross validation
2025-05-12 15:59:16,770:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:16,865:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,865:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,913:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,913:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,929:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:16,929:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:16,944:INFO:Calculating mean and std
2025-05-12 15:59:16,944:INFO:Creating metrics dataframe
2025-05-12 15:59:16,944:INFO:Uploading results into container
2025-05-12 15:59:16,944:INFO:Uploading model into container now
2025-05-12 15:59:16,944:INFO:_master_model_container: 15
2025-05-12 15:59:16,944:INFO:_display_container: 2
2025-05-12 15:59:16,944:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1)
2025-05-12 15:59:16,944:INFO:create_model() successfully completed......................................
2025-05-12 15:59:17,040:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:17,040:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:17,040:INFO:Initializing Bayesian Ridge w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:17,040:INFO:Total runtime is 0.3517070293426514 minutes
2025-05-12 15:59:17,040:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:17,040:INFO:Initializing create_model()
2025-05-12 15:59:17,040:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=br_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:17,040:INFO:Checking exceptions
2025-05-12 15:59:17,040:INFO:Importing libraries
2025-05-12 15:59:17,040:INFO:Copying training dataset
2025-05-12 15:59:17,040:INFO:Defining folds
2025-05-12 15:59:17,040:INFO:Declaring metric variables
2025-05-12 15:59:17,040:INFO:Importing untrained model
2025-05-12 15:59:17,040:INFO:Bayesian Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:17,040:INFO:Starting cross validation
2025-05-12 15:59:17,040:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:17,135:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,135:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,183:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,183:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,183:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,199:INFO:Calculating mean and std
2025-05-12 15:59:17,199:INFO:Creating metrics dataframe
2025-05-12 15:59:17,199:INFO:Uploading results into container
2025-05-12 15:59:17,199:INFO:Uploading model into container now
2025-05-12 15:59:17,199:INFO:_master_model_container: 16
2025-05-12 15:59:17,199:INFO:_display_container: 2
2025-05-12 15:59:17,199:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1)
2025-05-12 15:59:17,199:INFO:create_model() successfully completed......................................
2025-05-12 15:59:17,278:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:17,278:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:17,278:INFO:Initializing create_model()
2025-05-12 15:59:17,278:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=br_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:17,278:INFO:Checking exceptions
2025-05-12 15:59:17,278:INFO:Importing libraries
2025-05-12 15:59:17,278:INFO:Copying training dataset
2025-05-12 15:59:17,278:INFO:Defining folds
2025-05-12 15:59:17,278:INFO:Declaring metric variables
2025-05-12 15:59:17,278:INFO:Importing untrained model
2025-05-12 15:59:17,278:INFO:Bayesian Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:17,294:INFO:Starting cross validation
2025-05-12 15:59:17,294:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:17,373:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,373:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,437:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,437:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,437:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,437:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,452:INFO:Calculating mean and std
2025-05-12 15:59:17,452:INFO:Creating metrics dataframe
2025-05-12 15:59:17,452:INFO:Uploading results into container
2025-05-12 15:59:17,452:INFO:Uploading model into container now
2025-05-12 15:59:17,452:INFO:_master_model_container: 17
2025-05-12 15:59:17,452:INFO:_display_container: 2
2025-05-12 15:59:17,452:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1)
2025-05-12 15:59:17,452:INFO:create_model() successfully completed......................................
2025-05-12 15:59:17,547:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:17,547:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:17,547:INFO:Initializing Huber w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:17,547:INFO:Total runtime is 0.36016039450963344 minutes
2025-05-12 15:59:17,547:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:17,547:INFO:Initializing create_model()
2025-05-12 15:59:17,547:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=huber_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:17,547:INFO:Checking exceptions
2025-05-12 15:59:17,547:INFO:Importing libraries
2025-05-12 15:59:17,547:INFO:Copying training dataset
2025-05-12 15:59:17,547:INFO:Defining folds
2025-05-12 15:59:17,547:INFO:Declaring metric variables
2025-05-12 15:59:17,547:INFO:Importing untrained model
2025-05-12 15:59:17,547:INFO:Huber w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:17,547:INFO:Starting cross validation
2025-05-12 15:59:17,547:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:17,643:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,643:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,691:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,691:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,691:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,691:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,707:INFO:Calculating mean and std
2025-05-12 15:59:17,707:INFO:Creating metrics dataframe
2025-05-12 15:59:17,707:INFO:Uploading results into container
2025-05-12 15:59:17,707:INFO:Uploading model into container now
2025-05-12 15:59:17,707:INFO:_master_model_container: 18
2025-05-12 15:59:17,707:INFO:_display_container: 2
2025-05-12 15:59:17,707:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1)
2025-05-12 15:59:17,707:INFO:create_model() successfully completed......................................
2025-05-12 15:59:17,802:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:17,802:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:17,802:INFO:Initializing create_model()
2025-05-12 15:59:17,802:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=huber_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:17,802:INFO:Checking exceptions
2025-05-12 15:59:17,802:INFO:Importing libraries
2025-05-12 15:59:17,802:INFO:Copying training dataset
2025-05-12 15:59:17,802:INFO:Defining folds
2025-05-12 15:59:17,802:INFO:Declaring metric variables
2025-05-12 15:59:17,802:INFO:Importing untrained model
2025-05-12 15:59:17,802:INFO:Huber w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:17,802:INFO:Starting cross validation
2025-05-12 15:59:17,802:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:17,913:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,913:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,944:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,944:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,944:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:17,944:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:17,960:INFO:Calculating mean and std
2025-05-12 15:59:17,960:INFO:Creating metrics dataframe
2025-05-12 15:59:17,960:INFO:Uploading results into container
2025-05-12 15:59:17,960:INFO:Uploading model into container now
2025-05-12 15:59:17,960:INFO:_master_model_container: 19
2025-05-12 15:59:17,960:INFO:_display_container: 2
2025-05-12 15:59:17,960:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1)
2025-05-12 15:59:17,960:INFO:create_model() successfully completed......................................
2025-05-12 15:59:18,039:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:18,039:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:18,039:INFO:Initializing Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:18,039:INFO:Total runtime is 0.368356998761495 minutes
2025-05-12 15:59:18,039:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:18,039:INFO:Initializing create_model()
2025-05-12 15:59:18,039:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=omp_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:18,039:INFO:Checking exceptions
2025-05-12 15:59:18,039:INFO:Importing libraries
2025-05-12 15:59:18,039:INFO:Copying training dataset
2025-05-12 15:59:18,055:INFO:Defining folds
2025-05-12 15:59:18,055:INFO:Declaring metric variables
2025-05-12 15:59:18,055:INFO:Importing untrained model
2025-05-12 15:59:18,055:INFO:Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:18,055:INFO:Starting cross validation
2025-05-12 15:59:18,055:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:18,135:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,135:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,167:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,167:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,182:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,182:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,199:INFO:Calculating mean and std
2025-05-12 15:59:18,199:INFO:Creating metrics dataframe
2025-05-12 15:59:18,199:INFO:Uploading results into container
2025-05-12 15:59:18,199:INFO:Uploading model into container now
2025-05-12 15:59:18,199:INFO:_master_model_container: 20
2025-05-12 15:59:18,199:INFO:_display_container: 2
2025-05-12 15:59:18,199:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1)
2025-05-12 15:59:18,199:INFO:create_model() successfully completed......................................
2025-05-12 15:59:18,278:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:18,278:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:18,278:INFO:Initializing create_model()
2025-05-12 15:59:18,278:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=omp_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:18,278:INFO:Checking exceptions
2025-05-12 15:59:18,278:INFO:Importing libraries
2025-05-12 15:59:18,278:INFO:Copying training dataset
2025-05-12 15:59:18,278:INFO:Defining folds
2025-05-12 15:59:18,278:INFO:Declaring metric variables
2025-05-12 15:59:18,278:INFO:Importing untrained model
2025-05-12 15:59:18,294:INFO:Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:18,294:INFO:Starting cross validation
2025-05-12 15:59:18,294:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:18,405:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,405:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,421:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,421:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,437:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,437:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,453:INFO:Calculating mean and std
2025-05-12 15:59:18,453:INFO:Creating metrics dataframe
2025-05-12 15:59:18,453:INFO:Uploading results into container
2025-05-12 15:59:18,453:INFO:Uploading model into container now
2025-05-12 15:59:18,453:INFO:_master_model_container: 21
2025-05-12 15:59:18,453:INFO:_display_container: 2
2025-05-12 15:59:18,453:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1)
2025-05-12 15:59:18,453:INFO:create_model() successfully completed......................................
2025-05-12 15:59:18,549:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:18,549:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:18,549:INFO:Initializing K Neighbors w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:18,549:INFO:Total runtime is 0.3768454313278199 minutes
2025-05-12 15:59:18,549:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:18,549:INFO:Initializing create_model()
2025-05-12 15:59:18,549:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=knn_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:18,549:INFO:Checking exceptions
2025-05-12 15:59:18,549:INFO:Importing libraries
2025-05-12 15:59:18,549:INFO:Copying training dataset
2025-05-12 15:59:18,549:INFO:Defining folds
2025-05-12 15:59:18,549:INFO:Declaring metric variables
2025-05-12 15:59:18,549:INFO:Importing untrained model
2025-05-12 15:59:18,549:INFO:K Neighbors w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:18,549:INFO:Starting cross validation
2025-05-12 15:59:18,549:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:18,660:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,660:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,709:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,709:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,711:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,711:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,723:INFO:Calculating mean and std
2025-05-12 15:59:18,723:INFO:Creating metrics dataframe
2025-05-12 15:59:18,723:INFO:Uploading results into container
2025-05-12 15:59:18,723:INFO:Uploading model into container now
2025-05-12 15:59:18,723:INFO:_master_model_container: 22
2025-05-12 15:59:18,723:INFO:_display_container: 2
2025-05-12 15:59:18,723:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1)
2025-05-12 15:59:18,723:INFO:create_model() successfully completed......................................
2025-05-12 15:59:18,803:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:18,803:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:18,803:INFO:Initializing create_model()
2025-05-12 15:59:18,803:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=knn_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:18,803:INFO:Checking exceptions
2025-05-12 15:59:18,803:INFO:Importing libraries
2025-05-12 15:59:18,803:INFO:Copying training dataset
2025-05-12 15:59:18,803:INFO:Defining folds
2025-05-12 15:59:18,803:INFO:Declaring metric variables
2025-05-12 15:59:18,803:INFO:Importing untrained model
2025-05-12 15:59:18,819:INFO:K Neighbors w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:18,819:INFO:Starting cross validation
2025-05-12 15:59:18,819:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:18,899:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,899:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,946:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,946:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,946:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 15:59:18,946:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:18,962:INFO:Calculating mean and std
2025-05-12 15:59:18,962:INFO:Creating metrics dataframe
2025-05-12 15:59:18,962:INFO:Uploading results into container
2025-05-12 15:59:18,962:INFO:Uploading model into container now
2025-05-12 15:59:18,962:INFO:_master_model_container: 23
2025-05-12 15:59:18,962:INFO:_display_container: 2
2025-05-12 15:59:18,962:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1)
2025-05-12 15:59:18,962:INFO:create_model() successfully completed......................................
2025-05-12 15:59:19,042:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:19,042:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:19,042:INFO:Initializing Decision Tree w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:19,042:INFO:Total runtime is 0.38507451216379807 minutes
2025-05-12 15:59:19,042:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:19,042:INFO:Initializing create_model()
2025-05-12 15:59:19,042:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=dt_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:19,042:INFO:Checking exceptions
2025-05-12 15:59:19,042:INFO:Importing libraries
2025-05-12 15:59:19,042:INFO:Copying training dataset
2025-05-12 15:59:19,058:INFO:Defining folds
2025-05-12 15:59:19,058:INFO:Declaring metric variables
2025-05-12 15:59:19,058:INFO:Importing untrained model
2025-05-12 15:59:19,058:INFO:Decision Tree w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:19,058:INFO:Starting cross validation
2025-05-12 15:59:19,058:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:19,138:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,138:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,185:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,185:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,185:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,185:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,201:INFO:Calculating mean and std
2025-05-12 15:59:19,201:INFO:Creating metrics dataframe
2025-05-12 15:59:19,201:INFO:Uploading results into container
2025-05-12 15:59:19,201:INFO:Uploading model into container now
2025-05-12 15:59:19,201:INFO:_master_model_container: 24
2025-05-12 15:59:19,201:INFO:_display_container: 2
2025-05-12 15:59:19,201:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:59:19,201:INFO:create_model() successfully completed......................................
2025-05-12 15:59:19,281:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:19,281:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:19,281:INFO:Initializing create_model()
2025-05-12 15:59:19,281:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=dt_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:19,281:INFO:Checking exceptions
2025-05-12 15:59:19,281:INFO:Importing libraries
2025-05-12 15:59:19,281:INFO:Copying training dataset
2025-05-12 15:59:19,281:INFO:Defining folds
2025-05-12 15:59:19,281:INFO:Declaring metric variables
2025-05-12 15:59:19,281:INFO:Importing untrained model
2025-05-12 15:59:19,281:INFO:Decision Tree w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:19,281:INFO:Starting cross validation
2025-05-12 15:59:19,297:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:19,376:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,376:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,424:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,424:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,424:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,424:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,440:INFO:Calculating mean and std
2025-05-12 15:59:19,440:INFO:Creating metrics dataframe
2025-05-12 15:59:19,440:INFO:Uploading results into container
2025-05-12 15:59:19,440:INFO:Uploading model into container now
2025-05-12 15:59:19,440:INFO:_master_model_container: 25
2025-05-12 15:59:19,440:INFO:_display_container: 2
2025-05-12 15:59:19,440:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:59:19,440:INFO:create_model() successfully completed......................................
2025-05-12 15:59:19,519:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:19,519:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:19,519:INFO:Initializing Random Forest w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:19,519:INFO:Total runtime is 0.39302846590677903 minutes
2025-05-12 15:59:19,519:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:19,519:INFO:Initializing create_model()
2025-05-12 15:59:19,519:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=rf_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:19,519:INFO:Checking exceptions
2025-05-12 15:59:19,519:INFO:Importing libraries
2025-05-12 15:59:19,519:INFO:Copying training dataset
2025-05-12 15:59:19,519:INFO:Defining folds
2025-05-12 15:59:19,519:INFO:Declaring metric variables
2025-05-12 15:59:19,519:INFO:Importing untrained model
2025-05-12 15:59:19,535:INFO:Random Forest w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:19,535:INFO:Starting cross validation
2025-05-12 15:59:19,535:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:19,633:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,633:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,679:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,679:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,679:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,679:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,695:INFO:Calculating mean and std
2025-05-12 15:59:19,695:INFO:Creating metrics dataframe
2025-05-12 15:59:19,695:INFO:Uploading results into container
2025-05-12 15:59:19,695:INFO:Uploading model into container now
2025-05-12 15:59:19,695:INFO:_master_model_container: 26
2025-05-12 15:59:19,695:INFO:_display_container: 2
2025-05-12 15:59:19,695:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:59:19,695:INFO:create_model() successfully completed......................................
2025-05-12 15:59:19,790:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:19,790:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:19,790:INFO:Initializing create_model()
2025-05-12 15:59:19,790:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=rf_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:19,790:INFO:Checking exceptions
2025-05-12 15:59:19,790:INFO:Importing libraries
2025-05-12 15:59:19,790:INFO:Copying training dataset
2025-05-12 15:59:19,790:INFO:Defining folds
2025-05-12 15:59:19,790:INFO:Declaring metric variables
2025-05-12 15:59:19,790:INFO:Importing untrained model
2025-05-12 15:59:19,790:INFO:Random Forest w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:19,790:INFO:Starting cross validation
2025-05-12 15:59:19,790:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:19,886:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,886:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,933:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,933:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,933:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:19,933:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:19,949:INFO:Calculating mean and std
2025-05-12 15:59:19,949:INFO:Creating metrics dataframe
2025-05-12 15:59:19,949:INFO:Uploading results into container
2025-05-12 15:59:19,949:INFO:Uploading model into container now
2025-05-12 15:59:19,949:INFO:_master_model_container: 27
2025-05-12 15:59:19,949:INFO:_display_container: 2
2025-05-12 15:59:19,949:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:59:19,949:INFO:create_model() successfully completed......................................
2025-05-12 15:59:20,061:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:20,061:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:20,061:INFO:Initializing Extra Trees w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:20,061:INFO:Total runtime is 0.40206160147984826 minutes
2025-05-12 15:59:20,062:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:20,062:INFO:Initializing create_model()
2025-05-12 15:59:20,062:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=et_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:20,062:INFO:Checking exceptions
2025-05-12 15:59:20,062:INFO:Importing libraries
2025-05-12 15:59:20,062:INFO:Copying training dataset
2025-05-12 15:59:20,064:INFO:Defining folds
2025-05-12 15:59:20,064:INFO:Declaring metric variables
2025-05-12 15:59:20,064:INFO:Importing untrained model
2025-05-12 15:59:20,065:INFO:Extra Trees w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:20,067:INFO:Starting cross validation
2025-05-12 15:59:20,068:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:20,155:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,155:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,187:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,187:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,187:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,187:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,203:INFO:Calculating mean and std
2025-05-12 15:59:20,203:INFO:Creating metrics dataframe
2025-05-12 15:59:20,203:INFO:Uploading results into container
2025-05-12 15:59:20,203:INFO:Uploading model into container now
2025-05-12 15:59:20,203:INFO:_master_model_container: 28
2025-05-12 15:59:20,203:INFO:_display_container: 2
2025-05-12 15:59:20,203:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:59:20,203:INFO:create_model() successfully completed......................................
2025-05-12 15:59:20,298:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:20,298:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:20,298:INFO:Initializing create_model()
2025-05-12 15:59:20,298:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=et_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:20,298:INFO:Checking exceptions
2025-05-12 15:59:20,298:INFO:Importing libraries
2025-05-12 15:59:20,298:INFO:Copying training dataset
2025-05-12 15:59:20,298:INFO:Defining folds
2025-05-12 15:59:20,298:INFO:Declaring metric variables
2025-05-12 15:59:20,298:INFO:Importing untrained model
2025-05-12 15:59:20,298:INFO:Extra Trees w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:20,298:INFO:Starting cross validation
2025-05-12 15:59:20,298:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:20,394:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,394:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,426:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,426:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,426:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,426:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,442:INFO:Calculating mean and std
2025-05-12 15:59:20,442:INFO:Creating metrics dataframe
2025-05-12 15:59:20,442:INFO:Uploading results into container
2025-05-12 15:59:20,442:INFO:Uploading model into container now
2025-05-12 15:59:20,442:INFO:_master_model_container: 29
2025-05-12 15:59:20,442:INFO:_display_container: 2
2025-05-12 15:59:20,442:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:59:20,442:INFO:create_model() successfully completed......................................
2025-05-12 15:59:20,521:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:20,521:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:20,521:INFO:Initializing Gradient Boosting w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:20,521:INFO:Total runtime is 0.4097155173619589 minutes
2025-05-12 15:59:20,521:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:20,521:INFO:Initializing create_model()
2025-05-12 15:59:20,521:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=gbr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:20,521:INFO:Checking exceptions
2025-05-12 15:59:20,521:INFO:Importing libraries
2025-05-12 15:59:20,521:INFO:Copying training dataset
2025-05-12 15:59:20,521:INFO:Defining folds
2025-05-12 15:59:20,521:INFO:Declaring metric variables
2025-05-12 15:59:20,521:INFO:Importing untrained model
2025-05-12 15:59:20,521:INFO:Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:20,521:INFO:Starting cross validation
2025-05-12 15:59:20,521:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:20,615:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,615:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,679:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,679:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,679:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,695:INFO:Calculating mean and std
2025-05-12 15:59:20,695:INFO:Creating metrics dataframe
2025-05-12 15:59:20,695:INFO:Uploading results into container
2025-05-12 15:59:20,695:INFO:Uploading model into container now
2025-05-12 15:59:20,695:INFO:_master_model_container: 30
2025-05-12 15:59:20,695:INFO:_display_container: 2
2025-05-12 15:59:20,695:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:59:20,695:INFO:create_model() successfully completed......................................
2025-05-12 15:59:20,790:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:20,790:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:20,790:INFO:Initializing create_model()
2025-05-12 15:59:20,790:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=gbr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:20,790:INFO:Checking exceptions
2025-05-12 15:59:20,790:INFO:Importing libraries
2025-05-12 15:59:20,790:INFO:Copying training dataset
2025-05-12 15:59:20,790:INFO:Defining folds
2025-05-12 15:59:20,790:INFO:Declaring metric variables
2025-05-12 15:59:20,790:INFO:Importing untrained model
2025-05-12 15:59:20,790:INFO:Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:20,790:INFO:Starting cross validation
2025-05-12 15:59:20,790:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:20,885:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,885:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,933:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,933:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,933:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:20,933:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:20,949:INFO:Calculating mean and std
2025-05-12 15:59:20,949:INFO:Creating metrics dataframe
2025-05-12 15:59:20,949:INFO:Uploading results into container
2025-05-12 15:59:20,949:INFO:Uploading model into container now
2025-05-12 15:59:20,949:INFO:_master_model_container: 31
2025-05-12 15:59:20,949:INFO:_display_container: 2
2025-05-12 15:59:20,949:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:59:20,949:INFO:create_model() successfully completed......................................
2025-05-12 15:59:21,044:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:21,044:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:21,044:INFO:Initializing AdaBoost w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:21,044:INFO:Total runtime is 0.418434762954712 minutes
2025-05-12 15:59:21,044:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:21,044:INFO:Initializing create_model()
2025-05-12 15:59:21,044:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=ada_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:21,044:INFO:Checking exceptions
2025-05-12 15:59:21,044:INFO:Importing libraries
2025-05-12 15:59:21,044:INFO:Copying training dataset
2025-05-12 15:59:21,044:INFO:Defining folds
2025-05-12 15:59:21,044:INFO:Declaring metric variables
2025-05-12 15:59:21,044:INFO:Importing untrained model
2025-05-12 15:59:21,044:INFO:AdaBoost w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:21,044:INFO:Starting cross validation
2025-05-12 15:59:21,044:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:21,143:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,143:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:21,171:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,171:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:21,171:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,171:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:21,186:INFO:Calculating mean and std
2025-05-12 15:59:21,186:INFO:Creating metrics dataframe
2025-05-12 15:59:21,186:INFO:Uploading results into container
2025-05-12 15:59:21,186:INFO:Uploading model into container now
2025-05-12 15:59:21,186:INFO:_master_model_container: 32
2025-05-12 15:59:21,186:INFO:_display_container: 2
2025-05-12 15:59:21,186:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:59:21,186:INFO:create_model() successfully completed......................................
2025-05-12 15:59:21,265:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:21,265:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:21,265:INFO:Initializing create_model()
2025-05-12 15:59:21,265:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=ada_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:21,265:INFO:Checking exceptions
2025-05-12 15:59:21,265:INFO:Importing libraries
2025-05-12 15:59:21,265:INFO:Copying training dataset
2025-05-12 15:59:21,281:INFO:Defining folds
2025-05-12 15:59:21,281:INFO:Declaring metric variables
2025-05-12 15:59:21,281:INFO:Importing untrained model
2025-05-12 15:59:21,281:INFO:AdaBoost w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:21,281:INFO:Starting cross validation
2025-05-12 15:59:21,281:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:21,361:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,361:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:21,408:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,408:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:21,408:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,408:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 15:59:21,425:INFO:Calculating mean and std
2025-05-12 15:59:21,425:INFO:Creating metrics dataframe
2025-05-12 15:59:21,425:INFO:Uploading results into container
2025-05-12 15:59:21,425:INFO:Uploading model into container now
2025-05-12 15:59:21,425:INFO:_master_model_container: 33
2025-05-12 15:59:21,425:INFO:_display_container: 2
2025-05-12 15:59:21,425:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1)
2025-05-12 15:59:21,425:INFO:create_model() successfully completed......................................
2025-05-12 15:59:21,503:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:21,503:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:21,503:INFO:Initializing Light Gradient Boosting w/ Cond. Deseasonalize & Detrending
2025-05-12 15:59:21,503:INFO:Total runtime is 0.4260944565137228 minutes
2025-05-12 15:59:21,503:INFO:SubProcess create_model() called ==================================
2025-05-12 15:59:21,503:INFO:Initializing create_model()
2025-05-12 15:59:21,503:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=lightgbm_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:21,503:INFO:Checking exceptions
2025-05-12 15:59:21,503:INFO:Importing libraries
2025-05-12 15:59:21,503:INFO:Copying training dataset
2025-05-12 15:59:21,503:INFO:Defining folds
2025-05-12 15:59:21,503:INFO:Declaring metric variables
2025-05-12 15:59:21,503:INFO:Importing untrained model
2025-05-12 15:59:21,503:INFO:Light Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:21,503:INFO:Starting cross validation
2025-05-12 15:59:21,519:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:21,694:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,694:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:59:21,773:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,773:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:59:21,773:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:21,773:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:59:21,789:INFO:Calculating mean and std
2025-05-12 15:59:21,789:INFO:Creating metrics dataframe
2025-05-12 15:59:21,789:INFO:Uploading results into container
2025-05-12 15:59:21,789:INFO:Uploading model into container now
2025-05-12 15:59:21,789:INFO:_master_model_container: 34
2025-05-12 15:59:21,789:INFO:_display_container: 2
2025-05-12 15:59:21,789:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:59:21,789:INFO:create_model() successfully completed......................................
2025-05-12 15:59:21,886:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 15:59:21,886:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:21,886:INFO:Initializing create_model()
2025-05-12 15:59:21,886:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=lightgbm_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D7F49703A0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:21,886:INFO:Checking exceptions
2025-05-12 15:59:21,888:INFO:Importing libraries
2025-05-12 15:59:21,888:INFO:Copying training dataset
2025-05-12 15:59:21,890:INFO:Defining folds
2025-05-12 15:59:21,890:INFO:Declaring metric variables
2025-05-12 15:59:21,890:INFO:Importing untrained model
2025-05-12 15:59:21,892:INFO:Light Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 15:59:21,894:INFO:Starting cross validation
2025-05-12 15:59:21,894:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 15:59:22,059:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:22,059:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:59:22,122:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:22,122:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
,
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 15:59:22,122:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 15:59:22,138:INFO:Calculating mean and std
2025-05-12 15:59:22,138:INFO:Creating metrics dataframe
2025-05-12 15:59:22,138:INFO:Uploading results into container
2025-05-12 15:59:22,138:INFO:Uploading model into container now
2025-05-12 15:59:22,138:INFO:_master_model_container: 35
2025-05-12 15:59:22,138:INFO:_display_container: 2
2025-05-12 15:59:22,138:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 15:59:22,138:INFO:create_model() successfully completed......................................
2025-05-12 15:59:22,233:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 15:59:22,233:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:22,233:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-05-12 15:59:22,233:INFO:Initializing create_model()
2025-05-12 15:59:22,233:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=Croston(), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:22,233:INFO:Checking exceptions
2025-05-12 15:59:22,233:INFO:Importing libraries
2025-05-12 15:59:22,233:INFO:Copying training dataset
2025-05-12 15:59:22,233:INFO:Defining folds
2025-05-12 15:59:22,233:INFO:Declaring metric variables
2025-05-12 15:59:22,233:INFO:Importing untrained model
2025-05-12 15:59:22,240:INFO:Declaring custom model
2025-05-12 15:59:22,240:INFO:Croston Imported successfully
2025-05-12 15:59:22,240:INFO:Cross validation set to False
2025-05-12 15:59:22,240:INFO:Fitting Model
2025-05-12 15:59:22,253:INFO:Croston()
2025-05-12 15:59:22,253:INFO:create_model() successfully completed......................................
2025-05-12 15:59:22,327:ERROR:create_model() for Croston() raised an exception or returned all 0.0:
2025-05-12 15:59:22,327:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 954, in compare_models
    np.sum(
AssertionError

2025-05-12 15:59:22,343:INFO:_master_model_container: 35
2025-05-12 15:59:22,343:INFO:_display_container: 2
2025-05-12 15:59:22,343:INFO:Croston()
2025-05-12 15:59:22,343:INFO:compare_models() successfully completed......................................
2025-05-12 15:59:22,343:INFO:Initializing finalize_model()
2025-05-12 15:59:22,343:INFO:finalize_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=Croston(), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-05-12 15:59:22,343:INFO:Finalizing Croston()
2025-05-12 15:59:22,343:INFO:Initializing create_model()
2025-05-12 15:59:22,343:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001D7B7A1A050>, estimator=Croston(), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 15:59:22,343:INFO:Checking exceptions
2025-05-12 15:59:22,343:INFO:Importing libraries
2025-05-12 15:59:22,343:INFO:Copying training dataset
2025-05-12 15:59:22,343:INFO:Defining folds
2025-05-12 15:59:22,343:INFO:Declaring metric variables
2025-05-12 15:59:22,343:INFO:Importing untrained model
2025-05-12 15:59:22,343:INFO:Declaring custom model
2025-05-12 15:59:22,343:INFO:Croston Imported successfully
2025-05-12 15:59:22,343:INFO:Cross validation set to False
2025-05-12 15:59:22,343:INFO:Fitting Model
2025-05-12 15:59:22,359:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))])
2025-05-12 15:59:22,359:INFO:create_model() successfully completed......................................
2025-05-12 15:59:22,439:INFO:_master_model_container: 35
2025-05-12 15:59:22,439:INFO:_display_container: 2
2025-05-12 15:59:22,455:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))])
2025-05-12 15:59:22,455:INFO:finalize_model() successfully completed......................................
2025-05-12 15:59:22,551:INFO:Initializing save_model()
2025-05-12 15:59:22,551:INFO:save_model(model=ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))]), model_name=aapl_best_model, prep_pipe_=ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                DummyForecaster())]))]), verbose=True, use_case=MLUsecase.TIME_SERIES, kwargs={})
2025-05-12 15:59:22,551:INFO:Adding model into prep_pipe
2025-05-12 15:59:22,567:INFO:aapl_best_model.pkl saved in current working directory
2025-05-12 15:59:22,567:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ForecastingPipeline(steps=[('forecaster',
                                                                                            TransformedTargetForecaster(steps=[('model',
                                                                                                                                Croston())]))]))]))])
2025-05-12 15:59:22,567:INFO:save_model() successfully completed......................................
2025-05-12 16:00:09,529:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:00:10,539:INFO:Initializing load_model()
2025-05-12 16:00:10,539:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 16:00:13,545:INFO:Initializing load_model()
2025-05-12 16:00:13,546:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 16:01:59,297:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:00,806:INFO:PyCaret TSForecastingExperiment
2025-05-12 16:02:00,806:INFO:Logging name: ts-default-name
2025-05-12 16:02:00,806:INFO:ML Usecase: MLUsecase.TIME_SERIES
2025-05-12 16:02:00,806:INFO:version 3.3.2
2025-05-12 16:02:00,806:INFO:Initializing setup()
2025-05-12 16:02:00,806:INFO:self.USI: 85de
2025-05-12 16:02:00,806:INFO:self._variable_keys: {'significant_sps_no_harmonics', 'X', 'fh', 'memory', 'USI', 'fold_param', 'n_jobs_param', 'idx', 'significant_sps', 'fold_generator', 'model_engines', 'y_train', 'logging_param', 'enforce_exogenous', 'y_train_transformed', 'enforce_pi', 'all_sps_to_use', '_available_plots', 'log_plots_param', 'strictly_positive', 'y', 'X_test_transformed', 'pipeline', 'X_train_transformed', 'seasonality_present', 'index_type', 'exp_id', 'primary_sp_to_use', 'html_param', 'y_test_transformed', 'X_train', 'y_transformed', 'exogenous_present', 'exp_name_log', 'y_test', 'candidate_sps', 'gpu_param', 'X_test', '_ml_usecase', 'approach_type', 'seed', 'X_transformed', 'data', 'gpu_n_jobs_param'}
2025-05-12 16:02:00,821:INFO:Checking environment
2025-05-12 16:02:00,821:INFO:python_version: 3.10.16
2025-05-12 16:02:00,821:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-05-12 16:02:00,821:INFO:machine: AMD64
2025-05-12 16:02:00,836:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-12 16:02:00,842:INFO:Memory: svmem(total=17009004544, available=4326506496, percent=74.6, used=12682498048, free=4326506496)
2025-05-12 16:02:00,842:INFO:Physical Core: 6
2025-05-12 16:02:00,842:INFO:Logical Core: 12
2025-05-12 16:02:00,842:INFO:Checking libraries
2025-05-12 16:02:00,842:INFO:System:
2025-05-12 16:02:00,842:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-05-12 16:02:00,842:INFO:executable: C:\Users\Thayse\.conda\envs\mlpipeline\python.exe
2025-05-12 16:02:00,842:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-12 16:02:00,842:INFO:PyCaret required dependencies:
2025-05-12 16:02:00,924:INFO:                 pip: 25.0
2025-05-12 16:02:00,924:INFO:          setuptools: 75.8.0
2025-05-12 16:02:00,924:INFO:             pycaret: 3.3.2
2025-05-12 16:02:00,924:INFO:             IPython: 8.34.0
2025-05-12 16:02:00,924:INFO:          ipywidgets: 8.1.5
2025-05-12 16:02:00,924:INFO:                tqdm: 4.67.1
2025-05-12 16:02:00,924:INFO:               numpy: 1.26.4
2025-05-12 16:02:00,924:INFO:              pandas: 2.1.4
2025-05-12 16:02:00,924:INFO:              jinja2: 3.1.6
2025-05-12 16:02:00,924:INFO:               scipy: 1.11.4
2025-05-12 16:02:00,924:INFO:              joblib: 1.3.2
2025-05-12 16:02:00,924:INFO:             sklearn: 1.4.2
2025-05-12 16:02:00,924:INFO:                pyod: 2.0.4
2025-05-12 16:02:00,924:INFO:            imblearn: 0.13.0
2025-05-12 16:02:00,924:INFO:   category_encoders: 2.7.0
2025-05-12 16:02:00,924:INFO:            lightgbm: 4.6.0
2025-05-12 16:02:00,924:INFO:               numba: 0.61.0
2025-05-12 16:02:00,924:INFO:            requests: 2.32.3
2025-05-12 16:02:00,924:INFO:          matplotlib: 3.7.5
2025-05-12 16:02:00,924:INFO:          scikitplot: 0.3.7
2025-05-12 16:02:00,924:INFO:         yellowbrick: 1.5
2025-05-12 16:02:00,924:INFO:              plotly: 5.24.1
2025-05-12 16:02:00,924:INFO:    plotly-resampler: Not installed
2025-05-12 16:02:00,924:INFO:             kaleido: 0.2.1
2025-05-12 16:02:00,924:INFO:           schemdraw: 0.15
2025-05-12 16:02:00,924:INFO:         statsmodels: 0.14.4
2025-05-12 16:02:00,924:INFO:              sktime: 0.26.0
2025-05-12 16:02:00,924:INFO:               tbats: 1.1.3
2025-05-12 16:02:00,924:INFO:            pmdarima: 2.0.4
2025-05-12 16:02:00,924:INFO:              psutil: 7.0.0
2025-05-12 16:02:00,924:INFO:          markupsafe: 3.0.2
2025-05-12 16:02:00,924:INFO:             pickle5: Not installed
2025-05-12 16:02:00,924:INFO:         cloudpickle: 3.1.1
2025-05-12 16:02:00,924:INFO:         deprecation: 2.1.0
2025-05-12 16:02:00,924:INFO:              xxhash: 3.5.0
2025-05-12 16:02:00,924:INFO:           wurlitzer: Not installed
2025-05-12 16:02:00,924:INFO:PyCaret optional dependencies:
2025-05-12 16:02:01,349:INFO:                shap: Not installed
2025-05-12 16:02:01,349:INFO:           interpret: Not installed
2025-05-12 16:02:01,349:INFO:                umap: Not installed
2025-05-12 16:02:01,349:INFO:     ydata_profiling: 4.16.1
2025-05-12 16:02:01,349:INFO:  explainerdashboard: Not installed
2025-05-12 16:02:01,349:INFO:             autoviz: Not installed
2025-05-12 16:02:01,349:INFO:           fairlearn: Not installed
2025-05-12 16:02:01,349:INFO:          deepchecks: Not installed
2025-05-12 16:02:01,349:INFO:             xgboost: Not installed
2025-05-12 16:02:01,349:INFO:            catboost: Not installed
2025-05-12 16:02:01,349:INFO:              kmodes: Not installed
2025-05-12 16:02:01,349:INFO:             mlxtend: Not installed
2025-05-12 16:02:01,350:INFO:       statsforecast: Not installed
2025-05-12 16:02:01,350:INFO:        tune_sklearn: Not installed
2025-05-12 16:02:01,350:INFO:                 ray: Not installed
2025-05-12 16:02:01,350:INFO:            hyperopt: Not installed
2025-05-12 16:02:01,350:INFO:              optuna: 4.2.1
2025-05-12 16:02:01,350:INFO:               skopt: Not installed
2025-05-12 16:02:01,350:INFO:              mlflow: Not installed
2025-05-12 16:02:01,350:INFO:              gradio: Not installed
2025-05-12 16:02:01,350:INFO:             fastapi: 0.115.12
2025-05-12 16:02:01,350:INFO:             uvicorn: 0.34.2
2025-05-12 16:02:01,350:INFO:              m2cgen: Not installed
2025-05-12 16:02:01,350:INFO:           evidently: Not installed
2025-05-12 16:02:01,350:INFO:               fugue: Not installed
2025-05-12 16:02:01,350:INFO:           streamlit: 1.44.1
2025-05-12 16:02:01,350:INFO:             prophet: 1.1.6
2025-05-12 16:02:01,350:INFO:None
2025-05-12 16:02:01,351:INFO:Set Forecast Horizon.
2025-05-12 16:02:01,351:INFO:Set up Train-Test Splits.
2025-05-12 16:02:01,388:INFO:Finished creating preprocessing pipeline.
2025-05-12 16:02:01,388:INFO:Pipeline: ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                DummyForecaster())]))])
2025-05-12 16:02:01,388:INFO:Set up Seasonal Period.
2025-05-12 16:02:01,388:INFO:Setting the seasonal component type - 'add' or 'mul'.
2025-05-12 16:02:01,388:INFO:Checking if data is strictly positive.
2025-05-12 16:02:01,404:INFO:Creating final display dataframe.
2025-05-12 16:02:01,420:INFO:Setup Display Container:                                          Description                    Value
0                                         session_id                      123
1                                             Target                    Close
2                                           Approach               Univariate
3                                Exogenous Variables                  Present
4                                Original data shape                 (501, 2)
5                             Transformed data shape                 (501, 2)
6                        Transformed train set shape                 (500, 2)
7                         Transformed test set shape                   (1, 2)
8                           Rows with missing values                     0.0%
9                                     Fold Generator  ExpandingWindowSplitter
10                                       Fold Number                        3
11                       Enforce Prediction Interval                    False
12                   Splits used for hyperparameters                      all
13                   User Defined Seasonal Period(s)                     None
14                           Ignore Seasonality Test                    False
15                        Seasonality Detection Algo                     auto
16                            Max Period to Consider                       60
17                         Seasonal Period(s) Tested                       []
18                    Significant Seasonal Period(s)                      [1]
19  Significant Seasonal Period(s) without Harmonics                      [1]
20                                  Remove Harmonics                    False
21                            Harmonics Order Method             harmonic_max
22                          Num Seasonalities to Use                        1
23                          All Seasonalities to Use                      [1]
24                               Primary Seasonality                        1
25                               Seasonality Present                    False
26                                  Seasonality Type                     None
27                          Target Strictly Positive                     True
28                                Target White Noise                       No
29                                     Recommended d                        1
30                            Recommended Seasonal D                        0
31                                        Preprocess                    False
32                                          CPU Jobs                       -1
33                                           Use GPU                    False
34                                    Log Experiment                    False
35                                   Experiment Name          ts-default-name
36                                               USI                     85de
2025-05-12 16:02:01,436:INFO:Engine successfully changes for model 'auto_arima' to 'pmdarima'.
2025-05-12 16:02:01,476:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,520:INFO:Engine for model 'lr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,521:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,521:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,521:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,522:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,522:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,522:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,525:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,527:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,528:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,528:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,528:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,529:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,529:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,530:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,530:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,530:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,530:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,530:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,530:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,530:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,534:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,534:INFO:Engine for model 'lr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,535:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,535:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,536:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,536:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,536:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,537:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,539:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,541:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,541:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,541:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,541:INFO:Engine successfully changes for model 'lr_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,541:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,541:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,541:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,557:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,557:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,557:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,557:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,557:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,557:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,557:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,557:INFO:Engine for model 'en_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,563:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,563:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,563:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,563:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,563:INFO:Engine successfully changes for model 'en_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,573:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,573:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,573:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,573:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,573:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,589:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,589:INFO:Engine for model 'ridge_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,589:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,589:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,589:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,589:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,589:INFO:Engine successfully changes for model 'ridge_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,589:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,605:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,605:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,605:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,605:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,605:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,605:INFO:Engine for model 'lasso_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,605:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,621:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,621:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,621:INFO:Engine successfully changes for model 'lasso_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,621:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,621:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,621:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,636:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,638:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,640:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,640:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,640:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,640:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,642:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,642:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,642:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,642:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,642:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,642:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,642:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,642:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,646:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,646:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,646:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,646:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,652:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,652:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,652:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,652:INFO:Engine successfully changes for model 'lar_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,652:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,652:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,652:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,668:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,668:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,668:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,684:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,684:INFO:Engine for model 'llar_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,684:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,684:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,684:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,684:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,684:INFO:Engine successfully changes for model 'llar_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,699:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,699:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,699:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,699:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,706:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,706:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,706:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,708:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,708:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,708:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,708:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,708:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,708:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,708:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,710:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,710:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,710:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,714:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,715:INFO:Engine for model 'br_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,715:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,717:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,719:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,721:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,721:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,721:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,721:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,723:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,723:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,723:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,723:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,723:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,723:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,723:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,723:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,723:INFO:Engine successfully changes for model 'br_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,727:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,731:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,733:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,735:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,735:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,735:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,737:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,737:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,737:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,737:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,737:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,737:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,737:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,739:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,739:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,743:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,747:INFO:Engine for model 'huber_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,749:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,753:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,753:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,753:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,753:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,755:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,755:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,755:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,755:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,755:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,755:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,757:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,757:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,757:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,757:INFO:Engine successfully changes for model 'huber_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,761:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,767:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,769:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,769:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,771:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,771:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,771:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,771:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,771:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,773:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,773:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,773:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,773:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,773:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,773:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,777:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,782:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,784:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,784:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,786:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,786:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,786:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,786:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,788:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,788:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,788:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,788:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,788:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,788:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,788:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,788:INFO:Engine successfully changes for model 'par_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,792:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,798:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,800:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,802:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,802:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,802:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,802:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,802:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,804:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,804:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,804:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,804:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,804:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,804:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,808:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,814:INFO:Engine for model 'omp_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,816:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,816:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,816:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,816:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,816:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,818:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,818:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,818:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,818:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,818:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,818:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,818:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,818:INFO:Engine successfully changes for model 'omp_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,822:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,830:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,830:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,830:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,830:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,832:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,832:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,832:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,832:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,832:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,832:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,832:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,832:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,832:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,836:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,844:INFO:Engine for model 'knn_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,844:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,844:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,845:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,845:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,845:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,846:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,846:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,846:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,846:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,846:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,846:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,846:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,847:INFO:Engine successfully changes for model 'knn_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,850:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,856:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,857:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,857:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,857:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,859:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,859:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,859:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,859:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,859:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,859:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,863:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,869:INFO:Engine for model 'dt_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,871:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,871:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,871:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,871:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,871:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,871:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,871:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,871:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,872:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,872:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,872:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,872:INFO:Engine successfully changes for model 'dt_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,876:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,876:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,876:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,876:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,876:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,876:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,876:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,876:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,876:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,876:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,876:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,876:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,876:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,891:INFO:Engine for model 'rf_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,891:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,891:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,891:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,891:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,891:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,891:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,891:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,891:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,891:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,891:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,891:INFO:Engine successfully changes for model 'rf_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,891:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,907:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,907:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,907:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,907:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,907:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,907:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,907:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,907:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,907:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,907:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,907:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,923:INFO:Engine for model 'et_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,923:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,923:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,923:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,923:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,923:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,923:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,923:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,923:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,923:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,923:INFO:Engine successfully changes for model 'et_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,929:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,929:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,939:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,939:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,939:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,939:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,939:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,939:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,939:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,939:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,939:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,955:INFO:Engine for model 'gbr_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,955:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,955:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,955:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,955:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,955:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,955:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,955:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,955:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,955:INFO:Engine successfully changes for model 'gbr_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,955:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,971:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,971:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,971:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,971:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,971:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,971:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,971:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,971:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,976:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:01,986:INFO:Engine for model 'ada_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,986:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,986:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,986:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,986:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,987:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:01,987:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,988:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:01,988:INFO:Engine successfully changes for model 'ada_cds_dt' to 'sklearn'.
2025-05-12 16:02:01,992:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,000:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,000:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,000:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,000:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,000:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,000:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,000:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,004:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,008:INFO:Engine for model 'xgboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,008:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,008:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,008:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,008:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,008:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,008:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,008:INFO:Engine successfully changes for model 'xgboost_cds_dt' to 'sklearn'.
2025-05-12 16:02:02,008:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,023:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,023:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,023:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,023:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,023:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,023:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,023:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,023:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,023:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,023:INFO:Engine for model 'lightgbm_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,023:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,023:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,023:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,039:INFO:Engine successfully changes for model 'lightgbm_cds_dt' to 'sklearn'.
2025-05-12 16:02:02,039:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,039:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,039:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,039:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,039:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,039:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,039:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,055:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,055:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,055:INFO:Engine for model 'catboost_cds_dt' has not been set explicitly, hence returning None.
2025-05-12 16:02:02,055:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,055:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,055:INFO:Engine successfully changes for model 'catboost_cds_dt' to 'sklearn'.
2025-05-12 16:02:02,055:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,071:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,071:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,071:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,071:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,071:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,087:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,087:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,087:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,087:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,087:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,087:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,087:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,087:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,087:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,103:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:02,103:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,103:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,103:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,103:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:02:02,103:INFO:setup() successfully completed in 1.3s...............
2025-05-12 16:02:02,103:INFO:Initializing compare_models()
2025-05-12 16:02:02,103:INFO:compare_models(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, include=None, fold=None, round=4, cross_validation=True, sort=MAE, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'MAE', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'experiment_custom_tags': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.time_series.forecasting.oop.TSForecastingExperiment'>}, exclude=None)
2025-05-12 16:02:02,103:INFO:Checking exceptions
2025-05-12 16:02:02,103:INFO:Preparing display monitor
2025-05-12 16:02:02,103:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:713: UserWarning: Unsupported estimator `ensemble_forecaster` for method `compare_models()`, removing from model_library
  warnings.warn(

2025-05-12 16:02:02,103:INFO:Initializing ARIMA
2025-05-12 16:02:02,103:INFO:Total runtime is 0.0 minutes
2025-05-12 16:02:02,119:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:02,119:INFO:Initializing create_model()
2025-05-12 16:02:02,119:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:02,119:INFO:Checking exceptions
2025-05-12 16:02:02,119:INFO:Importing libraries
2025-05-12 16:02:02,119:INFO:Copying training dataset
2025-05-12 16:02:02,119:INFO:Defining folds
2025-05-12 16:02:02,119:INFO:Declaring metric variables
2025-05-12 16:02:02,119:INFO:Importing untrained model
2025-05-12 16:02:02,119:INFO:ARIMA Imported successfully
2025-05-12 16:02:02,119:INFO:Starting cross validation
2025-05-12 16:02:02,119:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:05,980:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 16:02:05,980:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 16:02:05,980:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 16:02:05,980:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 16:02:05,980:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 16:02:05,980:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 16:02:05,994:INFO:Calculating mean and std
2025-05-12 16:02:05,994:INFO:Creating metrics dataframe
2025-05-12 16:02:06,000:INFO:Uploading results into container
2025-05-12 16:02:06,000:INFO:Uploading model into container now
2025-05-12 16:02:06,000:INFO:_master_model_container: 1
2025-05-12 16:02:06,000:INFO:_display_container: 2
2025-05-12 16:02:06,000:INFO:ARIMA()
2025-05-12 16:02:06,000:INFO:create_model() successfully completed......................................
2025-05-12 16:02:06,108:WARNING:create_model() for ARIMA() raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:06,108:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:06,108:INFO:Initializing create_model()
2025-05-12 16:02:06,108:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:06,108:INFO:Checking exceptions
2025-05-12 16:02:06,108:INFO:Importing libraries
2025-05-12 16:02:06,108:INFO:Copying training dataset
2025-05-12 16:02:06,120:INFO:Defining folds
2025-05-12 16:02:06,120:INFO:Declaring metric variables
2025-05-12 16:02:06,120:INFO:Importing untrained model
2025-05-12 16:02:06,124:INFO:ARIMA Imported successfully
2025-05-12 16:02:06,124:INFO:Starting cross validation
2025-05-12 16:02:06,124:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:09,099:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 16:02:09,099:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 16:02:09,099:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 16:02:09,099:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 16:02:09,099:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ARIMA())]))])
2025-05-12 16:02:09,099:ERROR:Cannot cast ufunc 'equal' input 0 from dtype('<m8[ns]') to dtype('<m8') with casting rule 'same_kind'
2025-05-12 16:02:09,115:INFO:Calculating mean and std
2025-05-12 16:02:09,115:INFO:Creating metrics dataframe
2025-05-12 16:02:09,115:INFO:Uploading results into container
2025-05-12 16:02:09,115:INFO:Uploading model into container now
2025-05-12 16:02:09,115:INFO:_master_model_container: 2
2025-05-12 16:02:09,115:INFO:_display_container: 2
2025-05-12 16:02:09,115:INFO:ARIMA()
2025-05-12 16:02:09,115:INFO:create_model() successfully completed......................................
2025-05-12 16:02:09,226:ERROR:create_model() for ARIMA() raised an exception or returned all 0.0:
2025-05-12 16:02:09,226:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:09,226:INFO:Initializing Auto ARIMA
2025-05-12 16:02:09,226:INFO:Total runtime is 0.118719748655955 minutes
2025-05-12 16:02:09,226:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:09,226:INFO:Initializing create_model()
2025-05-12 16:02:09,226:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=auto_arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:09,226:INFO:Checking exceptions
2025-05-12 16:02:09,226:INFO:Importing libraries
2025-05-12 16:02:09,226:INFO:Copying training dataset
2025-05-12 16:02:09,242:INFO:Defining folds
2025-05-12 16:02:09,242:INFO:Declaring metric variables
2025-05-12 16:02:09,242:INFO:Importing untrained model
2025-05-12 16:02:09,242:INFO:Auto ARIMA Imported successfully
2025-05-12 16:02:09,242:INFO:Starting cross validation
2025-05-12 16:02:09,242:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:12,251:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 16:02:12,251:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 16:02:12,251:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 16:02:12,252:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 16:02:12,254:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 16:02:12,254:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 16:02:12,266:INFO:Calculating mean and std
2025-05-12 16:02:12,268:INFO:Creating metrics dataframe
2025-05-12 16:02:12,273:INFO:Uploading results into container
2025-05-12 16:02:12,273:INFO:Uploading model into container now
2025-05-12 16:02:12,273:INFO:_master_model_container: 3
2025-05-12 16:02:12,273:INFO:_display_container: 2
2025-05-12 16:02:12,273:INFO:AutoARIMA(random_state=123, suppress_warnings=True)
2025-05-12 16:02:12,273:INFO:create_model() successfully completed......................................
2025-05-12 16:02:12,391:WARNING:create_model() for AutoARIMA(random_state=123, suppress_warnings=True) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:12,391:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:12,391:INFO:Initializing create_model()
2025-05-12 16:02:12,391:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=auto_arima, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:12,391:INFO:Checking exceptions
2025-05-12 16:02:12,391:INFO:Importing libraries
2025-05-12 16:02:12,391:INFO:Copying training dataset
2025-05-12 16:02:12,393:INFO:Defining folds
2025-05-12 16:02:12,393:INFO:Declaring metric variables
2025-05-12 16:02:12,393:INFO:Importing untrained model
2025-05-12 16:02:12,393:INFO:Auto ARIMA Imported successfully
2025-05-12 16:02:12,393:INFO:Starting cross validation
2025-05-12 16:02:12,393:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:15,421:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 16:02:15,421:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 16:02:15,421:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 16:02:15,421:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                AutoARIMA(random_state=123,
                                                                          suppress_warnings=True))]))])
2025-05-12 16:02:15,421:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 16:02:15,421:ERROR:ufunc 'matmul' did not contain a loop with signature matching types (<class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float64DType'>) -> None
2025-05-12 16:02:15,437:INFO:Calculating mean and std
2025-05-12 16:02:15,437:INFO:Creating metrics dataframe
2025-05-12 16:02:15,437:INFO:Uploading results into container
2025-05-12 16:02:15,437:INFO:Uploading model into container now
2025-05-12 16:02:15,437:INFO:_master_model_container: 4
2025-05-12 16:02:15,437:INFO:_display_container: 2
2025-05-12 16:02:15,437:INFO:AutoARIMA(random_state=123, suppress_warnings=True)
2025-05-12 16:02:15,437:INFO:create_model() successfully completed......................................
2025-05-12 16:02:15,544:ERROR:create_model() for AutoARIMA(random_state=123, suppress_warnings=True) raised an exception or returned all 0.0:
2025-05-12 16:02:15,544:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:15,544:INFO:Initializing Croston
2025-05-12 16:02:15,544:INFO:Total runtime is 0.2240240732828776 minutes
2025-05-12 16:02:15,544:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:15,544:INFO:Initializing create_model()
2025-05-12 16:02:15,544:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=croston, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:15,544:INFO:Checking exceptions
2025-05-12 16:02:15,544:INFO:Importing libraries
2025-05-12 16:02:15,544:INFO:Copying training dataset
2025-05-12 16:02:15,560:INFO:Defining folds
2025-05-12 16:02:15,560:INFO:Declaring metric variables
2025-05-12 16:02:15,560:INFO:Importing untrained model
2025-05-12 16:02:15,560:INFO:Croston Imported successfully
2025-05-12 16:02:15,563:INFO:Starting cross validation
2025-05-12 16:02:15,563:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:15,608:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 16:02:15,608:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 16:02:15,608:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 16:02:15,608:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 16:02:15,609:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 16:02:15,610:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_regression.py:483: FutureWarning: 'squared' is deprecated in version 1.4 and will be removed in 1.6. To calculate the root mean squared error, use the function'root_mean_squared_error'.
  warnings.warn(

2025-05-12 16:02:15,627:INFO:Calculating mean and std
2025-05-12 16:02:15,628:INFO:Creating metrics dataframe
2025-05-12 16:02:15,630:INFO:Uploading results into container
2025-05-12 16:02:15,630:INFO:Uploading model into container now
2025-05-12 16:02:15,630:INFO:_master_model_container: 5
2025-05-12 16:02:15,630:INFO:_display_container: 2
2025-05-12 16:02:15,630:INFO:Croston()
2025-05-12 16:02:15,630:INFO:create_model() successfully completed......................................
2025-05-12 16:02:15,710:INFO:SubProcess create_model() end ==================================
2025-05-12 16:02:15,710:INFO:Creating metrics dataframe
2025-05-12 16:02:15,710:INFO:Initializing Linear w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:15,710:INFO:Total runtime is 0.22678922812143962 minutes
2025-05-12 16:02:15,710:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:15,710:INFO:Initializing create_model()
2025-05-12 16:02:15,710:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=lr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:15,710:INFO:Checking exceptions
2025-05-12 16:02:15,710:INFO:Importing libraries
2025-05-12 16:02:15,710:INFO:Copying training dataset
2025-05-12 16:02:15,726:INFO:Defining folds
2025-05-12 16:02:15,726:INFO:Declaring metric variables
2025-05-12 16:02:15,726:INFO:Importing untrained model
2025-05-12 16:02:15,726:INFO:Linear w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:15,726:INFO:Starting cross validation
2025-05-12 16:02:15,726:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:16,442:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:16,458:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:16,458:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:16,521:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:16,521:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:16,537:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:16,537:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:16,537:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:16,537:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:16,552:INFO:Calculating mean and std
2025-05-12 16:02:16,552:INFO:Creating metrics dataframe
2025-05-12 16:02:16,552:INFO:Uploading results into container
2025-05-12 16:02:16,552:INFO:Uploading model into container now
2025-05-12 16:02:16,552:INFO:_master_model_container: 6
2025-05-12 16:02:16,552:INFO:_display_container: 2
2025-05-12 16:02:16,552:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1)
2025-05-12 16:02:16,552:INFO:create_model() successfully completed......................................
2025-05-12 16:02:16,630:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:16,630:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:16,630:INFO:Initializing create_model()
2025-05-12 16:02:16,630:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=lr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:16,630:INFO:Checking exceptions
2025-05-12 16:02:16,630:INFO:Importing libraries
2025-05-12 16:02:16,630:INFO:Copying training dataset
2025-05-12 16:02:16,630:INFO:Defining folds
2025-05-12 16:02:16,630:INFO:Declaring metric variables
2025-05-12 16:02:16,630:INFO:Importing untrained model
2025-05-12 16:02:16,630:INFO:Linear w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:16,646:INFO:Starting cross validation
2025-05-12 16:02:16,646:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:17,380:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:17,380:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:17,380:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:17,458:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:17,458:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:17,458:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:17,458:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:17,458:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LinearRegression(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:17,458:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:17,474:INFO:Calculating mean and std
2025-05-12 16:02:17,474:INFO:Creating metrics dataframe
2025-05-12 16:02:17,474:INFO:Uploading results into container
2025-05-12 16:02:17,474:INFO:Uploading model into container now
2025-05-12 16:02:17,474:INFO:_master_model_container: 7
2025-05-12 16:02:17,474:INFO:_display_container: 2
2025-05-12 16:02:17,474:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1)
2025-05-12 16:02:17,474:INFO:create_model() successfully completed......................................
2025-05-12 16:02:17,563:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LinearRegression(n_jobs=-1), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:17,563:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:17,563:INFO:Initializing Elastic Net w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:17,563:INFO:Total runtime is 0.2576708674430847 minutes
2025-05-12 16:02:17,563:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:17,563:INFO:Initializing create_model()
2025-05-12 16:02:17,563:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=en_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:17,563:INFO:Checking exceptions
2025-05-12 16:02:17,563:INFO:Importing libraries
2025-05-12 16:02:17,563:INFO:Copying training dataset
2025-05-12 16:02:17,563:INFO:Defining folds
2025-05-12 16:02:17,563:INFO:Declaring metric variables
2025-05-12 16:02:17,563:INFO:Importing untrained model
2025-05-12 16:02:17,563:INFO:Elastic Net w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:17,563:INFO:Starting cross validation
2025-05-12 16:02:17,563:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:18,292:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:18,308:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:18,308:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:18,387:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:18,387:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:18,391:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:18,391:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:18,393:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:18,393:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:18,403:INFO:Calculating mean and std
2025-05-12 16:02:18,403:INFO:Creating metrics dataframe
2025-05-12 16:02:18,403:INFO:Uploading results into container
2025-05-12 16:02:18,403:INFO:Uploading model into container now
2025-05-12 16:02:18,403:INFO:_master_model_container: 8
2025-05-12 16:02:18,403:INFO:_display_container: 2
2025-05-12 16:02:18,403:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1)
2025-05-12 16:02:18,403:INFO:create_model() successfully completed......................................
2025-05-12 16:02:18,498:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:18,498:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:18,498:INFO:Initializing create_model()
2025-05-12 16:02:18,498:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=en_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:18,498:INFO:Checking exceptions
2025-05-12 16:02:18,498:INFO:Importing libraries
2025-05-12 16:02:18,498:INFO:Copying training dataset
2025-05-12 16:02:18,498:INFO:Defining folds
2025-05-12 16:02:18,498:INFO:Declaring metric variables
2025-05-12 16:02:18,498:INFO:Importing untrained model
2025-05-12 16:02:18,498:INFO:Elastic Net w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:18,498:INFO:Starting cross validation
2025-05-12 16:02:18,498:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:19,232:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:19,232:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:19,232:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:02:19,311:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,311:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,327:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,327:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,327:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ElasticNet(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,327:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,343:INFO:Calculating mean and std
2025-05-12 16:02:19,343:INFO:Creating metrics dataframe
2025-05-12 16:02:19,343:INFO:Uploading results into container
2025-05-12 16:02:19,343:INFO:Uploading model into container now
2025-05-12 16:02:19,343:INFO:_master_model_container: 9
2025-05-12 16:02:19,343:INFO:_display_container: 2
2025-05-12 16:02:19,343:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1)
2025-05-12 16:02:19,343:INFO:create_model() successfully completed......................................
2025-05-12 16:02:19,430:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ElasticNet(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:19,430:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:19,430:INFO:Initializing Ridge w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:19,430:INFO:Total runtime is 0.2887956182161967 minutes
2025-05-12 16:02:19,430:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:19,430:INFO:Initializing create_model()
2025-05-12 16:02:19,430:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=ridge_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:19,430:INFO:Checking exceptions
2025-05-12 16:02:19,430:INFO:Importing libraries
2025-05-12 16:02:19,430:INFO:Copying training dataset
2025-05-12 16:02:19,430:INFO:Defining folds
2025-05-12 16:02:19,430:INFO:Declaring metric variables
2025-05-12 16:02:19,430:INFO:Importing untrained model
2025-05-12 16:02:19,430:INFO:Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:19,446:INFO:Starting cross validation
2025-05-12 16:02:19,447:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:19,529:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,529:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,529:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,529:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,529:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,529:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,560:INFO:Calculating mean and std
2025-05-12 16:02:19,560:INFO:Creating metrics dataframe
2025-05-12 16:02:19,560:INFO:Uploading results into container
2025-05-12 16:02:19,560:INFO:Uploading model into container now
2025-05-12 16:02:19,560:INFO:_master_model_container: 10
2025-05-12 16:02:19,560:INFO:_display_container: 2
2025-05-12 16:02:19,560:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1)
2025-05-12 16:02:19,560:INFO:create_model() successfully completed......................................
2025-05-12 16:02:19,647:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:19,647:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:19,647:INFO:Initializing create_model()
2025-05-12 16:02:19,647:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=ridge_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:19,647:INFO:Checking exceptions
2025-05-12 16:02:19,647:INFO:Importing libraries
2025-05-12 16:02:19,647:INFO:Copying training dataset
2025-05-12 16:02:19,647:INFO:Defining folds
2025-05-12 16:02:19,647:INFO:Declaring metric variables
2025-05-12 16:02:19,647:INFO:Importing untrained model
2025-05-12 16:02:19,647:INFO:Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:19,663:INFO:Starting cross validation
2025-05-12 16:02:19,663:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:19,774:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,774:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,774:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,774:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,774:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Ridge(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,774:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,790:INFO:Calculating mean and std
2025-05-12 16:02:19,790:INFO:Creating metrics dataframe
2025-05-12 16:02:19,790:INFO:Uploading results into container
2025-05-12 16:02:19,790:INFO:Uploading model into container now
2025-05-12 16:02:19,790:INFO:_master_model_container: 11
2025-05-12 16:02:19,790:INFO:_display_container: 2
2025-05-12 16:02:19,790:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1)
2025-05-12 16:02:19,790:INFO:create_model() successfully completed......................................
2025-05-12 16:02:19,881:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Ridge(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:19,881:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:19,881:INFO:Initializing Lasso w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:19,881:INFO:Total runtime is 0.29629794756571454 minutes
2025-05-12 16:02:19,881:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:19,881:INFO:Initializing create_model()
2025-05-12 16:02:19,881:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=lasso_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:19,881:INFO:Checking exceptions
2025-05-12 16:02:19,881:INFO:Importing libraries
2025-05-12 16:02:19,881:INFO:Copying training dataset
2025-05-12 16:02:19,881:INFO:Defining folds
2025-05-12 16:02:19,881:INFO:Declaring metric variables
2025-05-12 16:02:19,881:INFO:Importing untrained model
2025-05-12 16:02:19,881:INFO:Lasso w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:19,881:INFO:Starting cross validation
2025-05-12 16:02:19,896:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:19,981:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,981:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,981:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,981:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:19,981:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,981:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:19,996:INFO:Calculating mean and std
2025-05-12 16:02:19,999:INFO:Creating metrics dataframe
2025-05-12 16:02:19,999:INFO:Uploading results into container
2025-05-12 16:02:19,999:INFO:Uploading model into container now
2025-05-12 16:02:19,999:INFO:_master_model_container: 12
2025-05-12 16:02:19,999:INFO:_display_container: 2
2025-05-12 16:02:19,999:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1)
2025-05-12 16:02:19,999:INFO:create_model() successfully completed......................................
2025-05-12 16:02:20,082:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:20,082:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:20,082:INFO:Initializing create_model()
2025-05-12 16:02:20,082:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=lasso_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:20,082:INFO:Checking exceptions
2025-05-12 16:02:20,082:INFO:Importing libraries
2025-05-12 16:02:20,082:INFO:Copying training dataset
2025-05-12 16:02:20,097:INFO:Defining folds
2025-05-12 16:02:20,097:INFO:Declaring metric variables
2025-05-12 16:02:20,097:INFO:Importing untrained model
2025-05-12 16:02:20,097:INFO:Lasso w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:20,097:INFO:Starting cross validation
2025-05-12 16:02:20,097:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:20,193:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,193:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,193:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,193:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,193:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=Lasso(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,193:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,209:INFO:Calculating mean and std
2025-05-12 16:02:20,211:INFO:Creating metrics dataframe
2025-05-12 16:02:20,211:INFO:Uploading results into container
2025-05-12 16:02:20,211:INFO:Uploading model into container now
2025-05-12 16:02:20,211:INFO:_master_model_container: 13
2025-05-12 16:02:20,211:INFO:_display_container: 2
2025-05-12 16:02:20,211:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1)
2025-05-12 16:02:20,211:INFO:create_model() successfully completed......................................
2025-05-12 16:02:20,297:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=Lasso(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:20,297:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:20,297:INFO:Initializing Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:20,297:INFO:Total runtime is 0.30324474175771077 minutes
2025-05-12 16:02:20,297:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:20,297:INFO:Initializing create_model()
2025-05-12 16:02:20,297:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=llar_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:20,297:INFO:Checking exceptions
2025-05-12 16:02:20,297:INFO:Importing libraries
2025-05-12 16:02:20,297:INFO:Copying training dataset
2025-05-12 16:02:20,297:INFO:Defining folds
2025-05-12 16:02:20,297:INFO:Declaring metric variables
2025-05-12 16:02:20,297:INFO:Importing untrained model
2025-05-12 16:02:20,297:INFO:Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:20,297:INFO:Starting cross validation
2025-05-12 16:02:20,297:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:20,409:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,409:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,409:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,409:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,409:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,409:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,425:INFO:Calculating mean and std
2025-05-12 16:02:20,425:INFO:Creating metrics dataframe
2025-05-12 16:02:20,425:INFO:Uploading results into container
2025-05-12 16:02:20,425:INFO:Uploading model into container now
2025-05-12 16:02:20,425:INFO:_master_model_container: 14
2025-05-12 16:02:20,425:INFO:_display_container: 2
2025-05-12 16:02:20,425:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1)
2025-05-12 16:02:20,425:INFO:create_model() successfully completed......................................
2025-05-12 16:02:20,514:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:20,514:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:20,514:INFO:Initializing create_model()
2025-05-12 16:02:20,514:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=llar_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:20,514:INFO:Checking exceptions
2025-05-12 16:02:20,514:INFO:Importing libraries
2025-05-12 16:02:20,514:INFO:Copying training dataset
2025-05-12 16:02:20,514:INFO:Defining folds
2025-05-12 16:02:20,514:INFO:Declaring metric variables
2025-05-12 16:02:20,514:INFO:Importing untrained model
2025-05-12 16:02:20,514:INFO:Lasso Least Angular Regressor w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:20,530:INFO:Starting cross validation
2025-05-12 16:02:20,531:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:20,613:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,613:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,613:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,613:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,613:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LassoLars(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,629:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,645:INFO:Calculating mean and std
2025-05-12 16:02:20,645:INFO:Creating metrics dataframe
2025-05-12 16:02:20,645:INFO:Uploading results into container
2025-05-12 16:02:20,645:INFO:Uploading model into container now
2025-05-12 16:02:20,645:INFO:_master_model_container: 15
2025-05-12 16:02:20,645:INFO:_display_container: 2
2025-05-12 16:02:20,645:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1)
2025-05-12 16:02:20,645:INFO:create_model() successfully completed......................................
2025-05-12 16:02:20,731:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LassoLars(random_state=123), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:20,731:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:20,731:INFO:Initializing Bayesian Ridge w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:20,731:INFO:Total runtime is 0.31046924193700154 minutes
2025-05-12 16:02:20,731:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:20,731:INFO:Initializing create_model()
2025-05-12 16:02:20,731:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=br_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:20,731:INFO:Checking exceptions
2025-05-12 16:02:20,731:INFO:Importing libraries
2025-05-12 16:02:20,731:INFO:Copying training dataset
2025-05-12 16:02:20,731:INFO:Defining folds
2025-05-12 16:02:20,731:INFO:Declaring metric variables
2025-05-12 16:02:20,731:INFO:Importing untrained model
2025-05-12 16:02:20,731:INFO:Bayesian Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:20,731:INFO:Starting cross validation
2025-05-12 16:02:20,731:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:20,826:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,826:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,826:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,826:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,826:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:20,826:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:20,842:INFO:Calculating mean and std
2025-05-12 16:02:20,842:INFO:Creating metrics dataframe
2025-05-12 16:02:20,842:INFO:Uploading results into container
2025-05-12 16:02:20,842:INFO:Uploading model into container now
2025-05-12 16:02:20,842:INFO:_master_model_container: 16
2025-05-12 16:02:20,842:INFO:_display_container: 2
2025-05-12 16:02:20,842:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1)
2025-05-12 16:02:20,842:INFO:create_model() successfully completed......................................
2025-05-12 16:02:20,931:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:20,931:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:20,931:INFO:Initializing create_model()
2025-05-12 16:02:20,931:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=br_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:20,931:INFO:Checking exceptions
2025-05-12 16:02:20,931:INFO:Importing libraries
2025-05-12 16:02:20,931:INFO:Copying training dataset
2025-05-12 16:02:20,931:INFO:Defining folds
2025-05-12 16:02:20,931:INFO:Declaring metric variables
2025-05-12 16:02:20,931:INFO:Importing untrained model
2025-05-12 16:02:20,931:INFO:Bayesian Ridge w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:20,931:INFO:Starting cross validation
2025-05-12 16:02:20,931:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:21,029:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,029:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,029:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,029:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,029:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=BayesianRidge(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,029:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,045:INFO:Calculating mean and std
2025-05-12 16:02:21,045:INFO:Creating metrics dataframe
2025-05-12 16:02:21,051:INFO:Uploading results into container
2025-05-12 16:02:21,051:INFO:Uploading model into container now
2025-05-12 16:02:21,051:INFO:_master_model_container: 17
2025-05-12 16:02:21,051:INFO:_display_container: 2
2025-05-12 16:02:21,051:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1)
2025-05-12 16:02:21,051:INFO:create_model() successfully completed......................................
2025-05-12 16:02:21,132:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=BayesianRidge(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:21,148:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:21,148:INFO:Initializing Huber w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:21,148:INFO:Total runtime is 0.31741498708724974 minutes
2025-05-12 16:02:21,148:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:21,148:INFO:Initializing create_model()
2025-05-12 16:02:21,148:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=huber_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:21,148:INFO:Checking exceptions
2025-05-12 16:02:21,148:INFO:Importing libraries
2025-05-12 16:02:21,148:INFO:Copying training dataset
2025-05-12 16:02:21,148:INFO:Defining folds
2025-05-12 16:02:21,148:INFO:Declaring metric variables
2025-05-12 16:02:21,148:INFO:Importing untrained model
2025-05-12 16:02:21,148:INFO:Huber w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:21,148:INFO:Starting cross validation
2025-05-12 16:02:21,156:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:21,248:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,248:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,248:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,248:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,248:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,248:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,264:INFO:Calculating mean and std
2025-05-12 16:02:21,264:INFO:Creating metrics dataframe
2025-05-12 16:02:21,264:INFO:Uploading results into container
2025-05-12 16:02:21,264:INFO:Uploading model into container now
2025-05-12 16:02:21,264:INFO:_master_model_container: 18
2025-05-12 16:02:21,264:INFO:_display_container: 2
2025-05-12 16:02:21,264:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1)
2025-05-12 16:02:21,264:INFO:create_model() successfully completed......................................
2025-05-12 16:02:21,349:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:21,364:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:21,364:INFO:Initializing create_model()
2025-05-12 16:02:21,364:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=huber_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:21,364:INFO:Checking exceptions
2025-05-12 16:02:21,364:INFO:Importing libraries
2025-05-12 16:02:21,364:INFO:Copying training dataset
2025-05-12 16:02:21,364:INFO:Defining folds
2025-05-12 16:02:21,364:INFO:Declaring metric variables
2025-05-12 16:02:21,364:INFO:Importing untrained model
2025-05-12 16:02:21,364:INFO:Huber w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:21,364:INFO:Starting cross validation
2025-05-12 16:02:21,364:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:21,464:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,464:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,464:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,464:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,464:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=HuberRegressor(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,464:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,481:INFO:Calculating mean and std
2025-05-12 16:02:21,481:INFO:Creating metrics dataframe
2025-05-12 16:02:21,481:INFO:Uploading results into container
2025-05-12 16:02:21,481:INFO:Uploading model into container now
2025-05-12 16:02:21,481:INFO:_master_model_container: 19
2025-05-12 16:02:21,481:INFO:_display_container: 2
2025-05-12 16:02:21,481:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1)
2025-05-12 16:02:21,481:INFO:create_model() successfully completed......................................
2025-05-12 16:02:21,566:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=HuberRegressor(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:21,566:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:21,566:INFO:Initializing Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:21,566:INFO:Total runtime is 0.324379026889801 minutes
2025-05-12 16:02:21,566:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:21,566:INFO:Initializing create_model()
2025-05-12 16:02:21,566:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=omp_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:21,566:INFO:Checking exceptions
2025-05-12 16:02:21,566:INFO:Importing libraries
2025-05-12 16:02:21,566:INFO:Copying training dataset
2025-05-12 16:02:21,566:INFO:Defining folds
2025-05-12 16:02:21,566:INFO:Declaring metric variables
2025-05-12 16:02:21,566:INFO:Importing untrained model
2025-05-12 16:02:21,566:INFO:Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:21,566:INFO:Starting cross validation
2025-05-12 16:02:21,566:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:21,664:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,664:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,664:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,664:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,664:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,664:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,696:INFO:Calculating mean and std
2025-05-12 16:02:21,696:INFO:Creating metrics dataframe
2025-05-12 16:02:21,696:INFO:Uploading results into container
2025-05-12 16:02:21,696:INFO:Uploading model into container now
2025-05-12 16:02:21,696:INFO:_master_model_container: 20
2025-05-12 16:02:21,696:INFO:_display_container: 2
2025-05-12 16:02:21,696:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1)
2025-05-12 16:02:21,696:INFO:create_model() successfully completed......................................
2025-05-12 16:02:21,796:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:21,796:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:21,796:INFO:Initializing create_model()
2025-05-12 16:02:21,796:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=omp_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:21,796:INFO:Checking exceptions
2025-05-12 16:02:21,796:INFO:Importing libraries
2025-05-12 16:02:21,796:INFO:Copying training dataset
2025-05-12 16:02:21,798:INFO:Defining folds
2025-05-12 16:02:21,798:INFO:Declaring metric variables
2025-05-12 16:02:21,798:INFO:Importing untrained model
2025-05-12 16:02:21,800:INFO:Orthogonal Matching Pursuit w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:21,802:INFO:Starting cross validation
2025-05-12 16:02:21,803:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:21,879:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,879:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,895:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,895:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,895:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=OrthogonalMatchingPursuit(),
                                                                                    window_length=1))]))])
2025-05-12 16:02:21,895:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:21,911:INFO:Calculating mean and std
2025-05-12 16:02:21,911:INFO:Creating metrics dataframe
2025-05-12 16:02:21,911:INFO:Uploading results into container
2025-05-12 16:02:21,911:INFO:Uploading model into container now
2025-05-12 16:02:21,911:INFO:_master_model_container: 21
2025-05-12 16:02:21,911:INFO:_display_container: 2
2025-05-12 16:02:21,911:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1)
2025-05-12 16:02:21,911:INFO:create_model() successfully completed......................................
2025-05-12 16:02:21,991:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=OrthogonalMatchingPursuit(), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:21,991:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:21,991:INFO:Initializing K Neighbors w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:21,991:INFO:Total runtime is 0.3314671834309896 minutes
2025-05-12 16:02:21,991:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:21,991:INFO:Initializing create_model()
2025-05-12 16:02:21,991:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=knn_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:21,991:INFO:Checking exceptions
2025-05-12 16:02:21,991:INFO:Importing libraries
2025-05-12 16:02:21,991:INFO:Copying training dataset
2025-05-12 16:02:21,991:INFO:Defining folds
2025-05-12 16:02:21,991:INFO:Declaring metric variables
2025-05-12 16:02:21,991:INFO:Importing untrained model
2025-05-12 16:02:21,991:INFO:K Neighbors w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:21,991:INFO:Starting cross validation
2025-05-12 16:02:21,991:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:22,087:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,087:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,087:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,087:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,087:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,087:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,103:INFO:Calculating mean and std
2025-05-12 16:02:22,103:INFO:Creating metrics dataframe
2025-05-12 16:02:22,103:INFO:Uploading results into container
2025-05-12 16:02:22,103:INFO:Uploading model into container now
2025-05-12 16:02:22,103:INFO:_master_model_container: 22
2025-05-12 16:02:22,103:INFO:_display_container: 2
2025-05-12 16:02:22,103:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1)
2025-05-12 16:02:22,103:INFO:create_model() successfully completed......................................
2025-05-12 16:02:22,179:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:22,179:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:22,179:INFO:Initializing create_model()
2025-05-12 16:02:22,179:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=knn_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:22,179:INFO:Checking exceptions
2025-05-12 16:02:22,179:INFO:Importing libraries
2025-05-12 16:02:22,179:INFO:Copying training dataset
2025-05-12 16:02:22,179:INFO:Defining folds
2025-05-12 16:02:22,179:INFO:Declaring metric variables
2025-05-12 16:02:22,179:INFO:Importing untrained model
2025-05-12 16:02:22,195:INFO:K Neighbors w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:22,195:INFO:Starting cross validation
2025-05-12 16:02:22,195:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:22,280:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,280:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,280:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,280:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,280:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=KNeighborsRegressor(n_jobs=-1),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,280:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,296:INFO:Calculating mean and std
2025-05-12 16:02:22,296:INFO:Creating metrics dataframe
2025-05-12 16:02:22,296:INFO:Uploading results into container
2025-05-12 16:02:22,296:INFO:Uploading model into container now
2025-05-12 16:02:22,296:INFO:_master_model_container: 23
2025-05-12 16:02:22,296:INFO:_display_container: 2
2025-05-12 16:02:22,296:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1)
2025-05-12 16:02:22,296:INFO:create_model() successfully completed......................................
2025-05-12 16:02:22,392:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=KNeighborsRegressor(n_jobs=-1), window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:22,396:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:22,396:INFO:Initializing Decision Tree w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:22,396:INFO:Total runtime is 0.3382136623064677 minutes
2025-05-12 16:02:22,396:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:22,396:INFO:Initializing create_model()
2025-05-12 16:02:22,396:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=dt_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:22,396:INFO:Checking exceptions
2025-05-12 16:02:22,396:INFO:Importing libraries
2025-05-12 16:02:22,396:INFO:Copying training dataset
2025-05-12 16:02:22,396:INFO:Defining folds
2025-05-12 16:02:22,408:INFO:Declaring metric variables
2025-05-12 16:02:22,408:INFO:Importing untrained model
2025-05-12 16:02:22,408:INFO:Decision Tree w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:22,408:INFO:Starting cross validation
2025-05-12 16:02:22,408:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:22,504:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,504:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,504:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,504:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,504:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,504:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,520:INFO:Calculating mean and std
2025-05-12 16:02:22,520:INFO:Creating metrics dataframe
2025-05-12 16:02:22,528:INFO:Uploading results into container
2025-05-12 16:02:22,528:INFO:Uploading model into container now
2025-05-12 16:02:22,528:INFO:_master_model_container: 24
2025-05-12 16:02:22,528:INFO:_display_container: 2
2025-05-12 16:02:22,532:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1)
2025-05-12 16:02:22,532:INFO:create_model() successfully completed......................................
2025-05-12 16:02:22,616:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:22,616:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:22,616:INFO:Initializing create_model()
2025-05-12 16:02:22,616:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=dt_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:22,616:INFO:Checking exceptions
2025-05-12 16:02:22,616:INFO:Importing libraries
2025-05-12 16:02:22,616:INFO:Copying training dataset
2025-05-12 16:02:22,616:INFO:Defining folds
2025-05-12 16:02:22,616:INFO:Declaring metric variables
2025-05-12 16:02:22,616:INFO:Importing untrained model
2025-05-12 16:02:22,616:INFO:Decision Tree w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:22,616:INFO:Starting cross validation
2025-05-12 16:02:22,616:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:22,714:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,714:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,714:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,714:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,714:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=DecisionTreeRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,714:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,730:INFO:Calculating mean and std
2025-05-12 16:02:22,730:INFO:Creating metrics dataframe
2025-05-12 16:02:22,730:INFO:Uploading results into container
2025-05-12 16:02:22,730:INFO:Uploading model into container now
2025-05-12 16:02:22,730:INFO:_master_model_container: 25
2025-05-12 16:02:22,730:INFO:_display_container: 2
2025-05-12 16:02:22,730:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1)
2025-05-12 16:02:22,730:INFO:create_model() successfully completed......................................
2025-05-12 16:02:22,826:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=DecisionTreeRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:22,826:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:22,826:INFO:Initializing Random Forest w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:22,826:INFO:Total runtime is 0.3453882217407227 minutes
2025-05-12 16:02:22,826:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:22,826:INFO:Initializing create_model()
2025-05-12 16:02:22,826:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=rf_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:22,826:INFO:Checking exceptions
2025-05-12 16:02:22,826:INFO:Importing libraries
2025-05-12 16:02:22,826:INFO:Copying training dataset
2025-05-12 16:02:22,826:INFO:Defining folds
2025-05-12 16:02:22,826:INFO:Declaring metric variables
2025-05-12 16:02:22,826:INFO:Importing untrained model
2025-05-12 16:02:22,826:INFO:Random Forest w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:22,826:INFO:Starting cross validation
2025-05-12 16:02:22,826:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:22,922:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,922:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,922:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,922:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,922:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:22,922:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:22,937:INFO:Calculating mean and std
2025-05-12 16:02:22,937:INFO:Creating metrics dataframe
2025-05-12 16:02:22,937:INFO:Uploading results into container
2025-05-12 16:02:22,937:INFO:Uploading model into container now
2025-05-12 16:02:22,937:INFO:_master_model_container: 26
2025-05-12 16:02:22,937:INFO:_display_container: 2
2025-05-12 16:02:22,937:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 16:02:22,937:INFO:create_model() successfully completed......................................
2025-05-12 16:02:23,017:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:23,017:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:23,017:INFO:Initializing create_model()
2025-05-12 16:02:23,017:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=rf_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:23,017:INFO:Checking exceptions
2025-05-12 16:02:23,017:INFO:Importing libraries
2025-05-12 16:02:23,017:INFO:Copying training dataset
2025-05-12 16:02:23,017:INFO:Defining folds
2025-05-12 16:02:23,017:INFO:Declaring metric variables
2025-05-12 16:02:23,017:INFO:Importing untrained model
2025-05-12 16:02:23,017:INFO:Random Forest w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:23,017:INFO:Starting cross validation
2025-05-12 16:02:23,017:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:23,129:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,129:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,129:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,129:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,129:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,129:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,145:INFO:Calculating mean and std
2025-05-12 16:02:23,145:INFO:Creating metrics dataframe
2025-05-12 16:02:23,145:INFO:Uploading results into container
2025-05-12 16:02:23,145:INFO:Uploading model into container now
2025-05-12 16:02:23,145:INFO:_master_model_container: 27
2025-05-12 16:02:23,145:INFO:_display_container: 2
2025-05-12 16:02:23,145:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 16:02:23,145:INFO:create_model() successfully completed......................................
2025-05-12 16:02:23,230:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=RandomForestRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:23,230:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:23,230:INFO:Initializing Extra Trees w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:23,230:INFO:Total runtime is 0.3521206498146058 minutes
2025-05-12 16:02:23,230:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:23,230:INFO:Initializing create_model()
2025-05-12 16:02:23,230:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=et_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:23,230:INFO:Checking exceptions
2025-05-12 16:02:23,230:INFO:Importing libraries
2025-05-12 16:02:23,230:INFO:Copying training dataset
2025-05-12 16:02:23,230:INFO:Defining folds
2025-05-12 16:02:23,230:INFO:Declaring metric variables
2025-05-12 16:02:23,230:INFO:Importing untrained model
2025-05-12 16:02:23,230:INFO:Extra Trees w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:23,230:INFO:Starting cross validation
2025-05-12 16:02:23,230:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:23,345:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,345:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,347:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,347:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,347:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,347:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,355:INFO:Calculating mean and std
2025-05-12 16:02:23,355:INFO:Creating metrics dataframe
2025-05-12 16:02:23,363:INFO:Uploading results into container
2025-05-12 16:02:23,363:INFO:Uploading model into container now
2025-05-12 16:02:23,363:INFO:_master_model_container: 28
2025-05-12 16:02:23,363:INFO:_display_container: 2
2025-05-12 16:02:23,365:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 16:02:23,365:INFO:create_model() successfully completed......................................
2025-05-12 16:02:23,451:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:23,451:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:23,451:INFO:Initializing create_model()
2025-05-12 16:02:23,451:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=et_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:23,451:INFO:Checking exceptions
2025-05-12 16:02:23,451:INFO:Importing libraries
2025-05-12 16:02:23,451:INFO:Copying training dataset
2025-05-12 16:02:23,451:INFO:Defining folds
2025-05-12 16:02:23,451:INFO:Declaring metric variables
2025-05-12 16:02:23,451:INFO:Importing untrained model
2025-05-12 16:02:23,451:INFO:Extra Trees w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:23,451:INFO:Starting cross validation
2025-05-12 16:02:23,451:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:23,546:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,546:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,546:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,546:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,546:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,546:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,562:INFO:Calculating mean and std
2025-05-12 16:02:23,562:INFO:Creating metrics dataframe
2025-05-12 16:02:23,562:INFO:Uploading results into container
2025-05-12 16:02:23,562:INFO:Uploading model into container now
2025-05-12 16:02:23,562:INFO:_master_model_container: 29
2025-05-12 16:02:23,562:INFO:_display_container: 2
2025-05-12 16:02:23,562:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 16:02:23,562:INFO:create_model() successfully completed......................................
2025-05-12 16:02:23,642:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=ExtraTreesRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:23,642:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:23,642:INFO:Initializing Gradient Boosting w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:23,642:INFO:Total runtime is 0.3589892188707988 minutes
2025-05-12 16:02:23,642:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:23,642:INFO:Initializing create_model()
2025-05-12 16:02:23,642:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=gbr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:23,642:INFO:Checking exceptions
2025-05-12 16:02:23,642:INFO:Importing libraries
2025-05-12 16:02:23,642:INFO:Copying training dataset
2025-05-12 16:02:23,642:INFO:Defining folds
2025-05-12 16:02:23,642:INFO:Declaring metric variables
2025-05-12 16:02:23,642:INFO:Importing untrained model
2025-05-12 16:02:23,642:INFO:Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:23,642:INFO:Starting cross validation
2025-05-12 16:02:23,658:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:23,757:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,757:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,757:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,757:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,757:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,757:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,764:INFO:Calculating mean and std
2025-05-12 16:02:23,764:INFO:Creating metrics dataframe
2025-05-12 16:02:23,764:INFO:Uploading results into container
2025-05-12 16:02:23,764:INFO:Uploading model into container now
2025-05-12 16:02:23,764:INFO:_master_model_container: 30
2025-05-12 16:02:23,764:INFO:_display_container: 2
2025-05-12 16:02:23,764:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1)
2025-05-12 16:02:23,764:INFO:create_model() successfully completed......................................
2025-05-12 16:02:23,860:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:23,860:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:23,860:INFO:Initializing create_model()
2025-05-12 16:02:23,860:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=gbr_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:23,860:INFO:Checking exceptions
2025-05-12 16:02:23,860:INFO:Importing libraries
2025-05-12 16:02:23,860:INFO:Copying training dataset
2025-05-12 16:02:23,860:INFO:Defining folds
2025-05-12 16:02:23,860:INFO:Declaring metric variables
2025-05-12 16:02:23,860:INFO:Importing untrained model
2025-05-12 16:02:23,860:INFO:Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:23,860:INFO:Starting cross validation
2025-05-12 16:02:23,860:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:23,955:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,955:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,955:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,955:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,955:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=GradientBoostingRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:23,955:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:23,971:INFO:Calculating mean and std
2025-05-12 16:02:23,971:INFO:Creating metrics dataframe
2025-05-12 16:02:23,971:INFO:Uploading results into container
2025-05-12 16:02:23,971:INFO:Uploading model into container now
2025-05-12 16:02:23,971:INFO:_master_model_container: 31
2025-05-12 16:02:23,971:INFO:_display_container: 2
2025-05-12 16:02:23,971:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1)
2025-05-12 16:02:23,971:INFO:create_model() successfully completed......................................
2025-05-12 16:02:24,051:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=GradientBoostingRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:24,051:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:24,051:INFO:Initializing AdaBoost w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:24,051:INFO:Total runtime is 0.3658028006553651 minutes
2025-05-12 16:02:24,051:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:24,051:INFO:Initializing create_model()
2025-05-12 16:02:24,051:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=ada_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:24,051:INFO:Checking exceptions
2025-05-12 16:02:24,051:INFO:Importing libraries
2025-05-12 16:02:24,051:INFO:Copying training dataset
2025-05-12 16:02:24,051:INFO:Defining folds
2025-05-12 16:02:24,051:INFO:Declaring metric variables
2025-05-12 16:02:24,051:INFO:Importing untrained model
2025-05-12 16:02:24,051:INFO:AdaBoost w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:24,051:INFO:Starting cross validation
2025-05-12 16:02:24,067:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:24,146:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,146:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:24,146:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,146:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:24,146:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,146:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:24,162:INFO:Calculating mean and std
2025-05-12 16:02:24,162:INFO:Creating metrics dataframe
2025-05-12 16:02:24,164:INFO:Uploading results into container
2025-05-12 16:02:24,164:INFO:Uploading model into container now
2025-05-12 16:02:24,164:INFO:_master_model_container: 32
2025-05-12 16:02:24,164:INFO:_display_container: 2
2025-05-12 16:02:24,166:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1)
2025-05-12 16:02:24,166:INFO:create_model() successfully completed......................................
2025-05-12 16:02:24,252:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:24,252:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:24,253:INFO:Initializing create_model()
2025-05-12 16:02:24,253:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=ada_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:24,253:INFO:Checking exceptions
2025-05-12 16:02:24,253:INFO:Importing libraries
2025-05-12 16:02:24,253:INFO:Copying training dataset
2025-05-12 16:02:24,255:INFO:Defining folds
2025-05-12 16:02:24,255:INFO:Declaring metric variables
2025-05-12 16:02:24,256:INFO:Importing untrained model
2025-05-12 16:02:24,257:INFO:AdaBoost w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:24,258:INFO:Starting cross validation
2025-05-12 16:02:24,259:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:24,345:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,345:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:24,345:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,345:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:24,345:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=AdaBoostRegressor(random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,345:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>)
2025-05-12 16:02:24,361:INFO:Calculating mean and std
2025-05-12 16:02:24,361:INFO:Creating metrics dataframe
2025-05-12 16:02:24,363:INFO:Uploading results into container
2025-05-12 16:02:24,363:INFO:Uploading model into container now
2025-05-12 16:02:24,365:INFO:_master_model_container: 33
2025-05-12 16:02:24,365:INFO:_display_container: 2
2025-05-12 16:02:24,365:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1)
2025-05-12 16:02:24,365:INFO:create_model() successfully completed......................................
2025-05-12 16:02:24,440:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=AdaBoostRegressor(random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:24,440:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:24,440:INFO:Initializing Light Gradient Boosting w/ Cond. Deseasonalize & Detrending
2025-05-12 16:02:24,440:INFO:Total runtime is 0.3722936431566875 minutes
2025-05-12 16:02:24,440:INFO:SubProcess create_model() called ==================================
2025-05-12 16:02:24,440:INFO:Initializing create_model()
2025-05-12 16:02:24,440:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=lightgbm_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:24,440:INFO:Checking exceptions
2025-05-12 16:02:24,440:INFO:Importing libraries
2025-05-12 16:02:24,440:INFO:Copying training dataset
2025-05-12 16:02:24,440:INFO:Defining folds
2025-05-12 16:02:24,440:INFO:Declaring metric variables
2025-05-12 16:02:24,440:INFO:Importing untrained model
2025-05-12 16:02:24,456:INFO:Light Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:24,456:INFO:Starting cross validation
2025-05-12 16:02:24,456:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:24,630:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,630:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 16:02:24,630:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,630:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 16:02:24,630:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,630:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 16:02:24,647:INFO:Calculating mean and std
2025-05-12 16:02:24,647:INFO:Creating metrics dataframe
2025-05-12 16:02:24,647:INFO:Uploading results into container
2025-05-12 16:02:24,647:INFO:Uploading model into container now
2025-05-12 16:02:24,647:INFO:_master_model_container: 34
2025-05-12 16:02:24,647:INFO:_display_container: 2
2025-05-12 16:02:24,647:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 16:02:24,647:INFO:create_model() successfully completed......................................
2025-05-12 16:02:24,742:WARNING:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0, trying without fit_kwargs:
2025-05-12 16:02:24,742:WARNING:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:24,742:INFO:Initializing create_model()
2025-05-12 16:02:24,742:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=lightgbm_cds_dt, fold=None, round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001F605B6FF40>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:24,742:INFO:Checking exceptions
2025-05-12 16:02:24,742:INFO:Importing libraries
2025-05-12 16:02:24,742:INFO:Copying training dataset
2025-05-12 16:02:24,742:INFO:Defining folds
2025-05-12 16:02:24,742:INFO:Declaring metric variables
2025-05-12 16:02:24,742:INFO:Importing untrained model
2025-05-12 16:02:24,742:INFO:Light Gradient Boosting w/ Cond. Deseasonalize & Detrending Imported successfully
2025-05-12 16:02:24,742:INFO:Starting cross validation
2025-05-12 16:02:24,742:INFO:Cross validating with ExpandingWindowSplitter(fh=ForecastingHorizon([1], dtype='int32', is_relative=True),
                        initial_window=497), n_jobs=-1
2025-05-12 16:02:24,926:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,926:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 16:02:24,926:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,926:ERROR:Fit failed on ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                                                                                   n_jobs=1)],
                                                                                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                                                                                    window_length=1))]))])
2025-05-12 16:02:24,926:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 16:02:24,926:ERROR:The DType <class 'numpy.dtypes.Float64DType'> could not be promoted by <class 'numpy.dtypes.DateTime64DType'>. This means that no common DType exists for the given inputs. For example they cannot be stored in a single array unless the dtype is `object`. The full list of DTypes is: (<class 'numpy.dtypes.Float64DType'>, <class 'numpy.dtypes.DateTime64DType'>, <class 'numpy.dtypes.Float32DType'>)
2025-05-12 16:02:24,942:INFO:Calculating mean and std
2025-05-12 16:02:24,942:INFO:Creating metrics dataframe
2025-05-12 16:02:24,942:INFO:Uploading results into container
2025-05-12 16:02:24,942:INFO:Uploading model into container now
2025-05-12 16:02:24,942:INFO:_master_model_container: 35
2025-05-12 16:02:24,942:INFO:_display_container: 2
2025-05-12 16:02:24,942:INFO:BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1)
2025-05-12 16:02:24,942:INFO:create_model() successfully completed......................................
2025-05-12 16:02:25,037:ERROR:create_model() for BaseCdsDtForecaster(fe_target_rr=[WindowSummarizer(lag_feature={'lag': [1]},
                                                   n_jobs=1)],
                    regressor=LGBMRegressor(n_jobs=-1, random_state=123),
                    window_length=1) raised an exception or returned all 0.0:
2025-05-12 16:02:25,037:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 797, in compare_models
    np.sum(
AssertionError

During handling of the above exception, another exception occurred:

Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 818, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:25,037:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-05-12 16:02:25,037:INFO:Initializing create_model()
2025-05-12 16:02:25,037:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=Croston(), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:25,037:INFO:Checking exceptions
2025-05-12 16:02:25,037:INFO:Importing libraries
2025-05-12 16:02:25,037:INFO:Copying training dataset
2025-05-12 16:02:25,037:INFO:Defining folds
2025-05-12 16:02:25,037:INFO:Declaring metric variables
2025-05-12 16:02:25,037:INFO:Importing untrained model
2025-05-12 16:02:25,037:INFO:Declaring custom model
2025-05-12 16:02:25,037:INFO:Croston Imported successfully
2025-05-12 16:02:25,037:INFO:Cross validation set to False
2025-05-12 16:02:25,037:INFO:Fitting Model
2025-05-12 16:02:25,053:INFO:Croston()
2025-05-12 16:02:25,053:INFO:create_model() successfully completed......................................
2025-05-12 16:02:25,132:ERROR:create_model() for Croston() raised an exception or returned all 0.0:
2025-05-12 16:02:25,132:ERROR:Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py", line 954, in compare_models
    np.sum(
AssertionError

2025-05-12 16:02:25,148:INFO:_master_model_container: 35
2025-05-12 16:02:25,148:INFO:_display_container: 2
2025-05-12 16:02:25,148:INFO:Croston()
2025-05-12 16:02:25,148:INFO:compare_models() successfully completed......................................
2025-05-12 16:02:25,148:INFO:Initializing finalize_model()
2025-05-12 16:02:25,148:INFO:finalize_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=Croston(), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-05-12 16:02:25,148:INFO:Finalizing Croston()
2025-05-12 16:02:25,148:INFO:Initializing create_model()
2025-05-12 16:02:25,148:INFO:create_model(self=<pycaret.time_series.forecasting.oop.TSForecastingExperiment object at 0x000001F648C59FF0>, estimator=Croston(), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:02:25,148:INFO:Checking exceptions
2025-05-12 16:02:25,148:INFO:Importing libraries
2025-05-12 16:02:25,148:INFO:Copying training dataset
2025-05-12 16:02:25,148:INFO:Defining folds
2025-05-12 16:02:25,148:INFO:Declaring metric variables
2025-05-12 16:02:25,148:INFO:Importing untrained model
2025-05-12 16:02:25,148:INFO:Declaring custom model
2025-05-12 16:02:25,148:INFO:Croston Imported successfully
2025-05-12 16:02:25,148:INFO:Cross validation set to False
2025-05-12 16:02:25,148:INFO:Fitting Model
2025-05-12 16:02:25,171:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))])
2025-05-12 16:02:25,171:INFO:create_model() successfully completed......................................
2025-05-12 16:02:25,260:INFO:_master_model_container: 35
2025-05-12 16:02:25,260:INFO:_display_container: 2
2025-05-12 16:02:25,262:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))])
2025-05-12 16:02:25,262:INFO:finalize_model() successfully completed......................................
2025-05-12 16:02:25,350:INFO:Initializing save_model()
2025-05-12 16:02:25,350:INFO:save_model(model=ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                Croston())]))]), model_name=aapl_best_model, prep_pipe_=ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                DummyForecaster())]))]), verbose=True, use_case=MLUsecase.TIME_SERIES, kwargs={})
2025-05-12 16:02:25,350:INFO:Adding model into prep_pipe
2025-05-12 16:02:25,350:INFO:aapl_best_model.pkl saved in current working directory
2025-05-12 16:02:25,369:INFO:ForecastingPipeline(steps=[('forecaster',
                            TransformedTargetForecaster(steps=[('model',
                                                                ForecastingPipeline(steps=[('forecaster',
                                                                                            TransformedTargetForecaster(steps=[('model',
                                                                                                                                Croston())]))]))]))])
2025-05-12 16:02:25,369:INFO:save_model() successfully completed......................................
2025-05-12 16:04:43,190:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:04:44,243:INFO:Initializing load_model()
2025-05-12 16:04:44,243:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 16:06:03,597:INFO:Initializing load_model()
2025-05-12 16:06:03,597:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 16:06:32,216:INFO:Initializing load_model()
2025-05-12 16:06:32,216:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 16:12:32,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:12:32,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:12:32,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:12:32,461:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:13:13,791:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:13:13,791:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:13:13,791:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:13:13,791:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:13:14,906:INFO:PyCaret ClassificationExperiment
2025-05-12 16:13:14,906:INFO:Logging name: clf-default-name
2025-05-12 16:13:14,906:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-12 16:13:14,906:INFO:version 3.3.2
2025-05-12 16:13:14,906:INFO:Initializing setup()
2025-05-12 16:13:14,906:INFO:self.USI: 9245
2025-05-12 16:13:14,906:INFO:self._variable_keys: {'_ml_usecase', 'X', 'target_param', 'y_train', 'memory', 'is_multiclass', 'html_param', 'gpu_param', 'seed', 'fold_groups_param', 'exp_id', 'fold_generator', 'logging_param', '_available_plots', 'y_test', 'pipeline', 'X_train', 'X_test', 'log_plots_param', 'USI', 'fix_imbalance', 'y', 'fold_shuffle_param', 'exp_name_log', 'n_jobs_param', 'data', 'gpu_n_jobs_param', 'idx'}
2025-05-12 16:13:14,906:INFO:Checking environment
2025-05-12 16:13:14,906:INFO:python_version: 3.10.16
2025-05-12 16:13:14,906:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-05-12 16:13:14,906:INFO:machine: AMD64
2025-05-12 16:13:14,927:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-12 16:13:14,927:INFO:Memory: svmem(total=17009004544, available=4542889984, percent=73.3, used=12466114560, free=4542889984)
2025-05-12 16:13:14,927:INFO:Physical Core: 6
2025-05-12 16:13:14,927:INFO:Logical Core: 12
2025-05-12 16:13:14,927:INFO:Checking libraries
2025-05-12 16:13:14,927:INFO:System:
2025-05-12 16:13:14,927:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-05-12 16:13:14,927:INFO:executable: C:\Users\Thayse\.conda\envs\mlpipeline\python.exe
2025-05-12 16:13:14,927:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-12 16:13:14,927:INFO:PyCaret required dependencies:
2025-05-12 16:13:15,049:INFO:                 pip: 25.0
2025-05-12 16:13:15,049:INFO:          setuptools: 75.8.0
2025-05-12 16:13:15,049:INFO:             pycaret: 3.3.2
2025-05-12 16:13:15,049:INFO:             IPython: 8.34.0
2025-05-12 16:13:15,049:INFO:          ipywidgets: 8.1.5
2025-05-12 16:13:15,049:INFO:                tqdm: 4.67.1
2025-05-12 16:13:15,049:INFO:               numpy: 1.26.4
2025-05-12 16:13:15,049:INFO:              pandas: 2.1.4
2025-05-12 16:13:15,049:INFO:              jinja2: 3.1.6
2025-05-12 16:13:15,049:INFO:               scipy: 1.11.4
2025-05-12 16:13:15,049:INFO:              joblib: 1.3.2
2025-05-12 16:13:15,049:INFO:             sklearn: 1.4.2
2025-05-12 16:13:15,049:INFO:                pyod: 2.0.4
2025-05-12 16:13:15,049:INFO:            imblearn: 0.13.0
2025-05-12 16:13:15,049:INFO:   category_encoders: 2.7.0
2025-05-12 16:13:15,049:INFO:            lightgbm: 4.6.0
2025-05-12 16:13:15,049:INFO:               numba: 0.61.0
2025-05-12 16:13:15,049:INFO:            requests: 2.32.3
2025-05-12 16:13:15,049:INFO:          matplotlib: 3.7.5
2025-05-12 16:13:15,049:INFO:          scikitplot: 0.3.7
2025-05-12 16:13:15,049:INFO:         yellowbrick: 1.5
2025-05-12 16:13:15,049:INFO:              plotly: 5.24.1
2025-05-12 16:13:15,049:INFO:    plotly-resampler: Not installed
2025-05-12 16:13:15,049:INFO:             kaleido: 0.2.1
2025-05-12 16:13:15,049:INFO:           schemdraw: 0.15
2025-05-12 16:13:15,049:INFO:         statsmodels: 0.14.4
2025-05-12 16:13:15,049:INFO:              sktime: 0.26.0
2025-05-12 16:13:15,049:INFO:               tbats: 1.1.3
2025-05-12 16:13:15,049:INFO:            pmdarima: 2.0.4
2025-05-12 16:13:15,049:INFO:              psutil: 7.0.0
2025-05-12 16:13:15,049:INFO:          markupsafe: 3.0.2
2025-05-12 16:13:15,049:INFO:             pickle5: Not installed
2025-05-12 16:13:15,049:INFO:         cloudpickle: 3.1.1
2025-05-12 16:13:15,049:INFO:         deprecation: 2.1.0
2025-05-12 16:13:15,049:INFO:              xxhash: 3.5.0
2025-05-12 16:13:15,049:INFO:           wurlitzer: Not installed
2025-05-12 16:13:15,049:INFO:PyCaret optional dependencies:
2025-05-12 16:13:15,438:INFO:                shap: Not installed
2025-05-12 16:13:15,438:INFO:           interpret: Not installed
2025-05-12 16:13:15,438:INFO:                umap: Not installed
2025-05-12 16:13:15,438:INFO:     ydata_profiling: 4.16.1
2025-05-12 16:13:15,438:INFO:  explainerdashboard: Not installed
2025-05-12 16:13:15,438:INFO:             autoviz: Not installed
2025-05-12 16:13:15,438:INFO:           fairlearn: Not installed
2025-05-12 16:13:15,438:INFO:          deepchecks: Not installed
2025-05-12 16:13:15,438:INFO:             xgboost: Not installed
2025-05-12 16:13:15,438:INFO:            catboost: Not installed
2025-05-12 16:13:15,438:INFO:              kmodes: Not installed
2025-05-12 16:13:15,438:INFO:             mlxtend: Not installed
2025-05-12 16:13:15,438:INFO:       statsforecast: Not installed
2025-05-12 16:13:15,438:INFO:        tune_sklearn: Not installed
2025-05-12 16:13:15,438:INFO:                 ray: Not installed
2025-05-12 16:13:15,438:INFO:            hyperopt: Not installed
2025-05-12 16:13:15,438:INFO:              optuna: 4.2.1
2025-05-12 16:13:15,438:INFO:               skopt: Not installed
2025-05-12 16:13:15,438:INFO:              mlflow: Not installed
2025-05-12 16:13:15,438:INFO:              gradio: Not installed
2025-05-12 16:13:15,438:INFO:             fastapi: 0.115.12
2025-05-12 16:13:15,438:INFO:             uvicorn: 0.34.2
2025-05-12 16:13:15,438:INFO:              m2cgen: Not installed
2025-05-12 16:13:15,438:INFO:           evidently: Not installed
2025-05-12 16:13:15,438:INFO:               fugue: Not installed
2025-05-12 16:13:15,438:INFO:           streamlit: 1.44.1
2025-05-12 16:13:15,438:INFO:             prophet: 1.1.6
2025-05-12 16:13:15,438:INFO:None
2025-05-12 16:13:15,438:INFO:Set up data.
2025-05-12 16:13:15,438:INFO:Set up folding strategy.
2025-05-12 16:13:15,438:INFO:Set up train/test split.
2025-05-12 16:13:15,438:INFO:Set up index.
2025-05-12 16:13:15,438:INFO:Assigning column types.
2025-05-12 16:13:15,438:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-12 16:13:15,486:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-12 16:13:15,486:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:13:15,517:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,517:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,549:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-12 16:13:15,549:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:13:15,581:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,581:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,581:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-12 16:13:15,613:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:13:15,629:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,644:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,677:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:13:15,704:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,704:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,704:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-12 16:13:15,767:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,767:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,831:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,831:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:15,831:INFO:Preparing preprocessing pipeline...
2025-05-12 16:13:15,831:INFO:Set up simple imputation.
2025-05-12 16:13:15,831:INFO:Set up column name cleaning.
2025-05-12 16:13:15,847:INFO:Finished creating preprocessing pipeline.
2025-05-12 16:13:15,847:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Thayse\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-12 16:13:15,847:INFO:Creating final display dataframe.
2025-05-12 16:13:15,927:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (150, 5)
4        Transformed data shape          (150, 5)
5   Transformed train set shape          (105, 5)
6    Transformed test set shape           (45, 5)
7              Numeric features                 4
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              9245
2025-05-12 16:13:16,002:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:16,002:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:16,053:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:16,069:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:13:16,069:INFO:setup() successfully completed in 1.16s...............
2025-05-12 16:13:16,069:INFO:Initializing compare_models()
2025-05-12 16:13:16,069:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-05-12 16:13:16,069:INFO:Checking exceptions
2025-05-12 16:13:16,069:INFO:Preparing display monitor
2025-05-12 16:13:16,069:INFO:Initializing Logistic Regression
2025-05-12 16:13:16,069:INFO:Total runtime is 0.0 minutes
2025-05-12 16:13:16,069:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:16,069:INFO:Initializing create_model()
2025-05-12 16:13:16,069:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:16,069:INFO:Checking exceptions
2025-05-12 16:13:16,069:INFO:Importing libraries
2025-05-12 16:13:16,069:INFO:Copying training dataset
2025-05-12 16:13:16,069:INFO:Defining folds
2025-05-12 16:13:16,069:INFO:Declaring metric variables
2025-05-12 16:13:16,069:INFO:Importing untrained model
2025-05-12 16:13:16,069:INFO:Logistic Regression Imported successfully
2025-05-12 16:13:16,069:INFO:Starting cross validation
2025-05-12 16:13:16,069:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:22,650:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,689:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,701:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,703:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,715:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,727:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,729:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,732:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,770:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,780:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:22,815:INFO:Calculating mean and std
2025-05-12 16:13:22,835:INFO:Creating metrics dataframe
2025-05-12 16:13:22,841:INFO:Uploading results into container
2025-05-12 16:13:22,843:INFO:Uploading model into container now
2025-05-12 16:13:22,843:INFO:_master_model_container: 1
2025-05-12 16:13:22,845:INFO:_display_container: 2
2025-05-12 16:13:22,845:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:13:22,845:INFO:create_model() successfully completed......................................
2025-05-12 16:13:22,969:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:22,969:INFO:Creating metrics dataframe
2025-05-12 16:13:22,969:INFO:Initializing K Neighbors Classifier
2025-05-12 16:13:22,969:INFO:Total runtime is 0.11499999364217123 minutes
2025-05-12 16:13:22,969:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:22,969:INFO:Initializing create_model()
2025-05-12 16:13:22,969:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:22,969:INFO:Checking exceptions
2025-05-12 16:13:22,969:INFO:Importing libraries
2025-05-12 16:13:22,969:INFO:Copying training dataset
2025-05-12 16:13:22,969:INFO:Defining folds
2025-05-12 16:13:22,969:INFO:Declaring metric variables
2025-05-12 16:13:22,969:INFO:Importing untrained model
2025-05-12 16:13:22,969:INFO:K Neighbors Classifier Imported successfully
2025-05-12 16:13:22,969:INFO:Starting cross validation
2025-05-12 16:13:22,969:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:25,790:INFO:Calculating mean and std
2025-05-12 16:13:25,792:INFO:Creating metrics dataframe
2025-05-12 16:13:25,794:INFO:Uploading results into container
2025-05-12 16:13:25,795:INFO:Uploading model into container now
2025-05-12 16:13:25,795:INFO:_master_model_container: 2
2025-05-12 16:13:25,795:INFO:_display_container: 2
2025-05-12 16:13:25,796:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-05-12 16:13:25,796:INFO:create_model() successfully completed......................................
2025-05-12 16:13:25,887:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:25,887:INFO:Creating metrics dataframe
2025-05-12 16:13:25,890:INFO:Initializing Naive Bayes
2025-05-12 16:13:25,890:INFO:Total runtime is 0.16368057330449423 minutes
2025-05-12 16:13:25,891:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:25,891:INFO:Initializing create_model()
2025-05-12 16:13:25,891:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:25,891:INFO:Checking exceptions
2025-05-12 16:13:25,891:INFO:Importing libraries
2025-05-12 16:13:25,891:INFO:Copying training dataset
2025-05-12 16:13:25,895:INFO:Defining folds
2025-05-12 16:13:25,895:INFO:Declaring metric variables
2025-05-12 16:13:25,895:INFO:Importing untrained model
2025-05-12 16:13:25,895:INFO:Naive Bayes Imported successfully
2025-05-12 16:13:25,896:INFO:Starting cross validation
2025-05-12 16:13:25,897:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:25,991:INFO:Calculating mean and std
2025-05-12 16:13:25,993:INFO:Creating metrics dataframe
2025-05-12 16:13:25,997:INFO:Uploading results into container
2025-05-12 16:13:25,998:INFO:Uploading model into container now
2025-05-12 16:13:25,999:INFO:_master_model_container: 3
2025-05-12 16:13:25,999:INFO:_display_container: 2
2025-05-12 16:13:25,999:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-05-12 16:13:25,999:INFO:create_model() successfully completed......................................
2025-05-12 16:13:26,070:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:26,070:INFO:Creating metrics dataframe
2025-05-12 16:13:26,073:INFO:Initializing Decision Tree Classifier
2025-05-12 16:13:26,074:INFO:Total runtime is 0.1667512575785319 minutes
2025-05-12 16:13:26,074:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:26,074:INFO:Initializing create_model()
2025-05-12 16:13:26,074:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:26,074:INFO:Checking exceptions
2025-05-12 16:13:26,074:INFO:Importing libraries
2025-05-12 16:13:26,074:INFO:Copying training dataset
2025-05-12 16:13:26,078:INFO:Defining folds
2025-05-12 16:13:26,078:INFO:Declaring metric variables
2025-05-12 16:13:26,078:INFO:Importing untrained model
2025-05-12 16:13:26,078:INFO:Decision Tree Classifier Imported successfully
2025-05-12 16:13:26,079:INFO:Starting cross validation
2025-05-12 16:13:26,079:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:26,158:INFO:Calculating mean and std
2025-05-12 16:13:26,158:INFO:Creating metrics dataframe
2025-05-12 16:13:26,160:INFO:Uploading results into container
2025-05-12 16:13:26,161:INFO:Uploading model into container now
2025-05-12 16:13:26,161:INFO:_master_model_container: 4
2025-05-12 16:13:26,161:INFO:_display_container: 2
2025-05-12 16:13:26,161:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-05-12 16:13:26,161:INFO:create_model() successfully completed......................................
2025-05-12 16:13:26,230:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:26,230:INFO:Creating metrics dataframe
2025-05-12 16:13:26,230:INFO:Initializing SVM - Linear Kernel
2025-05-12 16:13:26,230:INFO:Total runtime is 0.16934467156728109 minutes
2025-05-12 16:13:26,230:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:26,230:INFO:Initializing create_model()
2025-05-12 16:13:26,230:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:26,230:INFO:Checking exceptions
2025-05-12 16:13:26,230:INFO:Importing libraries
2025-05-12 16:13:26,230:INFO:Copying training dataset
2025-05-12 16:13:26,230:INFO:Defining folds
2025-05-12 16:13:26,230:INFO:Declaring metric variables
2025-05-12 16:13:26,230:INFO:Importing untrained model
2025-05-12 16:13:26,230:INFO:SVM - Linear Kernel Imported successfully
2025-05-12 16:13:26,230:INFO:Starting cross validation
2025-05-12 16:13:26,230:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:26,296:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,296:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,309:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,316:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,318:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:26,328:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,328:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,328:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,329:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,329:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,331:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,331:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:26,331:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:26,331:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:26,342:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:26,360:INFO:Calculating mean and std
2025-05-12 16:13:26,361:INFO:Creating metrics dataframe
2025-05-12 16:13:26,361:INFO:Uploading results into container
2025-05-12 16:13:26,361:INFO:Uploading model into container now
2025-05-12 16:13:26,361:INFO:_master_model_container: 5
2025-05-12 16:13:26,361:INFO:_display_container: 2
2025-05-12 16:13:26,361:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-05-12 16:13:26,361:INFO:create_model() successfully completed......................................
2025-05-12 16:13:26,427:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:26,427:INFO:Creating metrics dataframe
2025-05-12 16:13:26,427:INFO:Initializing Ridge Classifier
2025-05-12 16:13:26,427:INFO:Total runtime is 0.17264198064804076 minutes
2025-05-12 16:13:26,427:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:26,427:INFO:Initializing create_model()
2025-05-12 16:13:26,427:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:26,427:INFO:Checking exceptions
2025-05-12 16:13:26,427:INFO:Importing libraries
2025-05-12 16:13:26,427:INFO:Copying training dataset
2025-05-12 16:13:26,427:INFO:Defining folds
2025-05-12 16:13:26,427:INFO:Declaring metric variables
2025-05-12 16:13:26,427:INFO:Importing untrained model
2025-05-12 16:13:26,427:INFO:Ridge Classifier Imported successfully
2025-05-12 16:13:26,427:INFO:Starting cross validation
2025-05-12 16:13:26,441:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:26,478:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,478:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,478:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,478:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,495:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,495:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,495:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,495:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,497:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,499:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:26,527:INFO:Calculating mean and std
2025-05-12 16:13:26,530:INFO:Creating metrics dataframe
2025-05-12 16:13:26,530:INFO:Uploading results into container
2025-05-12 16:13:26,530:INFO:Uploading model into container now
2025-05-12 16:13:26,530:INFO:_master_model_container: 6
2025-05-12 16:13:26,530:INFO:_display_container: 2
2025-05-12 16:13:26,530:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-05-12 16:13:26,530:INFO:create_model() successfully completed......................................
2025-05-12 16:13:26,611:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:26,611:INFO:Creating metrics dataframe
2025-05-12 16:13:26,611:INFO:Initializing Random Forest Classifier
2025-05-12 16:13:26,611:INFO:Total runtime is 0.1757000724474589 minutes
2025-05-12 16:13:26,611:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:26,611:INFO:Initializing create_model()
2025-05-12 16:13:26,611:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:26,611:INFO:Checking exceptions
2025-05-12 16:13:26,611:INFO:Importing libraries
2025-05-12 16:13:26,611:INFO:Copying training dataset
2025-05-12 16:13:26,611:INFO:Defining folds
2025-05-12 16:13:26,611:INFO:Declaring metric variables
2025-05-12 16:13:26,611:INFO:Importing untrained model
2025-05-12 16:13:26,611:INFO:Random Forest Classifier Imported successfully
2025-05-12 16:13:26,611:INFO:Starting cross validation
2025-05-12 16:13:26,611:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:27,121:INFO:Calculating mean and std
2025-05-12 16:13:27,121:INFO:Creating metrics dataframe
2025-05-12 16:13:27,121:INFO:Uploading results into container
2025-05-12 16:13:27,121:INFO:Uploading model into container now
2025-05-12 16:13:27,121:INFO:_master_model_container: 7
2025-05-12 16:13:27,121:INFO:_display_container: 2
2025-05-12 16:13:27,121:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-05-12 16:13:27,121:INFO:create_model() successfully completed......................................
2025-05-12 16:13:27,191:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:27,191:INFO:Creating metrics dataframe
2025-05-12 16:13:27,209:INFO:Initializing Quadratic Discriminant Analysis
2025-05-12 16:13:27,210:INFO:Total runtime is 0.18569196065266927 minutes
2025-05-12 16:13:27,210:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:27,211:INFO:Initializing create_model()
2025-05-12 16:13:27,211:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:27,211:INFO:Checking exceptions
2025-05-12 16:13:27,211:INFO:Importing libraries
2025-05-12 16:13:27,211:INFO:Copying training dataset
2025-05-12 16:13:27,211:INFO:Defining folds
2025-05-12 16:13:27,211:INFO:Declaring metric variables
2025-05-12 16:13:27,211:INFO:Importing untrained model
2025-05-12 16:13:27,211:INFO:Quadratic Discriminant Analysis Imported successfully
2025-05-12 16:13:27,211:INFO:Starting cross validation
2025-05-12 16:13:27,211:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:27,266:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,268:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,268:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,270:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,273:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,275:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,280:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,284:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,284:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,284:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,309:INFO:Calculating mean and std
2025-05-12 16:13:27,313:INFO:Creating metrics dataframe
2025-05-12 16:13:27,317:INFO:Uploading results into container
2025-05-12 16:13:27,318:INFO:Uploading model into container now
2025-05-12 16:13:27,319:INFO:_master_model_container: 8
2025-05-12 16:13:27,319:INFO:_display_container: 2
2025-05-12 16:13:27,320:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-05-12 16:13:27,320:INFO:create_model() successfully completed......................................
2025-05-12 16:13:27,393:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:27,393:INFO:Creating metrics dataframe
2025-05-12 16:13:27,399:INFO:Initializing Ada Boost Classifier
2025-05-12 16:13:27,399:INFO:Total runtime is 0.18883321285247803 minutes
2025-05-12 16:13:27,399:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:27,399:INFO:Initializing create_model()
2025-05-12 16:13:27,399:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:27,399:INFO:Checking exceptions
2025-05-12 16:13:27,399:INFO:Importing libraries
2025-05-12 16:13:27,399:INFO:Copying training dataset
2025-05-12 16:13:27,399:INFO:Defining folds
2025-05-12 16:13:27,399:INFO:Declaring metric variables
2025-05-12 16:13:27,399:INFO:Importing untrained model
2025-05-12 16:13:27,399:INFO:Ada Boost Classifier Imported successfully
2025-05-12 16:13:27,399:INFO:Starting cross validation
2025-05-12 16:13:27,399:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:27,431:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,431:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,435:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,437:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,439:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,441:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,443:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,443:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,445:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,447:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:13:27,607:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,609:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,631:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,631:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,631:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,631:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,631:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,631:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,631:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,645:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:27,660:INFO:Calculating mean and std
2025-05-12 16:13:27,660:INFO:Creating metrics dataframe
2025-05-12 16:13:27,662:INFO:Uploading results into container
2025-05-12 16:13:27,662:INFO:Uploading model into container now
2025-05-12 16:13:27,662:INFO:_master_model_container: 9
2025-05-12 16:13:27,662:INFO:_display_container: 2
2025-05-12 16:13:27,664:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-05-12 16:13:27,664:INFO:create_model() successfully completed......................................
2025-05-12 16:13:27,731:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:27,731:INFO:Creating metrics dataframe
2025-05-12 16:13:27,733:INFO:Initializing Gradient Boosting Classifier
2025-05-12 16:13:27,733:INFO:Total runtime is 0.19440681139628094 minutes
2025-05-12 16:13:27,733:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:27,733:INFO:Initializing create_model()
2025-05-12 16:13:27,733:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:27,734:INFO:Checking exceptions
2025-05-12 16:13:27,734:INFO:Importing libraries
2025-05-12 16:13:27,734:INFO:Copying training dataset
2025-05-12 16:13:27,736:INFO:Defining folds
2025-05-12 16:13:27,736:INFO:Declaring metric variables
2025-05-12 16:13:27,736:INFO:Importing untrained model
2025-05-12 16:13:27,737:INFO:Gradient Boosting Classifier Imported successfully
2025-05-12 16:13:27,737:INFO:Starting cross validation
2025-05-12 16:13:27,738:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:28,237:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,273:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,308:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,308:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,308:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,308:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,308:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,332:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,347:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,348:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,357:INFO:Calculating mean and std
2025-05-12 16:13:28,358:INFO:Creating metrics dataframe
2025-05-12 16:13:28,360:INFO:Uploading results into container
2025-05-12 16:13:28,360:INFO:Uploading model into container now
2025-05-12 16:13:28,361:INFO:_master_model_container: 10
2025-05-12 16:13:28,361:INFO:_display_container: 2
2025-05-12 16:13:28,361:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-05-12 16:13:28,361:INFO:create_model() successfully completed......................................
2025-05-12 16:13:28,427:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:28,427:INFO:Creating metrics dataframe
2025-05-12 16:13:28,430:INFO:Initializing Linear Discriminant Analysis
2025-05-12 16:13:28,430:INFO:Total runtime is 0.2060259540875753 minutes
2025-05-12 16:13:28,430:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:28,430:INFO:Initializing create_model()
2025-05-12 16:13:28,430:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:28,431:INFO:Checking exceptions
2025-05-12 16:13:28,431:INFO:Importing libraries
2025-05-12 16:13:28,431:INFO:Copying training dataset
2025-05-12 16:13:28,434:INFO:Defining folds
2025-05-12 16:13:28,434:INFO:Declaring metric variables
2025-05-12 16:13:28,434:INFO:Importing untrained model
2025-05-12 16:13:28,435:INFO:Linear Discriminant Analysis Imported successfully
2025-05-12 16:13:28,435:INFO:Starting cross validation
2025-05-12 16:13:28,436:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:28,477:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,479:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,481:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,483:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,485:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,489:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,493:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,493:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,495:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,495:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:13:28,523:INFO:Calculating mean and std
2025-05-12 16:13:28,523:INFO:Creating metrics dataframe
2025-05-12 16:13:28,523:INFO:Uploading results into container
2025-05-12 16:13:28,523:INFO:Uploading model into container now
2025-05-12 16:13:28,523:INFO:_master_model_container: 11
2025-05-12 16:13:28,523:INFO:_display_container: 2
2025-05-12 16:13:28,523:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-05-12 16:13:28,523:INFO:create_model() successfully completed......................................
2025-05-12 16:13:28,607:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:28,607:INFO:Creating metrics dataframe
2025-05-12 16:13:28,610:INFO:Initializing Extra Trees Classifier
2025-05-12 16:13:28,610:INFO:Total runtime is 0.20901284615198773 minutes
2025-05-12 16:13:28,610:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:28,610:INFO:Initializing create_model()
2025-05-12 16:13:28,610:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:28,610:INFO:Checking exceptions
2025-05-12 16:13:28,610:INFO:Importing libraries
2025-05-12 16:13:28,610:INFO:Copying training dataset
2025-05-12 16:13:28,610:INFO:Defining folds
2025-05-12 16:13:28,610:INFO:Declaring metric variables
2025-05-12 16:13:28,610:INFO:Importing untrained model
2025-05-12 16:13:28,610:INFO:Extra Trees Classifier Imported successfully
2025-05-12 16:13:28,610:INFO:Starting cross validation
2025-05-12 16:13:28,610:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:29,066:INFO:Calculating mean and std
2025-05-12 16:13:29,066:INFO:Creating metrics dataframe
2025-05-12 16:13:29,066:INFO:Uploading results into container
2025-05-12 16:13:29,066:INFO:Uploading model into container now
2025-05-12 16:13:29,066:INFO:_master_model_container: 12
2025-05-12 16:13:29,066:INFO:_display_container: 2
2025-05-12 16:13:29,066:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-05-12 16:13:29,066:INFO:create_model() successfully completed......................................
2025-05-12 16:13:29,143:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:29,143:INFO:Creating metrics dataframe
2025-05-12 16:13:29,143:INFO:Initializing Light Gradient Boosting Machine
2025-05-12 16:13:29,143:INFO:Total runtime is 0.21790551344553633 minutes
2025-05-12 16:13:29,143:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:29,143:INFO:Initializing create_model()
2025-05-12 16:13:29,143:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:29,143:INFO:Checking exceptions
2025-05-12 16:13:29,143:INFO:Importing libraries
2025-05-12 16:13:29,143:INFO:Copying training dataset
2025-05-12 16:13:29,159:INFO:Defining folds
2025-05-12 16:13:29,159:INFO:Declaring metric variables
2025-05-12 16:13:29,159:INFO:Importing untrained model
2025-05-12 16:13:29,159:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-12 16:13:29,159:INFO:Starting cross validation
2025-05-12 16:13:29,159:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:30,893:INFO:Calculating mean and std
2025-05-12 16:13:30,895:INFO:Creating metrics dataframe
2025-05-12 16:13:30,899:INFO:Uploading results into container
2025-05-12 16:13:30,901:INFO:Uploading model into container now
2025-05-12 16:13:30,901:INFO:_master_model_container: 13
2025-05-12 16:13:30,901:INFO:_display_container: 2
2025-05-12 16:13:30,903:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-12 16:13:30,903:INFO:create_model() successfully completed......................................
2025-05-12 16:13:30,994:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:30,994:INFO:Creating metrics dataframe
2025-05-12 16:13:30,994:INFO:Initializing Dummy Classifier
2025-05-12 16:13:30,994:INFO:Total runtime is 0.24875086148579917 minutes
2025-05-12 16:13:30,994:INFO:SubProcess create_model() called ==================================
2025-05-12 16:13:30,994:INFO:Initializing create_model()
2025-05-12 16:13:30,994:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x0000029A48333850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:30,994:INFO:Checking exceptions
2025-05-12 16:13:30,994:INFO:Importing libraries
2025-05-12 16:13:30,994:INFO:Copying training dataset
2025-05-12 16:13:30,994:INFO:Defining folds
2025-05-12 16:13:30,994:INFO:Declaring metric variables
2025-05-12 16:13:30,994:INFO:Importing untrained model
2025-05-12 16:13:30,994:INFO:Dummy Classifier Imported successfully
2025-05-12 16:13:30,994:INFO:Starting cross validation
2025-05-12 16:13:30,994:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:13:31,044:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,044:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,044:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,044:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,044:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,044:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,062:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,062:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,064:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,066:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:13:31,076:INFO:Calculating mean and std
2025-05-12 16:13:31,076:INFO:Creating metrics dataframe
2025-05-12 16:13:31,076:INFO:Uploading results into container
2025-05-12 16:13:31,076:INFO:Uploading model into container now
2025-05-12 16:13:31,076:INFO:_master_model_container: 14
2025-05-12 16:13:31,076:INFO:_display_container: 2
2025-05-12 16:13:31,076:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-05-12 16:13:31,076:INFO:create_model() successfully completed......................................
2025-05-12 16:13:31,161:INFO:SubProcess create_model() end ==================================
2025-05-12 16:13:31,161:INFO:Creating metrics dataframe
2025-05-12 16:13:31,161:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-05-12 16:13:31,161:INFO:Initializing create_model()
2025-05-12 16:13:31,161:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:31,161:INFO:Checking exceptions
2025-05-12 16:13:31,161:INFO:Importing libraries
2025-05-12 16:13:31,161:INFO:Copying training dataset
2025-05-12 16:13:31,176:INFO:Defining folds
2025-05-12 16:13:31,176:INFO:Declaring metric variables
2025-05-12 16:13:31,176:INFO:Importing untrained model
2025-05-12 16:13:31,176:INFO:Declaring custom model
2025-05-12 16:13:31,176:INFO:Logistic Regression Imported successfully
2025-05-12 16:13:31,176:INFO:Cross validation set to False
2025-05-12 16:13:31,176:INFO:Fitting Model
2025-05-12 16:13:31,192:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:13:31,192:INFO:create_model() successfully completed......................................
2025-05-12 16:13:31,276:INFO:_master_model_container: 14
2025-05-12 16:13:31,276:INFO:_display_container: 2
2025-05-12 16:13:31,277:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:13:31,277:INFO:compare_models() successfully completed......................................
2025-05-12 16:13:31,282:INFO:Initializing finalize_model()
2025-05-12 16:13:31,282:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-05-12 16:13:31,282:INFO:Finalizing LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:13:31,282:INFO:Initializing create_model()
2025-05-12 16:13:31,282:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000029A0E798700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:13:31,282:INFO:Checking exceptions
2025-05-12 16:13:31,282:INFO:Importing libraries
2025-05-12 16:13:31,282:INFO:Copying training dataset
2025-05-12 16:13:31,282:INFO:Defining folds
2025-05-12 16:13:31,282:INFO:Declaring metric variables
2025-05-12 16:13:31,282:INFO:Importing untrained model
2025-05-12 16:13:31,282:INFO:Declaring custom model
2025-05-12 16:13:31,282:INFO:Logistic Regression Imported successfully
2025-05-12 16:13:31,282:INFO:Cross validation set to False
2025-05-12 16:13:31,282:INFO:Fitting Model
2025-05-12 16:13:31,311:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-05-12 16:13:31,312:INFO:create_model() successfully completed......................................
2025-05-12 16:13:31,376:INFO:_master_model_container: 14
2025-05-12 16:13:31,376:INFO:_display_container: 2
2025-05-12 16:13:31,383:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-05-12 16:13:31,384:INFO:finalize_model() successfully completed......................................
2025-05-12 16:13:31,457:INFO:Initializing save_model()
2025-05-12 16:13:31,457:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False), model_name=model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\Thayse\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-12 16:13:31,457:INFO:Adding model into prep_pipe
2025-05-12 16:13:31,457:WARNING:Only Model saved as it was a pipeline.
2025-05-12 16:13:31,457:INFO:model.pkl saved in current working directory
2025-05-12 16:13:31,473:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-05-12 16:13:31,473:INFO:save_model() successfully completed......................................
2025-05-12 16:14:04,812:INFO:Soft dependency imported: prophet: 1.1.6
2025-05-12 16:14:05,958:INFO:Initializing load_model()
2025-05-12 16:14:05,958:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 16:14:08,821:INFO:Initializing load_model()
2025-05-12 16:14:08,821:INFO:load_model(model_name=aapl_best_model, platform=None, authentication=None, verbose=True)
2025-05-12 16:14:45,987:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:14:45,987:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:14:45,987:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:14:45,987:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:14:47,117:INFO:PyCaret ClassificationExperiment
2025-05-12 16:14:47,117:INFO:Logging name: clf-default-name
2025-05-12 16:14:47,117:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-05-12 16:14:47,117:INFO:version 3.3.2
2025-05-12 16:14:47,117:INFO:Initializing setup()
2025-05-12 16:14:47,117:INFO:self.USI: b8eb
2025-05-12 16:14:47,117:INFO:self._variable_keys: {'y_train', 'target_param', 'gpu_n_jobs_param', 'logging_param', 'X_test', 'seed', 'pipeline', 'log_plots_param', 'y_test', 'fold_shuffle_param', 'y', 'gpu_param', 'is_multiclass', 'fold_groups_param', 'USI', 'n_jobs_param', 'fold_generator', '_available_plots', 'fix_imbalance', 'data', 'exp_id', 'idx', '_ml_usecase', 'X', 'html_param', 'exp_name_log', 'X_train', 'memory'}
2025-05-12 16:14:47,117:INFO:Checking environment
2025-05-12 16:14:47,117:INFO:python_version: 3.10.16
2025-05-12 16:14:47,117:INFO:python_build: ('main', 'Dec 11 2024 16:19:12')
2025-05-12 16:14:47,117:INFO:machine: AMD64
2025-05-12 16:14:47,133:INFO:platform: Windows-10-10.0.26100-SP0
2025-05-12 16:14:47,133:INFO:Memory: svmem(total=17009004544, available=4553408512, percent=73.2, used=12455596032, free=4553408512)
2025-05-12 16:14:47,133:INFO:Physical Core: 6
2025-05-12 16:14:47,133:INFO:Logical Core: 12
2025-05-12 16:14:47,133:INFO:Checking libraries
2025-05-12 16:14:47,133:INFO:System:
2025-05-12 16:14:47,133:INFO:    python: 3.10.16 | packaged by Anaconda, Inc. | (main, Dec 11 2024, 16:19:12) [MSC v.1929 64 bit (AMD64)]
2025-05-12 16:14:47,133:INFO:executable: C:\Users\Thayse\.conda\envs\mlpipeline\python.exe
2025-05-12 16:14:47,133:INFO:   machine: Windows-10-10.0.26100-SP0
2025-05-12 16:14:47,133:INFO:PyCaret required dependencies:
2025-05-12 16:14:47,260:INFO:                 pip: 25.0
2025-05-12 16:14:47,260:INFO:          setuptools: 75.8.0
2025-05-12 16:14:47,260:INFO:             pycaret: 3.3.2
2025-05-12 16:14:47,260:INFO:             IPython: 8.34.0
2025-05-12 16:14:47,260:INFO:          ipywidgets: 8.1.5
2025-05-12 16:14:47,260:INFO:                tqdm: 4.67.1
2025-05-12 16:14:47,260:INFO:               numpy: 1.26.4
2025-05-12 16:14:47,260:INFO:              pandas: 2.1.4
2025-05-12 16:14:47,260:INFO:              jinja2: 3.1.6
2025-05-12 16:14:47,260:INFO:               scipy: 1.11.4
2025-05-12 16:14:47,260:INFO:              joblib: 1.3.2
2025-05-12 16:14:47,260:INFO:             sklearn: 1.4.2
2025-05-12 16:14:47,260:INFO:                pyod: 2.0.4
2025-05-12 16:14:47,260:INFO:            imblearn: 0.13.0
2025-05-12 16:14:47,260:INFO:   category_encoders: 2.7.0
2025-05-12 16:14:47,260:INFO:            lightgbm: 4.6.0
2025-05-12 16:14:47,260:INFO:               numba: 0.61.0
2025-05-12 16:14:47,260:INFO:            requests: 2.32.3
2025-05-12 16:14:47,260:INFO:          matplotlib: 3.7.5
2025-05-12 16:14:47,260:INFO:          scikitplot: 0.3.7
2025-05-12 16:14:47,260:INFO:         yellowbrick: 1.5
2025-05-12 16:14:47,260:INFO:              plotly: 5.24.1
2025-05-12 16:14:47,260:INFO:    plotly-resampler: Not installed
2025-05-12 16:14:47,260:INFO:             kaleido: 0.2.1
2025-05-12 16:14:47,260:INFO:           schemdraw: 0.15
2025-05-12 16:14:47,260:INFO:         statsmodels: 0.14.4
2025-05-12 16:14:47,260:INFO:              sktime: 0.26.0
2025-05-12 16:14:47,260:INFO:               tbats: 1.1.3
2025-05-12 16:14:47,260:INFO:            pmdarima: 2.0.4
2025-05-12 16:14:47,260:INFO:              psutil: 7.0.0
2025-05-12 16:14:47,260:INFO:          markupsafe: 3.0.2
2025-05-12 16:14:47,260:INFO:             pickle5: Not installed
2025-05-12 16:14:47,260:INFO:         cloudpickle: 3.1.1
2025-05-12 16:14:47,260:INFO:         deprecation: 2.1.0
2025-05-12 16:14:47,260:INFO:              xxhash: 3.5.0
2025-05-12 16:14:47,260:INFO:           wurlitzer: Not installed
2025-05-12 16:14:47,260:INFO:PyCaret optional dependencies:
2025-05-12 16:14:47,649:INFO:                shap: Not installed
2025-05-12 16:14:47,649:INFO:           interpret: Not installed
2025-05-12 16:14:47,649:INFO:                umap: Not installed
2025-05-12 16:14:47,649:INFO:     ydata_profiling: 4.16.1
2025-05-12 16:14:47,649:INFO:  explainerdashboard: Not installed
2025-05-12 16:14:47,649:INFO:             autoviz: Not installed
2025-05-12 16:14:47,649:INFO:           fairlearn: Not installed
2025-05-12 16:14:47,649:INFO:          deepchecks: Not installed
2025-05-12 16:14:47,649:INFO:             xgboost: Not installed
2025-05-12 16:14:47,649:INFO:            catboost: Not installed
2025-05-12 16:14:47,649:INFO:              kmodes: Not installed
2025-05-12 16:14:47,649:INFO:             mlxtend: Not installed
2025-05-12 16:14:47,649:INFO:       statsforecast: Not installed
2025-05-12 16:14:47,649:INFO:        tune_sklearn: Not installed
2025-05-12 16:14:47,649:INFO:                 ray: Not installed
2025-05-12 16:14:47,649:INFO:            hyperopt: Not installed
2025-05-12 16:14:47,649:INFO:              optuna: 4.2.1
2025-05-12 16:14:47,649:INFO:               skopt: Not installed
2025-05-12 16:14:47,649:INFO:              mlflow: Not installed
2025-05-12 16:14:47,649:INFO:              gradio: Not installed
2025-05-12 16:14:47,649:INFO:             fastapi: 0.115.12
2025-05-12 16:14:47,649:INFO:             uvicorn: 0.34.2
2025-05-12 16:14:47,649:INFO:              m2cgen: Not installed
2025-05-12 16:14:47,649:INFO:           evidently: Not installed
2025-05-12 16:14:47,649:INFO:               fugue: Not installed
2025-05-12 16:14:47,649:INFO:           streamlit: 1.44.1
2025-05-12 16:14:47,649:INFO:             prophet: 1.1.6
2025-05-12 16:14:47,649:INFO:None
2025-05-12 16:14:47,649:INFO:Set up data.
2025-05-12 16:14:47,649:INFO:Set up folding strategy.
2025-05-12 16:14:47,649:INFO:Set up train/test split.
2025-05-12 16:14:47,649:INFO:Set up index.
2025-05-12 16:14:47,649:INFO:Assigning column types.
2025-05-12 16:14:47,649:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-05-12 16:14:47,697:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-12 16:14:47,697:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:14:47,728:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,728:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,760:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-05-12 16:14:47,760:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:14:47,792:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,792:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,792:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-05-12 16:14:47,826:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:14:47,859:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,859:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,899:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-05-12 16:14:47,921:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,921:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,921:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-05-12 16:14:47,994:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:47,994:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:48,055:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:48,055:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:48,055:INFO:Preparing preprocessing pipeline...
2025-05-12 16:14:48,055:INFO:Set up simple imputation.
2025-05-12 16:14:48,055:INFO:Set up column name cleaning.
2025-05-12 16:14:48,087:INFO:Finished creating preprocessing pipeline.
2025-05-12 16:14:48,087:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\Thayse\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-05-12 16:14:48,087:INFO:Creating final display dataframe.
2025-05-12 16:14:48,161:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (150, 5)
4        Transformed data shape          (150, 5)
5   Transformed train set shape          (105, 5)
6    Transformed test set shape           (45, 5)
7              Numeric features                 4
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              b8eb
2025-05-12 16:14:48,219:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:48,219:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:48,290:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:48,290:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-05-12 16:14:48,290:INFO:setup() successfully completed in 1.17s...............
2025-05-12 16:14:48,290:INFO:Initializing compare_models()
2025-05-12 16:14:48,290:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2025-05-12 16:14:48,290:INFO:Checking exceptions
2025-05-12 16:14:48,290:INFO:Preparing display monitor
2025-05-12 16:14:48,300:INFO:Initializing Logistic Regression
2025-05-12 16:14:48,300:INFO:Total runtime is 0.0 minutes
2025-05-12 16:14:48,300:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:48,300:INFO:Initializing create_model()
2025-05-12 16:14:48,300:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:48,300:INFO:Checking exceptions
2025-05-12 16:14:48,300:INFO:Importing libraries
2025-05-12 16:14:48,300:INFO:Copying training dataset
2025-05-12 16:14:48,303:INFO:Defining folds
2025-05-12 16:14:48,303:INFO:Declaring metric variables
2025-05-12 16:14:48,303:INFO:Importing untrained model
2025-05-12 16:14:48,303:INFO:Logistic Regression Imported successfully
2025-05-12 16:14:48,303:INFO:Starting cross validation
2025-05-12 16:14:48,303:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:55,110:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,122:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,126:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,130:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,142:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,155:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,157:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,157:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,167:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,192:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:55,222:INFO:Calculating mean and std
2025-05-12 16:14:55,226:INFO:Creating metrics dataframe
2025-05-12 16:14:55,232:INFO:Uploading results into container
2025-05-12 16:14:55,234:INFO:Uploading model into container now
2025-05-12 16:14:55,234:INFO:_master_model_container: 1
2025-05-12 16:14:55,236:INFO:_display_container: 2
2025-05-12 16:14:55,236:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:14:55,236:INFO:create_model() successfully completed......................................
2025-05-12 16:14:55,390:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:55,390:INFO:Creating metrics dataframe
2025-05-12 16:14:55,390:INFO:Initializing K Neighbors Classifier
2025-05-12 16:14:55,390:INFO:Total runtime is 0.11817177136739095 minutes
2025-05-12 16:14:55,390:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:55,390:INFO:Initializing create_model()
2025-05-12 16:14:55,390:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:55,390:INFO:Checking exceptions
2025-05-12 16:14:55,390:INFO:Importing libraries
2025-05-12 16:14:55,390:INFO:Copying training dataset
2025-05-12 16:14:55,406:INFO:Defining folds
2025-05-12 16:14:55,406:INFO:Declaring metric variables
2025-05-12 16:14:55,406:INFO:Importing untrained model
2025-05-12 16:14:55,406:INFO:K Neighbors Classifier Imported successfully
2025-05-12 16:14:55,406:INFO:Starting cross validation
2025-05-12 16:14:55,406:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:58,206:INFO:Calculating mean and std
2025-05-12 16:14:58,206:INFO:Creating metrics dataframe
2025-05-12 16:14:58,206:INFO:Uploading results into container
2025-05-12 16:14:58,206:INFO:Uploading model into container now
2025-05-12 16:14:58,206:INFO:_master_model_container: 2
2025-05-12 16:14:58,206:INFO:_display_container: 2
2025-05-12 16:14:58,206:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-05-12 16:14:58,206:INFO:create_model() successfully completed......................................
2025-05-12 16:14:58,292:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:58,292:INFO:Creating metrics dataframe
2025-05-12 16:14:58,292:INFO:Initializing Naive Bayes
2025-05-12 16:14:58,292:INFO:Total runtime is 0.16654721895853677 minutes
2025-05-12 16:14:58,292:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:58,292:INFO:Initializing create_model()
2025-05-12 16:14:58,292:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:58,292:INFO:Checking exceptions
2025-05-12 16:14:58,292:INFO:Importing libraries
2025-05-12 16:14:58,292:INFO:Copying training dataset
2025-05-12 16:14:58,292:INFO:Defining folds
2025-05-12 16:14:58,292:INFO:Declaring metric variables
2025-05-12 16:14:58,292:INFO:Importing untrained model
2025-05-12 16:14:58,292:INFO:Naive Bayes Imported successfully
2025-05-12 16:14:58,292:INFO:Starting cross validation
2025-05-12 16:14:58,292:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:58,407:INFO:Calculating mean and std
2025-05-12 16:14:58,407:INFO:Creating metrics dataframe
2025-05-12 16:14:58,407:INFO:Uploading results into container
2025-05-12 16:14:58,407:INFO:Uploading model into container now
2025-05-12 16:14:58,407:INFO:_master_model_container: 3
2025-05-12 16:14:58,407:INFO:_display_container: 2
2025-05-12 16:14:58,407:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-05-12 16:14:58,423:INFO:create_model() successfully completed......................................
2025-05-12 16:14:58,493:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:58,493:INFO:Creating metrics dataframe
2025-05-12 16:14:58,493:INFO:Initializing Decision Tree Classifier
2025-05-12 16:14:58,493:INFO:Total runtime is 0.1698813199996948 minutes
2025-05-12 16:14:58,493:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:58,493:INFO:Initializing create_model()
2025-05-12 16:14:58,493:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:58,493:INFO:Checking exceptions
2025-05-12 16:14:58,493:INFO:Importing libraries
2025-05-12 16:14:58,493:INFO:Copying training dataset
2025-05-12 16:14:58,493:INFO:Defining folds
2025-05-12 16:14:58,493:INFO:Declaring metric variables
2025-05-12 16:14:58,493:INFO:Importing untrained model
2025-05-12 16:14:58,493:INFO:Decision Tree Classifier Imported successfully
2025-05-12 16:14:58,493:INFO:Starting cross validation
2025-05-12 16:14:58,493:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:58,590:INFO:Calculating mean and std
2025-05-12 16:14:58,590:INFO:Creating metrics dataframe
2025-05-12 16:14:58,590:INFO:Uploading results into container
2025-05-12 16:14:58,590:INFO:Uploading model into container now
2025-05-12 16:14:58,590:INFO:_master_model_container: 4
2025-05-12 16:14:58,590:INFO:_display_container: 2
2025-05-12 16:14:58,590:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-05-12 16:14:58,590:INFO:create_model() successfully completed......................................
2025-05-12 16:14:58,667:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:58,667:INFO:Creating metrics dataframe
2025-05-12 16:14:58,671:INFO:Initializing SVM - Linear Kernel
2025-05-12 16:14:58,671:INFO:Total runtime is 0.17285429636637367 minutes
2025-05-12 16:14:58,671:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:58,671:INFO:Initializing create_model()
2025-05-12 16:14:58,671:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:58,671:INFO:Checking exceptions
2025-05-12 16:14:58,671:INFO:Importing libraries
2025-05-12 16:14:58,671:INFO:Copying training dataset
2025-05-12 16:14:58,675:INFO:Defining folds
2025-05-12 16:14:58,675:INFO:Declaring metric variables
2025-05-12 16:14:58,675:INFO:Importing untrained model
2025-05-12 16:14:58,676:INFO:SVM - Linear Kernel Imported successfully
2025-05-12 16:14:58,676:INFO:Starting cross validation
2025-05-12 16:14:58,676:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:58,725:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,725:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,739:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,745:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,745:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:14:58,745:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:14:58,757:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,759:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,759:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,759:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,761:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,767:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:14:58,767:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:14:58,767:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:14:58,771:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:14:58,787:INFO:Calculating mean and std
2025-05-12 16:14:58,787:INFO:Creating metrics dataframe
2025-05-12 16:14:58,787:INFO:Uploading results into container
2025-05-12 16:14:58,787:INFO:Uploading model into container now
2025-05-12 16:14:58,787:INFO:_master_model_container: 5
2025-05-12 16:14:58,787:INFO:_display_container: 2
2025-05-12 16:14:58,787:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-05-12 16:14:58,787:INFO:create_model() successfully completed......................................
2025-05-12 16:14:58,862:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:58,862:INFO:Creating metrics dataframe
2025-05-12 16:14:58,865:INFO:Initializing Ridge Classifier
2025-05-12 16:14:58,865:INFO:Total runtime is 0.1760895729064941 minutes
2025-05-12 16:14:58,865:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:58,865:INFO:Initializing create_model()
2025-05-12 16:14:58,865:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:58,866:INFO:Checking exceptions
2025-05-12 16:14:58,866:INFO:Importing libraries
2025-05-12 16:14:58,866:INFO:Copying training dataset
2025-05-12 16:14:58,868:INFO:Defining folds
2025-05-12 16:14:58,869:INFO:Declaring metric variables
2025-05-12 16:14:58,869:INFO:Importing untrained model
2025-05-12 16:14:58,869:INFO:Ridge Classifier Imported successfully
2025-05-12 16:14:58,869:INFO:Starting cross validation
2025-05-12 16:14:58,870:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:58,911:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,911:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,913:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,915:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,917:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,921:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,921:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,923:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,925:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,925:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:58,955:INFO:Calculating mean and std
2025-05-12 16:14:58,955:INFO:Creating metrics dataframe
2025-05-12 16:14:58,955:INFO:Uploading results into container
2025-05-12 16:14:58,955:INFO:Uploading model into container now
2025-05-12 16:14:58,955:INFO:_master_model_container: 6
2025-05-12 16:14:58,955:INFO:_display_container: 2
2025-05-12 16:14:58,955:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-05-12 16:14:58,955:INFO:create_model() successfully completed......................................
2025-05-12 16:14:59,025:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:59,025:INFO:Creating metrics dataframe
2025-05-12 16:14:59,025:INFO:Initializing Random Forest Classifier
2025-05-12 16:14:59,025:INFO:Total runtime is 0.17875965436299637 minutes
2025-05-12 16:14:59,025:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:59,025:INFO:Initializing create_model()
2025-05-12 16:14:59,025:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:59,025:INFO:Checking exceptions
2025-05-12 16:14:59,025:INFO:Importing libraries
2025-05-12 16:14:59,025:INFO:Copying training dataset
2025-05-12 16:14:59,025:INFO:Defining folds
2025-05-12 16:14:59,025:INFO:Declaring metric variables
2025-05-12 16:14:59,025:INFO:Importing untrained model
2025-05-12 16:14:59,025:INFO:Random Forest Classifier Imported successfully
2025-05-12 16:14:59,041:INFO:Starting cross validation
2025-05-12 16:14:59,041:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:59,541:INFO:Calculating mean and std
2025-05-12 16:14:59,541:INFO:Creating metrics dataframe
2025-05-12 16:14:59,545:INFO:Uploading results into container
2025-05-12 16:14:59,545:INFO:Uploading model into container now
2025-05-12 16:14:59,545:INFO:_master_model_container: 7
2025-05-12 16:14:59,545:INFO:_display_container: 2
2025-05-12 16:14:59,553:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-05-12 16:14:59,553:INFO:create_model() successfully completed......................................
2025-05-12 16:14:59,638:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:59,638:INFO:Creating metrics dataframe
2025-05-12 16:14:59,642:INFO:Initializing Quadratic Discriminant Analysis
2025-05-12 16:14:59,642:INFO:Total runtime is 0.18903869390487665 minutes
2025-05-12 16:14:59,642:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:59,642:INFO:Initializing create_model()
2025-05-12 16:14:59,642:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:59,642:INFO:Checking exceptions
2025-05-12 16:14:59,643:INFO:Importing libraries
2025-05-12 16:14:59,643:INFO:Copying training dataset
2025-05-12 16:14:59,643:INFO:Defining folds
2025-05-12 16:14:59,643:INFO:Declaring metric variables
2025-05-12 16:14:59,643:INFO:Importing untrained model
2025-05-12 16:14:59,643:INFO:Quadratic Discriminant Analysis Imported successfully
2025-05-12 16:14:59,643:INFO:Starting cross validation
2025-05-12 16:14:59,643:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:59,688:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,693:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,697:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,697:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,698:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,702:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,702:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,702:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,704:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,707:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,724:INFO:Calculating mean and std
2025-05-12 16:14:59,725:INFO:Creating metrics dataframe
2025-05-12 16:14:59,726:INFO:Uploading results into container
2025-05-12 16:14:59,726:INFO:Uploading model into container now
2025-05-12 16:14:59,726:INFO:_master_model_container: 8
2025-05-12 16:14:59,726:INFO:_display_container: 2
2025-05-12 16:14:59,726:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-05-12 16:14:59,726:INFO:create_model() successfully completed......................................
2025-05-12 16:14:59,790:INFO:SubProcess create_model() end ==================================
2025-05-12 16:14:59,790:INFO:Creating metrics dataframe
2025-05-12 16:14:59,790:INFO:Initializing Ada Boost Classifier
2025-05-12 16:14:59,790:INFO:Total runtime is 0.19150489171346022 minutes
2025-05-12 16:14:59,790:INFO:SubProcess create_model() called ==================================
2025-05-12 16:14:59,790:INFO:Initializing create_model()
2025-05-12 16:14:59,790:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:14:59,790:INFO:Checking exceptions
2025-05-12 16:14:59,790:INFO:Importing libraries
2025-05-12 16:14:59,790:INFO:Copying training dataset
2025-05-12 16:14:59,790:INFO:Defining folds
2025-05-12 16:14:59,790:INFO:Declaring metric variables
2025-05-12 16:14:59,790:INFO:Importing untrained model
2025-05-12 16:14:59,790:INFO:Ada Boost Classifier Imported successfully
2025-05-12 16:14:59,790:INFO:Starting cross validation
2025-05-12 16:14:59,790:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:14:59,827:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,829:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,831:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,831:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,833:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,837:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,837:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,837:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,839:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,842:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-05-12 16:14:59,972:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,976:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:14:59,996:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,002:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,002:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,009:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,013:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,013:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,013:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,025:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,040:INFO:Calculating mean and std
2025-05-12 16:15:00,040:INFO:Creating metrics dataframe
2025-05-12 16:15:00,040:INFO:Uploading results into container
2025-05-12 16:15:00,040:INFO:Uploading model into container now
2025-05-12 16:15:00,040:INFO:_master_model_container: 9
2025-05-12 16:15:00,040:INFO:_display_container: 2
2025-05-12 16:15:00,040:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-05-12 16:15:00,040:INFO:create_model() successfully completed......................................
2025-05-12 16:15:00,107:INFO:SubProcess create_model() end ==================================
2025-05-12 16:15:00,107:INFO:Creating metrics dataframe
2025-05-12 16:15:00,110:INFO:Initializing Gradient Boosting Classifier
2025-05-12 16:15:00,110:INFO:Total runtime is 0.19683195749918614 minutes
2025-05-12 16:15:00,110:INFO:SubProcess create_model() called ==================================
2025-05-12 16:15:00,110:INFO:Initializing create_model()
2025-05-12 16:15:00,110:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:15:00,110:INFO:Checking exceptions
2025-05-12 16:15:00,110:INFO:Importing libraries
2025-05-12 16:15:00,110:INFO:Copying training dataset
2025-05-12 16:15:00,110:INFO:Defining folds
2025-05-12 16:15:00,110:INFO:Declaring metric variables
2025-05-12 16:15:00,110:INFO:Importing untrained model
2025-05-12 16:15:00,110:INFO:Gradient Boosting Classifier Imported successfully
2025-05-12 16:15:00,110:INFO:Starting cross validation
2025-05-12 16:15:00,110:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:15:00,583:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,616:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,638:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,642:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,642:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,652:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,668:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,668:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,684:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,684:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,716:INFO:Calculating mean and std
2025-05-12 16:15:00,716:INFO:Creating metrics dataframe
2025-05-12 16:15:00,716:INFO:Uploading results into container
2025-05-12 16:15:00,716:INFO:Uploading model into container now
2025-05-12 16:15:00,716:INFO:_master_model_container: 10
2025-05-12 16:15:00,716:INFO:_display_container: 2
2025-05-12 16:15:00,716:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-05-12 16:15:00,716:INFO:create_model() successfully completed......................................
2025-05-12 16:15:00,809:INFO:SubProcess create_model() end ==================================
2025-05-12 16:15:00,810:INFO:Creating metrics dataframe
2025-05-12 16:15:00,810:INFO:Initializing Linear Discriminant Analysis
2025-05-12 16:15:00,810:INFO:Total runtime is 0.20850765307744337 minutes
2025-05-12 16:15:00,810:INFO:SubProcess create_model() called ==================================
2025-05-12 16:15:00,810:INFO:Initializing create_model()
2025-05-12 16:15:00,810:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:15:00,810:INFO:Checking exceptions
2025-05-12 16:15:00,810:INFO:Importing libraries
2025-05-12 16:15:00,810:INFO:Copying training dataset
2025-05-12 16:15:00,810:INFO:Defining folds
2025-05-12 16:15:00,810:INFO:Declaring metric variables
2025-05-12 16:15:00,810:INFO:Importing untrained model
2025-05-12 16:15:00,810:INFO:Linear Discriminant Analysis Imported successfully
2025-05-12 16:15:00,810:INFO:Starting cross validation
2025-05-12 16:15:00,810:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:15:00,861:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,863:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,863:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,866:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,867:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,870:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,871:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,873:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,876:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,878:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-05-12 16:15:00,907:INFO:Calculating mean and std
2025-05-12 16:15:00,911:INFO:Creating metrics dataframe
2025-05-12 16:15:00,911:INFO:Uploading results into container
2025-05-12 16:15:00,911:INFO:Uploading model into container now
2025-05-12 16:15:00,911:INFO:_master_model_container: 11
2025-05-12 16:15:00,911:INFO:_display_container: 2
2025-05-12 16:15:00,911:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-05-12 16:15:00,911:INFO:create_model() successfully completed......................................
2025-05-12 16:15:00,998:INFO:SubProcess create_model() end ==================================
2025-05-12 16:15:00,998:INFO:Creating metrics dataframe
2025-05-12 16:15:01,001:INFO:Initializing Extra Trees Classifier
2025-05-12 16:15:01,001:INFO:Total runtime is 0.21168994108835848 minutes
2025-05-12 16:15:01,001:INFO:SubProcess create_model() called ==================================
2025-05-12 16:15:01,002:INFO:Initializing create_model()
2025-05-12 16:15:01,002:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:15:01,002:INFO:Checking exceptions
2025-05-12 16:15:01,002:INFO:Importing libraries
2025-05-12 16:15:01,002:INFO:Copying training dataset
2025-05-12 16:15:01,005:INFO:Defining folds
2025-05-12 16:15:01,005:INFO:Declaring metric variables
2025-05-12 16:15:01,006:INFO:Importing untrained model
2025-05-12 16:15:01,006:INFO:Extra Trees Classifier Imported successfully
2025-05-12 16:15:01,006:INFO:Starting cross validation
2025-05-12 16:15:01,007:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:15:01,439:INFO:Calculating mean and std
2025-05-12 16:15:01,439:INFO:Creating metrics dataframe
2025-05-12 16:15:01,439:INFO:Uploading results into container
2025-05-12 16:15:01,439:INFO:Uploading model into container now
2025-05-12 16:15:01,439:INFO:_master_model_container: 12
2025-05-12 16:15:01,439:INFO:_display_container: 2
2025-05-12 16:15:01,439:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-05-12 16:15:01,439:INFO:create_model() successfully completed......................................
2025-05-12 16:15:01,505:INFO:SubProcess create_model() end ==================================
2025-05-12 16:15:01,505:INFO:Creating metrics dataframe
2025-05-12 16:15:01,509:INFO:Initializing Light Gradient Boosting Machine
2025-05-12 16:15:01,509:INFO:Total runtime is 0.2201591253280639 minutes
2025-05-12 16:15:01,509:INFO:SubProcess create_model() called ==================================
2025-05-12 16:15:01,510:INFO:Initializing create_model()
2025-05-12 16:15:01,510:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:15:01,510:INFO:Checking exceptions
2025-05-12 16:15:01,510:INFO:Importing libraries
2025-05-12 16:15:01,510:INFO:Copying training dataset
2025-05-12 16:15:01,510:INFO:Defining folds
2025-05-12 16:15:01,510:INFO:Declaring metric variables
2025-05-12 16:15:01,510:INFO:Importing untrained model
2025-05-12 16:15:01,510:INFO:Light Gradient Boosting Machine Imported successfully
2025-05-12 16:15:01,510:INFO:Starting cross validation
2025-05-12 16:15:01,510:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:15:03,228:INFO:Calculating mean and std
2025-05-12 16:15:03,230:INFO:Creating metrics dataframe
2025-05-12 16:15:03,234:INFO:Uploading results into container
2025-05-12 16:15:03,234:INFO:Uploading model into container now
2025-05-12 16:15:03,236:INFO:_master_model_container: 13
2025-05-12 16:15:03,236:INFO:_display_container: 2
2025-05-12 16:15:03,238:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-05-12 16:15:03,238:INFO:create_model() successfully completed......................................
2025-05-12 16:15:03,327:INFO:SubProcess create_model() end ==================================
2025-05-12 16:15:03,328:INFO:Creating metrics dataframe
2025-05-12 16:15:03,329:INFO:Initializing Dummy Classifier
2025-05-12 16:15:03,329:INFO:Total runtime is 0.2504895567893981 minutes
2025-05-12 16:15:03,329:INFO:SubProcess create_model() called ==================================
2025-05-12 16:15:03,329:INFO:Initializing create_model()
2025-05-12 16:15:03,329:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x00000199A4D73850>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:15:03,329:INFO:Checking exceptions
2025-05-12 16:15:03,329:INFO:Importing libraries
2025-05-12 16:15:03,329:INFO:Copying training dataset
2025-05-12 16:15:03,329:INFO:Defining folds
2025-05-12 16:15:03,329:INFO:Declaring metric variables
2025-05-12 16:15:03,329:INFO:Importing untrained model
2025-05-12 16:15:03,329:INFO:Dummy Classifier Imported successfully
2025-05-12 16:15:03,329:INFO:Starting cross validation
2025-05-12 16:15:03,329:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-05-12 16:15:03,386:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,388:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,388:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,390:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,390:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,392:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,395:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,395:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,395:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,395:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-05-12 16:15:03,410:INFO:Calculating mean and std
2025-05-12 16:15:03,410:INFO:Creating metrics dataframe
2025-05-12 16:15:03,412:INFO:Uploading results into container
2025-05-12 16:15:03,412:INFO:Uploading model into container now
2025-05-12 16:15:03,413:INFO:_master_model_container: 14
2025-05-12 16:15:03,413:INFO:_display_container: 2
2025-05-12 16:15:03,413:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-05-12 16:15:03,413:INFO:create_model() successfully completed......................................
2025-05-12 16:15:03,476:INFO:SubProcess create_model() end ==================================
2025-05-12 16:15:03,476:INFO:Creating metrics dataframe
2025-05-12 16:15:03,476:WARNING:C:\Users\Thayse\.conda\envs\mlpipeline\lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2025-05-12 16:15:03,482:INFO:Initializing create_model()
2025-05-12 16:15:03,482:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:15:03,482:INFO:Checking exceptions
2025-05-12 16:15:03,484:INFO:Importing libraries
2025-05-12 16:15:03,484:INFO:Copying training dataset
2025-05-12 16:15:03,486:INFO:Defining folds
2025-05-12 16:15:03,486:INFO:Declaring metric variables
2025-05-12 16:15:03,488:INFO:Importing untrained model
2025-05-12 16:15:03,488:INFO:Declaring custom model
2025-05-12 16:15:03,488:INFO:Logistic Regression Imported successfully
2025-05-12 16:15:03,488:INFO:Cross validation set to False
2025-05-12 16:15:03,488:INFO:Fitting Model
2025-05-12 16:15:03,497:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:15:03,497:INFO:create_model() successfully completed......................................
2025-05-12 16:15:03,587:INFO:_master_model_container: 14
2025-05-12 16:15:03,587:INFO:_display_container: 2
2025-05-12 16:15:03,587:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:15:03,587:INFO:compare_models() successfully completed......................................
2025-05-12 16:15:03,595:INFO:Initializing finalize_model()
2025-05-12 16:15:03,595:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-05-12 16:15:03,595:INFO:Finalizing LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-05-12 16:15:03,595:INFO:Initializing create_model()
2025-05-12 16:15:03,595:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x00000199EB228700>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-05-12 16:15:03,595:INFO:Checking exceptions
2025-05-12 16:15:03,595:INFO:Importing libraries
2025-05-12 16:15:03,595:INFO:Copying training dataset
2025-05-12 16:15:03,595:INFO:Defining folds
2025-05-12 16:15:03,595:INFO:Declaring metric variables
2025-05-12 16:15:03,595:INFO:Importing untrained model
2025-05-12 16:15:03,595:INFO:Declaring custom model
2025-05-12 16:15:03,595:INFO:Logistic Regression Imported successfully
2025-05-12 16:15:03,595:INFO:Cross validation set to False
2025-05-12 16:15:03,595:INFO:Fitting Model
2025-05-12 16:15:03,617:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-05-12 16:15:03,617:INFO:create_model() successfully completed......................................
2025-05-12 16:15:03,687:INFO:_master_model_container: 14
2025-05-12 16:15:03,687:INFO:_display_container: 2
2025-05-12 16:15:03,694:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-05-12 16:15:03,694:INFO:finalize_model() successfully completed......................................
2025-05-12 16:15:03,758:INFO:Initializing save_model()
2025-05-12 16:15:03,758:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False), model_name=model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\Thayse\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-05-12 16:15:03,758:INFO:Adding model into prep_pipe
2025-05-12 16:15:03,758:WARNING:Only Model saved as it was a pipeline.
2025-05-12 16:15:03,773:INFO:model.pkl saved in current working directory
2025-05-12 16:15:03,773:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-05-12 16:15:03,773:INFO:save_model() successfully completed......................................
2025-05-12 16:15:31,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:15:31,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:15:31,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:15:31,779:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-05-12 16:15:32,989:INFO:Initializing load_model()
2025-05-12 16:15:32,989:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:15:41,345:INFO:Initializing load_model()
2025-05-12 16:15:41,346:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:08,851:INFO:Initializing load_model()
2025-05-12 16:16:08,851:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:13,098:INFO:Initializing load_model()
2025-05-12 16:16:13,098:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:14,596:INFO:Initializing load_model()
2025-05-12 16:16:14,597:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:14,991:INFO:Initializing load_model()
2025-05-12 16:16:14,991:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:16,206:INFO:Initializing load_model()
2025-05-12 16:16:16,207:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:17,063:INFO:Initializing load_model()
2025-05-12 16:16:17,063:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:17,765:INFO:Initializing load_model()
2025-05-12 16:16:17,766:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:19,167:INFO:Initializing load_model()
2025-05-12 16:16:19,167:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:19,953:INFO:Initializing load_model()
2025-05-12 16:16:19,954:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:21,381:INFO:Initializing load_model()
2025-05-12 16:16:21,381:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-05-12 16:16:21,412:INFO:Initializing predict_model()
2025-05-12 16:16:21,412:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001F005137A30>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001F0057C3EB0>)
2025-05-12 16:16:21,412:INFO:Checking exceptions
2025-05-12 16:16:21,412:INFO:Preloading libraries
2025-05-12 16:16:21,413:INFO:Set up data.
2025-05-12 16:16:21,416:INFO:Set up index.
2025-06-02 01:52:53,095:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:52:53,096:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:52:53,096:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:52:53,096:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:53:41,367:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:53:41,367:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:53:41,367:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:53:41,367:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:53:51,697:INFO:Initializing load_model()
2025-06-02 01:53:51,698:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 01:53:52,102:WARNING:C:\Users\kawav\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 01:54:34,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:54:34,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:54:34,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:54:34,225:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:54:38,688:INFO:Initializing load_model()
2025-06-02 01:54:38,688:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 01:54:38,756:WARNING:C:\Users\kawav\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 01:58:07,224:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:58:07,224:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:58:07,224:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:58:07,224:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 01:58:08,800:INFO:Initializing load_model()
2025-06-02 01:58:08,800:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 01:58:08,838:WARNING:C:\Users\kawav\AppData\Local\Packages\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\LocalCache\local-packages\Python311\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:32:20,333:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 20:32:20,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 20:32:20,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 20:32:20,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-02 20:32:49,160:INFO:Initializing load_model()
2025-06-02 20:32:49,160:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:32:50,307:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:33:28,529:INFO:Initializing load_model()
2025-06-02 20:33:28,529:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:33:28,539:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:33:29,496:INFO:Initializing load_model()
2025-06-02 20:33:29,496:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:33:29,496:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:33:30,262:INFO:Initializing load_model()
2025-06-02 20:33:30,263:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:33:30,270:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:33:31,249:INFO:Initializing load_model()
2025-06-02 20:33:31,249:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:33:31,258:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:33:32,167:INFO:Initializing load_model()
2025-06-02 20:33:32,167:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:33:32,172:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:33:33,115:INFO:Initializing load_model()
2025-06-02 20:33:33,115:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:33:33,122:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:34:04,652:INFO:Initializing load_model()
2025-06-02 20:34:04,652:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:34:04,666:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:34:06,503:INFO:Initializing load_model()
2025-06-02 20:34:06,503:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-02 20:34:06,515:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-02 20:34:06,631:INFO:Initializing predict_model()
2025-06-02 20:34:06,631:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001B2203FA410>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001B21EFF7740>)
2025-06-02 20:34:06,632:INFO:Checking exceptions
2025-06-02 20:34:06,632:INFO:Preloading libraries
2025-06-02 20:34:06,633:INFO:Set up data.
2025-06-02 20:34:06,640:INFO:Set up index.
2025-06-09 19:56:13,155:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 19:56:13,156:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 19:56:13,156:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 19:56:13,157:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 19:56:14,693:INFO:Initializing load_model()
2025-06-09 19:56:14,693:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 19:56:14,752:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-09 19:56:20,360:INFO:Initializing load_model()
2025-06-09 19:56:20,361:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 19:56:20,367:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-09 19:57:26,011:INFO:Initializing load_model()
2025-06-09 19:57:26,012:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 19:57:26,017:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-09 19:57:30,746:INFO:Initializing load_model()
2025-06-09 19:57:30,755:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 19:57:30,759:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-09 19:57:34,550:INFO:Initializing load_model()
2025-06-09 19:57:34,550:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 19:57:34,550:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pipeline.py:148: UserWarning: Version mismatch:
current: {'deps_info': {'pip': '24.0', 'setuptools': '65.5.0', 'pycaret': '3.3.2', 'IPython': '9.3.0', 'ipywidgets': '8.1.7', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.5', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.2', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.11.9', 'machine': 'AMD64'}}
pickle: {'deps_info': {'pip': '25.0', 'setuptools': '75.8.0', 'pycaret': '3.3.2', 'IPython': '8.34.0', 'ipywidgets': '8.1.5', 'tqdm': '4.67.1', 'numpy': '1.26.4', 'pandas': '2.1.4', 'jinja2': '3.1.6', 'scipy': '1.11.4', 'joblib': '1.3.2', 'sklearn': '1.4.2', 'pyod': '2.0.4', 'imblearn': '0.13.0', 'category_encoders': '2.7.0', 'lightgbm': '4.6.0', 'numba': '0.61.0', 'requests': '2.32.3', 'matplotlib': '3.7.5', 'scikitplot': '0.3.7', 'yellowbrick': '1.5', 'plotly': '5.24.1', 'plotly-resampler': 'Not installed', 'kaleido': '0.2.1', 'schemdraw': '0.15', 'statsmodels': '0.14.4', 'sktime': '0.26.0', 'tbats': '1.1.3', 'pmdarima': '2.0.4', 'psutil': '7.0.0', 'markupsafe': '3.0.2', 'pickle5': 'Not installed', 'cloudpickle': '3.1.1', 'deprecation': '2.1.0', 'xxhash': '3.5.0', 'wurlitzer': 'Not installed'}, 'python': {'version': '3.10.16', 'machine': 'AMD64'}}
  warnings.warn(

2025-06-09 19:57:34,671:INFO:Initializing predict_model()
2025-06-09 19:57:34,671:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x0000027DC16C8C10>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x0000027DC1BCC360>)
2025-06-09 19:57:34,671:INFO:Checking exceptions
2025-06-09 19:57:34,671:INFO:Preloading libraries
2025-06-09 19:57:34,671:INFO:Set up data.
2025-06-09 19:57:34,680:INFO:Set up index.
2025-06-09 20:42:25,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:42:25,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:42:25,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:42:25,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:42:27,945:INFO:PyCaret ClassificationExperiment
2025-06-09 20:42:27,945:INFO:Logging name: clf-default-name
2025-06-09 20:42:27,945:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-09 20:42:27,946:INFO:version 3.3.2
2025-06-09 20:42:27,946:INFO:Initializing setup()
2025-06-09 20:42:27,946:INFO:self.USI: 10ab
2025-06-09 20:42:27,946:INFO:self._variable_keys: {'X_test', 'y_test', 'fold_generator', 'fold_shuffle_param', 'USI', 'pipeline', 'exp_name_log', '_available_plots', 'is_multiclass', 'logging_param', 'exp_id', 'gpu_n_jobs_param', 'log_plots_param', 'html_param', 'target_param', 'X', 'gpu_param', 'X_train', 'memory', 'idx', 'fix_imbalance', 'y', 'data', 'fold_groups_param', '_ml_usecase', 'n_jobs_param', 'seed', 'y_train'}
2025-06-09 20:42:27,946:INFO:Checking environment
2025-06-09 20:42:27,946:INFO:python_version: 3.11.9
2025-06-09 20:42:27,946:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2025-06-09 20:42:27,946:INFO:machine: AMD64
2025-06-09 20:42:27,962:INFO:platform: Windows-10-10.0.19045-SP0
2025-06-09 20:42:27,967:INFO:Memory: svmem(total=8506490880, available=1889480704, percent=77.8, used=6617010176, free=1889480704)
2025-06-09 20:42:27,967:INFO:Physical Core: 2
2025-06-09 20:42:27,968:INFO:Logical Core: 4
2025-06-09 20:42:27,968:INFO:Checking libraries
2025-06-09 20:42:27,968:INFO:System:
2025-06-09 20:42:27,968:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2025-06-09 20:42:27,968:INFO:executable: C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Scripts\python.exe
2025-06-09 20:42:27,968:INFO:   machine: Windows-10-10.0.19045-SP0
2025-06-09 20:42:27,968:INFO:PyCaret required dependencies:
2025-06-09 20:42:28,017:INFO:                 pip: 24.0
2025-06-09 20:42:28,018:INFO:          setuptools: 65.5.0
2025-06-09 20:42:28,018:INFO:             pycaret: 3.3.2
2025-06-09 20:42:28,018:INFO:             IPython: 9.3.0
2025-06-09 20:42:28,018:INFO:          ipywidgets: 8.1.7
2025-06-09 20:42:28,018:INFO:                tqdm: 4.67.1
2025-06-09 20:42:28,018:INFO:               numpy: 1.26.4
2025-06-09 20:42:28,018:INFO:              pandas: 2.1.4
2025-06-09 20:42:28,018:INFO:              jinja2: 3.1.6
2025-06-09 20:42:28,018:INFO:               scipy: 1.11.4
2025-06-09 20:42:28,018:INFO:              joblib: 1.3.2
2025-06-09 20:42:28,018:INFO:             sklearn: 1.4.2
2025-06-09 20:42:28,018:INFO:                pyod: 2.0.5
2025-06-09 20:42:28,018:INFO:            imblearn: 0.13.0
2025-06-09 20:42:28,019:INFO:   category_encoders: 2.7.0
2025-06-09 20:42:28,019:INFO:            lightgbm: 4.6.0
2025-06-09 20:42:28,019:INFO:               numba: 0.61.2
2025-06-09 20:42:28,019:INFO:            requests: 2.32.3
2025-06-09 20:42:28,019:INFO:          matplotlib: 3.7.5
2025-06-09 20:42:28,019:INFO:          scikitplot: 0.3.7
2025-06-09 20:42:28,019:INFO:         yellowbrick: 1.5
2025-06-09 20:42:28,019:INFO:              plotly: 5.24.1
2025-06-09 20:42:28,019:INFO:    plotly-resampler: Not installed
2025-06-09 20:42:28,019:INFO:             kaleido: 0.2.1
2025-06-09 20:42:28,019:INFO:           schemdraw: 0.15
2025-06-09 20:42:28,019:INFO:         statsmodels: 0.14.4
2025-06-09 20:42:28,019:INFO:              sktime: 0.26.0
2025-06-09 20:42:28,019:INFO:               tbats: 1.1.3
2025-06-09 20:42:28,019:INFO:            pmdarima: 2.0.4
2025-06-09 20:42:28,020:INFO:              psutil: 7.0.0
2025-06-09 20:42:28,020:INFO:          markupsafe: 3.0.2
2025-06-09 20:42:28,020:INFO:             pickle5: Not installed
2025-06-09 20:42:28,020:INFO:         cloudpickle: 3.1.1
2025-06-09 20:42:28,020:INFO:         deprecation: 2.1.0
2025-06-09 20:42:28,020:INFO:              xxhash: 3.5.0
2025-06-09 20:42:28,020:INFO:           wurlitzer: Not installed
2025-06-09 20:42:28,020:INFO:PyCaret optional dependencies:
2025-06-09 20:42:28,050:INFO:                shap: Not installed
2025-06-09 20:42:28,050:INFO:           interpret: Not installed
2025-06-09 20:42:28,050:INFO:                umap: Not installed
2025-06-09 20:42:28,050:INFO:     ydata_profiling: Not installed
2025-06-09 20:42:28,050:INFO:  explainerdashboard: Not installed
2025-06-09 20:42:28,050:INFO:             autoviz: Not installed
2025-06-09 20:42:28,050:INFO:           fairlearn: Not installed
2025-06-09 20:42:28,050:INFO:          deepchecks: Not installed
2025-06-09 20:42:28,051:INFO:             xgboost: Not installed
2025-06-09 20:42:28,051:INFO:            catboost: Not installed
2025-06-09 20:42:28,051:INFO:              kmodes: Not installed
2025-06-09 20:42:28,051:INFO:             mlxtend: Not installed
2025-06-09 20:42:28,051:INFO:       statsforecast: Not installed
2025-06-09 20:42:28,051:INFO:        tune_sklearn: Not installed
2025-06-09 20:42:28,051:INFO:                 ray: Not installed
2025-06-09 20:42:28,051:INFO:            hyperopt: Not installed
2025-06-09 20:42:28,051:INFO:              optuna: Not installed
2025-06-09 20:42:28,051:INFO:               skopt: Not installed
2025-06-09 20:42:28,051:INFO:              mlflow: Not installed
2025-06-09 20:42:28,051:INFO:              gradio: Not installed
2025-06-09 20:42:28,051:INFO:             fastapi: Not installed
2025-06-09 20:42:28,051:INFO:             uvicorn: Not installed
2025-06-09 20:42:28,051:INFO:              m2cgen: Not installed
2025-06-09 20:42:28,052:INFO:           evidently: Not installed
2025-06-09 20:42:28,052:INFO:               fugue: Not installed
2025-06-09 20:42:28,052:INFO:           streamlit: 1.45.1
2025-06-09 20:42:28,052:INFO:             prophet: Not installed
2025-06-09 20:42:28,052:INFO:None
2025-06-09 20:42:28,052:INFO:Set up data.
2025-06-09 20:42:28,063:INFO:Set up folding strategy.
2025-06-09 20:42:28,063:INFO:Set up train/test split.
2025-06-09 20:42:28,077:INFO:Set up index.
2025-06-09 20:42:28,083:INFO:Assigning column types.
2025-06-09 20:42:28,090:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2025-06-09 20:42:28,282:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-09 20:42:28,322:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-09 20:42:28,470:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,470:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,558:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2025-06-09 20:42:28,560:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-09 20:42:28,621:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,621:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,622:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2025-06-09 20:42:28,696:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-09 20:42:28,768:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,769:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,856:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2025-06-09 20:42:28,905:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,905:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:28,906:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2025-06-09 20:42:29,127:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,128:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,255:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,256:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,261:INFO:Preparing preprocessing pipeline...
2025-06-09 20:42:29,268:INFO:Set up simple imputation.
2025-06-09 20:42:29,269:INFO:Set up column name cleaning.
2025-06-09 20:42:29,306:INFO:Finished creating preprocessing pipeline.
2025-06-09 20:42:29,313:INFO:Pipeline: Pipeline(memory=FastMemory(location=C:\Users\kawav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False)
2025-06-09 20:42:29,314:INFO:Creating final display dataframe.
2025-06-09 20:42:29,425:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target            target
2                   Target type        Multiclass
3           Original data shape          (150, 5)
4        Transformed data shape          (150, 5)
5   Transformed train set shape          (105, 5)
6    Transformed test set shape           (45, 5)
7              Numeric features                 4
8                    Preprocess              True
9               Imputation type            simple
10           Numeric imputation              mean
11       Categorical imputation              mode
12               Fold Generator   StratifiedKFold
13                  Fold Number                10
14                     CPU Jobs                -1
15                      Use GPU             False
16               Log Experiment             False
17              Experiment Name  clf-default-name
18                          USI              10ab
2025-06-09 20:42:29,533:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,534:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,642:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,643:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2025-06-09 20:42:29,644:INFO:setup() successfully completed in 1.71s...............
2025-06-09 20:42:29,645:INFO:Initializing compare_models()
2025-06-09 20:42:29,645:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, include=None, exclude=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>})
2025-06-09 20:42:29,645:INFO:Checking exceptions
2025-06-09 20:42:29,653:INFO:Preparing display monitor
2025-06-09 20:42:29,659:INFO:Initializing Logistic Regression
2025-06-09 20:42:29,660:INFO:Total runtime is 1.7122427622477213e-05 minutes
2025-06-09 20:42:29,660:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:29,660:INFO:Initializing create_model()
2025-06-09 20:42:29,661:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:29,661:INFO:Checking exceptions
2025-06-09 20:42:29,661:INFO:Importing libraries
2025-06-09 20:42:29,661:INFO:Copying training dataset
2025-06-09 20:42:29,669:INFO:Defining folds
2025-06-09 20:42:29,669:INFO:Declaring metric variables
2025-06-09 20:42:29,670:INFO:Importing untrained model
2025-06-09 20:42:29,670:INFO:Logistic Regression Imported successfully
2025-06-09 20:42:29,670:INFO:Starting cross validation
2025-06-09 20:42:29,671:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:41,620:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:41,635:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:41,905:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:41,918:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:41,918:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:41,932:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:42,027:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:42,033:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:42,060:INFO:Calculating mean and std
2025-06-09 20:42:42,071:INFO:Creating metrics dataframe
2025-06-09 20:42:42,074:INFO:Uploading results into container
2025-06-09 20:42:42,075:INFO:Uploading model into container now
2025-06-09 20:42:42,076:INFO:_master_model_container: 1
2025-06-09 20:42:42,076:INFO:_display_container: 2
2025-06-09 20:42:42,076:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-09 20:42:42,076:INFO:create_model() successfully completed......................................
2025-06-09 20:42:42,213:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:42,213:INFO:Creating metrics dataframe
2025-06-09 20:42:42,219:INFO:Initializing K Neighbors Classifier
2025-06-09 20:42:42,219:INFO:Total runtime is 0.2093225121498108 minutes
2025-06-09 20:42:42,219:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:42,220:INFO:Initializing create_model()
2025-06-09 20:42:42,220:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:42,220:INFO:Checking exceptions
2025-06-09 20:42:42,220:INFO:Importing libraries
2025-06-09 20:42:42,220:INFO:Copying training dataset
2025-06-09 20:42:42,225:INFO:Defining folds
2025-06-09 20:42:42,225:INFO:Declaring metric variables
2025-06-09 20:42:42,226:INFO:Importing untrained model
2025-06-09 20:42:42,226:INFO:K Neighbors Classifier Imported successfully
2025-06-09 20:42:42,227:INFO:Starting cross validation
2025-06-09 20:42:42,228:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:42,646:INFO:Calculating mean and std
2025-06-09 20:42:42,648:INFO:Creating metrics dataframe
2025-06-09 20:42:42,651:INFO:Uploading results into container
2025-06-09 20:42:42,652:INFO:Uploading model into container now
2025-06-09 20:42:42,652:INFO:_master_model_container: 2
2025-06-09 20:42:42,653:INFO:_display_container: 2
2025-06-09 20:42:42,653:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2025-06-09 20:42:42,653:INFO:create_model() successfully completed......................................
2025-06-09 20:42:42,756:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:42,756:INFO:Creating metrics dataframe
2025-06-09 20:42:42,762:INFO:Initializing Naive Bayes
2025-06-09 20:42:42,763:INFO:Total runtime is 0.2184010108311971 minutes
2025-06-09 20:42:42,764:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:42,764:INFO:Initializing create_model()
2025-06-09 20:42:42,764:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:42,764:INFO:Checking exceptions
2025-06-09 20:42:42,764:INFO:Importing libraries
2025-06-09 20:42:42,764:INFO:Copying training dataset
2025-06-09 20:42:42,769:INFO:Defining folds
2025-06-09 20:42:42,770:INFO:Declaring metric variables
2025-06-09 20:42:42,770:INFO:Importing untrained model
2025-06-09 20:42:42,770:INFO:Naive Bayes Imported successfully
2025-06-09 20:42:42,771:INFO:Starting cross validation
2025-06-09 20:42:42,772:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:43,027:INFO:Calculating mean and std
2025-06-09 20:42:43,029:INFO:Creating metrics dataframe
2025-06-09 20:42:43,033:INFO:Uploading results into container
2025-06-09 20:42:43,034:INFO:Uploading model into container now
2025-06-09 20:42:43,034:INFO:_master_model_container: 3
2025-06-09 20:42:43,035:INFO:_display_container: 2
2025-06-09 20:42:43,035:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2025-06-09 20:42:43,035:INFO:create_model() successfully completed......................................
2025-06-09 20:42:43,141:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:43,142:INFO:Creating metrics dataframe
2025-06-09 20:42:43,146:INFO:Initializing Decision Tree Classifier
2025-06-09 20:42:43,146:INFO:Total runtime is 0.2247779369354248 minutes
2025-06-09 20:42:43,146:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:43,147:INFO:Initializing create_model()
2025-06-09 20:42:43,147:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:43,148:INFO:Checking exceptions
2025-06-09 20:42:43,148:INFO:Importing libraries
2025-06-09 20:42:43,148:INFO:Copying training dataset
2025-06-09 20:42:43,153:INFO:Defining folds
2025-06-09 20:42:43,153:INFO:Declaring metric variables
2025-06-09 20:42:43,154:INFO:Importing untrained model
2025-06-09 20:42:43,154:INFO:Decision Tree Classifier Imported successfully
2025-06-09 20:42:43,155:INFO:Starting cross validation
2025-06-09 20:42:43,156:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:43,403:INFO:Calculating mean and std
2025-06-09 20:42:43,404:INFO:Creating metrics dataframe
2025-06-09 20:42:43,407:INFO:Uploading results into container
2025-06-09 20:42:43,408:INFO:Uploading model into container now
2025-06-09 20:42:43,408:INFO:_master_model_container: 4
2025-06-09 20:42:43,409:INFO:_display_container: 2
2025-06-09 20:42:43,409:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2025-06-09 20:42:43,409:INFO:create_model() successfully completed......................................
2025-06-09 20:42:43,511:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:43,511:INFO:Creating metrics dataframe
2025-06-09 20:42:43,517:INFO:Initializing SVM - Linear Kernel
2025-06-09 20:42:43,517:INFO:Total runtime is 0.2309567411740621 minutes
2025-06-09 20:42:43,517:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:43,517:INFO:Initializing create_model()
2025-06-09 20:42:43,518:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:43,518:INFO:Checking exceptions
2025-06-09 20:42:43,518:INFO:Importing libraries
2025-06-09 20:42:43,518:INFO:Copying training dataset
2025-06-09 20:42:43,522:INFO:Defining folds
2025-06-09 20:42:43,523:INFO:Declaring metric variables
2025-06-09 20:42:43,523:INFO:Importing untrained model
2025-06-09 20:42:43,524:INFO:SVM - Linear Kernel Imported successfully
2025-06-09 20:42:43,524:INFO:Starting cross validation
2025-06-09 20:42:43,525:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:43,622:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,624:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,628:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,636:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,663:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:43,666:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:43,668:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:43,739:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,753:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,760:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,764:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,777:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:43,822:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,832:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:43,833:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:43,838:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:43,855:INFO:Calculating mean and std
2025-06-09 20:42:43,856:INFO:Creating metrics dataframe
2025-06-09 20:42:43,859:INFO:Uploading results into container
2025-06-09 20:42:43,860:INFO:Uploading model into container now
2025-06-09 20:42:43,860:INFO:_master_model_container: 5
2025-06-09 20:42:43,860:INFO:_display_container: 2
2025-06-09 20:42:43,861:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2025-06-09 20:42:43,861:INFO:create_model() successfully completed......................................
2025-06-09 20:42:43,953:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:43,953:INFO:Creating metrics dataframe
2025-06-09 20:42:43,956:INFO:Initializing Ridge Classifier
2025-06-09 20:42:43,957:INFO:Total runtime is 0.23829305171966553 minutes
2025-06-09 20:42:43,957:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:43,957:INFO:Initializing create_model()
2025-06-09 20:42:43,957:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:43,957:INFO:Checking exceptions
2025-06-09 20:42:43,957:INFO:Importing libraries
2025-06-09 20:42:43,957:INFO:Copying training dataset
2025-06-09 20:42:43,961:INFO:Defining folds
2025-06-09 20:42:43,962:INFO:Declaring metric variables
2025-06-09 20:42:43,962:INFO:Importing untrained model
2025-06-09 20:42:43,962:INFO:Ridge Classifier Imported successfully
2025-06-09 20:42:43,963:INFO:Starting cross validation
2025-06-09 20:42:43,964:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:44,058:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,059:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,062:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,072:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,129:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,133:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,134:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,146:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,186:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,189:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:44,211:INFO:Calculating mean and std
2025-06-09 20:42:44,212:INFO:Creating metrics dataframe
2025-06-09 20:42:44,216:INFO:Uploading results into container
2025-06-09 20:42:44,216:INFO:Uploading model into container now
2025-06-09 20:42:44,217:INFO:_master_model_container: 6
2025-06-09 20:42:44,217:INFO:_display_container: 2
2025-06-09 20:42:44,217:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2025-06-09 20:42:44,217:INFO:create_model() successfully completed......................................
2025-06-09 20:42:44,313:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:44,313:INFO:Creating metrics dataframe
2025-06-09 20:42:44,317:INFO:Initializing Random Forest Classifier
2025-06-09 20:42:44,317:INFO:Total runtime is 0.24429685672124227 minutes
2025-06-09 20:42:44,318:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:44,318:INFO:Initializing create_model()
2025-06-09 20:42:44,318:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:44,318:INFO:Checking exceptions
2025-06-09 20:42:44,318:INFO:Importing libraries
2025-06-09 20:42:44,318:INFO:Copying training dataset
2025-06-09 20:42:44,323:INFO:Defining folds
2025-06-09 20:42:44,323:INFO:Declaring metric variables
2025-06-09 20:42:44,323:INFO:Importing untrained model
2025-06-09 20:42:44,324:INFO:Random Forest Classifier Imported successfully
2025-06-09 20:42:44,324:INFO:Starting cross validation
2025-06-09 20:42:44,325:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:46,008:INFO:Calculating mean and std
2025-06-09 20:42:46,009:INFO:Creating metrics dataframe
2025-06-09 20:42:46,012:INFO:Uploading results into container
2025-06-09 20:42:46,013:INFO:Uploading model into container now
2025-06-09 20:42:46,013:INFO:_master_model_container: 7
2025-06-09 20:42:46,013:INFO:_display_container: 2
2025-06-09 20:42:46,014:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2025-06-09 20:42:46,014:INFO:create_model() successfully completed......................................
2025-06-09 20:42:46,106:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:46,106:INFO:Creating metrics dataframe
2025-06-09 20:42:46,110:INFO:Initializing Quadratic Discriminant Analysis
2025-06-09 20:42:46,110:INFO:Total runtime is 0.2741764585177104 minutes
2025-06-09 20:42:46,110:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:46,110:INFO:Initializing create_model()
2025-06-09 20:42:46,110:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:46,111:INFO:Checking exceptions
2025-06-09 20:42:46,111:INFO:Importing libraries
2025-06-09 20:42:46,111:INFO:Copying training dataset
2025-06-09 20:42:46,116:INFO:Defining folds
2025-06-09 20:42:46,117:INFO:Declaring metric variables
2025-06-09 20:42:46,117:INFO:Importing untrained model
2025-06-09 20:42:46,117:INFO:Quadratic Discriminant Analysis Imported successfully
2025-06-09 20:42:46,117:INFO:Starting cross validation
2025-06-09 20:42:46,119:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:46,197:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,198:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,264:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,267:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,270:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,278:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,323:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,326:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,349:INFO:Calculating mean and std
2025-06-09 20:42:46,351:INFO:Creating metrics dataframe
2025-06-09 20:42:46,354:INFO:Uploading results into container
2025-06-09 20:42:46,354:INFO:Uploading model into container now
2025-06-09 20:42:46,355:INFO:_master_model_container: 8
2025-06-09 20:42:46,355:INFO:_display_container: 2
2025-06-09 20:42:46,355:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2025-06-09 20:42:46,355:INFO:create_model() successfully completed......................................
2025-06-09 20:42:46,449:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:46,449:INFO:Creating metrics dataframe
2025-06-09 20:42:46,453:INFO:Initializing Ada Boost Classifier
2025-06-09 20:42:46,454:INFO:Total runtime is 0.2799081881841024 minutes
2025-06-09 20:42:46,454:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:46,454:INFO:Initializing create_model()
2025-06-09 20:42:46,454:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:46,454:INFO:Checking exceptions
2025-06-09 20:42:46,454:INFO:Importing libraries
2025-06-09 20:42:46,454:INFO:Copying training dataset
2025-06-09 20:42:46,459:INFO:Defining folds
2025-06-09 20:42:46,459:INFO:Declaring metric variables
2025-06-09 20:42:46,459:INFO:Importing untrained model
2025-06-09 20:42:46,460:INFO:Ada Boost Classifier Imported successfully
2025-06-09 20:42:46,460:INFO:Starting cross validation
2025-06-09 20:42:46,461:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:46,523:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:46,523:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:46,794:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,802:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,804:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,805:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:46,842:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:46,850:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:46,853:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:46,862:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:47,107:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:47,113:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:47,124:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:47,125:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:47,157:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:47,161:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\ensemble\_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2025-06-09 20:42:47,314:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:47,322:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:47,339:INFO:Calculating mean and std
2025-06-09 20:42:47,340:INFO:Creating metrics dataframe
2025-06-09 20:42:47,343:INFO:Uploading results into container
2025-06-09 20:42:47,343:INFO:Uploading model into container now
2025-06-09 20:42:47,344:INFO:_master_model_container: 9
2025-06-09 20:42:47,344:INFO:_display_container: 2
2025-06-09 20:42:47,344:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2025-06-09 20:42:47,345:INFO:create_model() successfully completed......................................
2025-06-09 20:42:47,446:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:47,446:INFO:Creating metrics dataframe
2025-06-09 20:42:47,449:INFO:Initializing Gradient Boosting Classifier
2025-06-09 20:42:47,450:INFO:Total runtime is 0.2965102752049764 minutes
2025-06-09 20:42:47,450:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:47,450:INFO:Initializing create_model()
2025-06-09 20:42:47,451:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:47,451:INFO:Checking exceptions
2025-06-09 20:42:47,452:INFO:Importing libraries
2025-06-09 20:42:47,452:INFO:Copying training dataset
2025-06-09 20:42:47,456:INFO:Defining folds
2025-06-09 20:42:47,456:INFO:Declaring metric variables
2025-06-09 20:42:47,457:INFO:Importing untrained model
2025-06-09 20:42:47,457:INFO:Gradient Boosting Classifier Imported successfully
2025-06-09 20:42:47,457:INFO:Starting cross validation
2025-06-09 20:42:47,458:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:48,290:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:48,304:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:48,333:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:48,340:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:49,141:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:49,142:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:49,175:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:49,176:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:49,693:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:49,713:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:49,739:INFO:Calculating mean and std
2025-06-09 20:42:49,742:INFO:Creating metrics dataframe
2025-06-09 20:42:49,746:INFO:Uploading results into container
2025-06-09 20:42:49,747:INFO:Uploading model into container now
2025-06-09 20:42:49,748:INFO:_master_model_container: 10
2025-06-09 20:42:49,749:INFO:_display_container: 2
2025-06-09 20:42:49,751:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2025-06-09 20:42:49,751:INFO:create_model() successfully completed......................................
2025-06-09 20:42:49,868:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:49,868:INFO:Creating metrics dataframe
2025-06-09 20:42:49,875:INFO:Initializing Linear Discriminant Analysis
2025-06-09 20:42:49,875:INFO:Total runtime is 0.33692188262939454 minutes
2025-06-09 20:42:49,876:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:49,876:INFO:Initializing create_model()
2025-06-09 20:42:49,876:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:49,876:INFO:Checking exceptions
2025-06-09 20:42:49,876:INFO:Importing libraries
2025-06-09 20:42:49,876:INFO:Copying training dataset
2025-06-09 20:42:49,881:INFO:Defining folds
2025-06-09 20:42:49,881:INFO:Declaring metric variables
2025-06-09 20:42:49,881:INFO:Importing untrained model
2025-06-09 20:42:49,882:INFO:Linear Discriminant Analysis Imported successfully
2025-06-09 20:42:49,882:INFO:Starting cross validation
2025-06-09 20:42:49,883:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:49,983:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,003:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,009:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,013:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,162:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,193:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,195:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,197:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,268:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,296:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 196, in _score
    return super()._score(
           ^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
                        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\utils\_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
           ^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
           ^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2025-06-09 20:42:50,323:INFO:Calculating mean and std
2025-06-09 20:42:50,324:INFO:Creating metrics dataframe
2025-06-09 20:42:50,327:INFO:Uploading results into container
2025-06-09 20:42:50,327:INFO:Uploading model into container now
2025-06-09 20:42:50,328:INFO:_master_model_container: 11
2025-06-09 20:42:50,328:INFO:_display_container: 2
2025-06-09 20:42:50,329:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2025-06-09 20:42:50,329:INFO:create_model() successfully completed......................................
2025-06-09 20:42:50,473:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:50,474:INFO:Creating metrics dataframe
2025-06-09 20:42:50,478:INFO:Initializing Extra Trees Classifier
2025-06-09 20:42:50,478:INFO:Total runtime is 0.34698111613591515 minutes
2025-06-09 20:42:50,479:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:50,479:INFO:Initializing create_model()
2025-06-09 20:42:50,479:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:50,479:INFO:Checking exceptions
2025-06-09 20:42:50,479:INFO:Importing libraries
2025-06-09 20:42:50,479:INFO:Copying training dataset
2025-06-09 20:42:50,484:INFO:Defining folds
2025-06-09 20:42:50,484:INFO:Declaring metric variables
2025-06-09 20:42:50,485:INFO:Importing untrained model
2025-06-09 20:42:50,485:INFO:Extra Trees Classifier Imported successfully
2025-06-09 20:42:50,486:INFO:Starting cross validation
2025-06-09 20:42:50,488:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:52,570:INFO:Calculating mean and std
2025-06-09 20:42:52,572:INFO:Creating metrics dataframe
2025-06-09 20:42:52,575:INFO:Uploading results into container
2025-06-09 20:42:52,576:INFO:Uploading model into container now
2025-06-09 20:42:52,576:INFO:_master_model_container: 12
2025-06-09 20:42:52,577:INFO:_display_container: 2
2025-06-09 20:42:52,577:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2025-06-09 20:42:52,577:INFO:create_model() successfully completed......................................
2025-06-09 20:42:52,699:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:52,699:INFO:Creating metrics dataframe
2025-06-09 20:42:52,703:INFO:Initializing Light Gradient Boosting Machine
2025-06-09 20:42:52,704:INFO:Total runtime is 0.3840697805086772 minutes
2025-06-09 20:42:52,704:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:52,704:INFO:Initializing create_model()
2025-06-09 20:42:52,704:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:52,704:INFO:Checking exceptions
2025-06-09 20:42:52,705:INFO:Importing libraries
2025-06-09 20:42:52,705:INFO:Copying training dataset
2025-06-09 20:42:52,711:INFO:Defining folds
2025-06-09 20:42:52,711:INFO:Declaring metric variables
2025-06-09 20:42:52,712:INFO:Importing untrained model
2025-06-09 20:42:52,713:INFO:Light Gradient Boosting Machine Imported successfully
2025-06-09 20:42:52,713:INFO:Starting cross validation
2025-06-09 20:42:52,714:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:58,102:INFO:Calculating mean and std
2025-06-09 20:42:58,105:INFO:Creating metrics dataframe
2025-06-09 20:42:58,110:INFO:Uploading results into container
2025-06-09 20:42:58,113:INFO:Uploading model into container now
2025-06-09 20:42:58,114:INFO:_master_model_container: 13
2025-06-09 20:42:58,115:INFO:_display_container: 2
2025-06-09 20:42:58,117:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2025-06-09 20:42:58,117:INFO:create_model() successfully completed......................................
2025-06-09 20:42:58,226:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:58,226:INFO:Creating metrics dataframe
2025-06-09 20:42:58,232:INFO:Initializing Dummy Classifier
2025-06-09 20:42:58,232:INFO:Total runtime is 0.47620725631713867 minutes
2025-06-09 20:42:58,232:INFO:SubProcess create_model() called ==================================
2025-06-09 20:42:58,233:INFO:Initializing create_model()
2025-06-09 20:42:58,233:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x000001D43C963910>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:58,233:INFO:Checking exceptions
2025-06-09 20:42:58,233:INFO:Importing libraries
2025-06-09 20:42:58,233:INFO:Copying training dataset
2025-06-09 20:42:58,238:INFO:Defining folds
2025-06-09 20:42:58,238:INFO:Declaring metric variables
2025-06-09 20:42:58,239:INFO:Importing untrained model
2025-06-09 20:42:58,239:INFO:Dummy Classifier Imported successfully
2025-06-09 20:42:58,239:INFO:Starting cross validation
2025-06-09 20:42:58,240:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2025-06-09 20:42:58,339:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,342:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,485:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,582:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,741:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,773:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,806:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,844:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,915:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,920:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\sklearn\metrics\_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2025-06-09 20:42:58,942:INFO:Calculating mean and std
2025-06-09 20:42:58,943:INFO:Creating metrics dataframe
2025-06-09 20:42:58,949:INFO:Uploading results into container
2025-06-09 20:42:58,949:INFO:Uploading model into container now
2025-06-09 20:42:58,950:INFO:_master_model_container: 14
2025-06-09 20:42:58,950:INFO:_display_container: 2
2025-06-09 20:42:58,950:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2025-06-09 20:42:58,950:INFO:create_model() successfully completed......................................
2025-06-09 20:42:59,068:INFO:SubProcess create_model() end ==================================
2025-06-09 20:42:59,068:INFO:Creating metrics dataframe
2025-06-09 20:42:59,089:WARNING:C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Lib\site-packages\pycaret\internal\pycaret_experiment\supervised_experiment.py:339: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  .applymap(highlight_cols, subset=["TT (Sec)"])

2025-06-09 20:42:59,093:INFO:Initializing create_model()
2025-06-09 20:42:59,093:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:59,093:INFO:Checking exceptions
2025-06-09 20:42:59,096:INFO:Importing libraries
2025-06-09 20:42:59,096:INFO:Copying training dataset
2025-06-09 20:42:59,104:INFO:Defining folds
2025-06-09 20:42:59,104:INFO:Declaring metric variables
2025-06-09 20:42:59,104:INFO:Importing untrained model
2025-06-09 20:42:59,104:INFO:Declaring custom model
2025-06-09 20:42:59,104:INFO:Logistic Regression Imported successfully
2025-06-09 20:42:59,104:INFO:Cross validation set to False
2025-06-09 20:42:59,114:INFO:Fitting Model
2025-06-09 20:42:59,240:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-09 20:42:59,240:INFO:create_model() successfully completed......................................
2025-06-09 20:42:59,410:INFO:_master_model_container: 14
2025-06-09 20:42:59,410:INFO:_display_container: 2
2025-06-09 20:42:59,412:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-09 20:42:59,412:INFO:compare_models() successfully completed......................................
2025-06-09 20:42:59,468:INFO:Initializing finalize_model()
2025-06-09 20:42:59,469:INFO:finalize_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fit_kwargs=None, groups=None, model_only=False, experiment_custom_tags=None)
2025-06-09 20:42:59,470:INFO:Finalizing LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2025-06-09 20:42:59,473:INFO:Initializing create_model()
2025-06-09 20:42:59,473:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001D425F582D0>, estimator=LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False), fold=None, round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=False, metrics=None, display=None, model_only=False, return_train_score=False, error_score=0.0, kwargs={})
2025-06-09 20:42:59,473:INFO:Checking exceptions
2025-06-09 20:42:59,473:INFO:Importing libraries
2025-06-09 20:42:59,473:INFO:Copying training dataset
2025-06-09 20:42:59,473:INFO:Defining folds
2025-06-09 20:42:59,473:INFO:Declaring metric variables
2025-06-09 20:42:59,473:INFO:Importing untrained model
2025-06-09 20:42:59,484:INFO:Declaring custom model
2025-06-09 20:42:59,485:INFO:Logistic Regression Imported successfully
2025-06-09 20:42:59,485:INFO:Cross validation set to False
2025-06-09 20:42:59,485:INFO:Fitting Model
2025-06-09 20:42:59,543:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-06-09 20:42:59,544:INFO:create_model() successfully completed......................................
2025-06-09 20:42:59,644:INFO:_master_model_container: 14
2025-06-09 20:42:59,644:INFO:_display_container: 2
2025-06-09 20:42:59,652:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-06-09 20:42:59,652:INFO:finalize_model() successfully completed......................................
2025-06-09 20:42:59,761:INFO:Initializing save_model()
2025-06-09 20:42:59,761:INFO:save_model(model=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False), model_name=model, prep_pipe_=Pipeline(memory=FastMemory(location=C:\Users\kawav\AppData\Local\Temp\joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'...
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2025-06-09 20:42:59,761:INFO:Adding model into prep_pipe
2025-06-09 20:42:59,761:WARNING:Only Model saved as it was a pipeline.
2025-06-09 20:42:59,766:INFO:model.pkl saved in current working directory
2025-06-09 20:42:59,773:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWr...
                 TransformerWrapper(exclude=None, include=None,
                                    transformer=CleanColumnNames(match='[\\]\\[\\,\\{\\}\\"\\:]+'))),
                ('actual_estimator',
                 LogisticRegression(C=1.0, class_weight=None, dual=False,
                                    fit_intercept=True, intercept_scaling=1,
                                    l1_ratio=None, max_iter=1000,
                                    multi_class='auto', n_jobs=None,
                                    penalty='l2', random_state=123,
                                    solver='lbfgs', tol=0.0001, verbose=0,
                                    warm_start=False))],
         verbose=False)
2025-06-09 20:42:59,773:INFO:save_model() successfully completed......................................
2025-06-09 20:46:20,147:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:46:20,147:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:46:20,148:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:46:20,148:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:46:22,134:INFO:Initializing load_model()
2025-06-09 20:46:22,134:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:46:36,196:INFO:Initializing load_model()
2025-06-09 20:46:36,196:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:46:52,721:INFO:Initializing load_model()
2025-06-09 20:46:52,722:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:49:40,919:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:49:40,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:49:40,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:49:40,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:49:55,407:INFO:Initializing load_model()
2025-06-09 20:49:55,407:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:50:03,159:INFO:Initializing load_model()
2025-06-09 20:50:03,160:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:50:06,546:INFO:Initializing load_model()
2025-06-09 20:50:06,546:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:50:08,642:INFO:Initializing load_model()
2025-06-09 20:50:08,642:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:50:49,003:INFO:Initializing load_model()
2025-06-09 20:50:49,003:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:51:40,688:INFO:Initializing load_model()
2025-06-09 20:51:40,689:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:53:09,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:09,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:09,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:09,751:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:12,572:INFO:Initializing load_model()
2025-06-09 20:53:12,578:INFO:load_model(model_name=C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\model, platform=None, authentication=None, verbose=True)
2025-06-09 20:53:42,423:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:42,424:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:42,424:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:42,424:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:53:44,865:INFO:Initializing load_model()
2025-06-09 20:53:44,865:INFO:load_model(model_name=C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\model, platform=None, authentication=None, verbose=True)
2025-06-09 20:54:10,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:54:10,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:54:10,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:54:10,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:54:12,068:INFO:Initializing load_model()
2025-06-09 20:54:12,069:INFO:load_model(model_name=C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\model, platform=None, authentication=None, verbose=True)
2025-06-09 20:55:06,992:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:55:06,992:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:55:06,992:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:55:06,992:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 20:55:10,333:INFO:Initializing load_model()
2025-06-09 20:55:10,333:INFO:load_model(model_name=model, platform=None, authentication=None, verbose=True)
2025-06-09 20:55:17,647:INFO:Initializing predict_model()
2025-06-09 20:55:17,648:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x000001BD398F3F90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['sepal length (cm)',
                                             'sepal width (cm)',
                                             'petal length (cm)',
                                             'petal width (cm)'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('clean_column_names',
                 TransformerWrapper(transformer=CleanColumnNames())),
                ('actual_estimator',
                 LogisticRegression(max_iter=1000, random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x000001BD39C36DE0>)
2025-06-09 20:55:17,648:INFO:Checking exceptions
2025-06-09 20:55:17,648:INFO:Preloading libraries
2025-06-09 20:55:17,648:INFO:Set up data.
2025-06-09 20:55:17,656:INFO:Set up index.
2025-06-09 21:00:46,591:INFO:PyCaret ClassificationExperiment
2025-06-09 21:00:46,592:INFO:Logging name: clf-default-name
2025-06-09 21:00:46,592:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2025-06-09 21:00:46,592:INFO:version 3.3.2
2025-06-09 21:00:46,592:INFO:Initializing setup()
2025-06-09 21:00:46,592:INFO:self.USI: b573
2025-06-09 21:00:46,592:INFO:self._variable_keys: {'X_train', 'target_param', 'fold_groups_param', 'logging_param', 'fold_shuffle_param', 'pipeline', 'X_test', 'html_param', 'fold_generator', 'idx', 'data', 'y_train', 'gpu_n_jobs_param', 'y', '_available_plots', 'gpu_param', 'is_multiclass', 'X', 'USI', 'log_plots_param', 'fix_imbalance', '_ml_usecase', 'n_jobs_param', 'y_test', 'exp_name_log', 'exp_id', 'memory', 'seed'}
2025-06-09 21:00:46,592:INFO:Checking environment
2025-06-09 21:00:46,592:INFO:python_version: 3.11.9
2025-06-09 21:00:46,592:INFO:python_build: ('tags/v3.11.9:de54cf5', 'Apr  2 2024 10:12:12')
2025-06-09 21:00:46,592:INFO:machine: AMD64
2025-06-09 21:00:46,657:INFO:platform: Windows-10-10.0.19045-SP0
2025-06-09 21:00:46,663:INFO:Memory: svmem(total=8506490880, available=1639583744, percent=80.7, used=6866907136, free=1639583744)
2025-06-09 21:00:46,664:INFO:Physical Core: 2
2025-06-09 21:00:46,664:INFO:Logical Core: 4
2025-06-09 21:00:46,664:INFO:Checking libraries
2025-06-09 21:00:46,664:INFO:System:
2025-06-09 21:00:46,664:INFO:    python: 3.11.9 (tags/v3.11.9:de54cf5, Apr  2 2024, 10:12:12) [MSC v.1938 64 bit (AMD64)]
2025-06-09 21:00:46,664:INFO:executable: C:\Users\kawav\OneDrive\rea de Trabalho\dk-main\dk-main\ml_app\.venv\Scripts\python.exe
2025-06-09 21:00:46,664:INFO:   machine: Windows-10-10.0.19045-SP0
2025-06-09 21:00:46,664:INFO:PyCaret required dependencies:
2025-06-09 21:00:46,665:INFO:                 pip: 24.0
2025-06-09 21:00:46,665:INFO:          setuptools: 65.5.0
2025-06-09 21:00:46,665:INFO:             pycaret: 3.3.2
2025-06-09 21:00:46,665:INFO:             IPython: 9.3.0
2025-06-09 21:00:46,665:INFO:          ipywidgets: 8.1.7
2025-06-09 21:00:46,665:INFO:                tqdm: 4.67.1
2025-06-09 21:00:46,665:INFO:               numpy: 1.26.4
2025-06-09 21:00:46,665:INFO:              pandas: 2.1.4
2025-06-09 21:00:46,665:INFO:              jinja2: 3.1.6
2025-06-09 21:00:46,665:INFO:               scipy: 1.11.4
2025-06-09 21:00:46,665:INFO:              joblib: 1.3.2
2025-06-09 21:00:46,665:INFO:             sklearn: 1.4.2
2025-06-09 21:00:46,665:INFO:                pyod: 2.0.5
2025-06-09 21:00:46,666:INFO:            imblearn: 0.13.0
2025-06-09 21:00:46,666:INFO:   category_encoders: 2.7.0
2025-06-09 21:00:46,666:INFO:            lightgbm: 4.6.0
2025-06-09 21:00:46,666:INFO:               numba: 0.61.2
2025-06-09 21:00:46,666:INFO:            requests: 2.32.3
2025-06-09 21:00:46,666:INFO:          matplotlib: 3.7.5
2025-06-09 21:00:46,666:INFO:          scikitplot: 0.3.7
2025-06-09 21:00:46,666:INFO:         yellowbrick: 1.5
2025-06-09 21:00:46,666:INFO:              plotly: 5.24.1
2025-06-09 21:00:46,666:INFO:    plotly-resampler: Not installed
2025-06-09 21:00:46,666:INFO:             kaleido: 0.2.1
2025-06-09 21:00:46,666:INFO:           schemdraw: 0.15
2025-06-09 21:00:46,667:INFO:         statsmodels: 0.14.4
2025-06-09 21:00:46,667:INFO:              sktime: 0.26.0
2025-06-09 21:00:46,667:INFO:               tbats: 1.1.3
2025-06-09 21:00:46,667:INFO:            pmdarima: 2.0.4
2025-06-09 21:00:46,667:INFO:              psutil: 7.0.0
2025-06-09 21:00:46,667:INFO:          markupsafe: 3.0.2
2025-06-09 21:00:46,667:INFO:             pickle5: Not installed
2025-06-09 21:00:46,668:INFO:         cloudpickle: 3.1.1
2025-06-09 21:00:46,668:INFO:         deprecation: 2.1.0
2025-06-09 21:00:46,668:INFO:              xxhash: 3.5.0
2025-06-09 21:00:46,668:INFO:           wurlitzer: Not installed
2025-06-09 21:00:46,668:INFO:PyCaret optional dependencies:
2025-06-09 21:00:46,710:INFO:                shap: Not installed
2025-06-09 21:00:46,710:INFO:           interpret: Not installed
2025-06-09 21:00:46,710:INFO:                umap: Not installed
2025-06-09 21:00:46,710:INFO:     ydata_profiling: Not installed
2025-06-09 21:00:46,710:INFO:  explainerdashboard: Not installed
2025-06-09 21:00:46,710:INFO:             autoviz: Not installed
2025-06-09 21:00:46,711:INFO:           fairlearn: Not installed
2025-06-09 21:00:46,711:INFO:          deepchecks: Not installed
2025-06-09 21:00:46,711:INFO:             xgboost: Not installed
2025-06-09 21:00:46,711:INFO:            catboost: Not installed
2025-06-09 21:00:46,711:INFO:              kmodes: Not installed
2025-06-09 21:00:46,711:INFO:             mlxtend: Not installed
2025-06-09 21:00:46,711:INFO:       statsforecast: Not installed
2025-06-09 21:00:46,711:INFO:        tune_sklearn: Not installed
2025-06-09 21:00:46,711:INFO:                 ray: Not installed
2025-06-09 21:00:46,711:INFO:            hyperopt: Not installed
2025-06-09 21:00:46,711:INFO:              optuna: Not installed
2025-06-09 21:00:46,711:INFO:               skopt: Not installed
2025-06-09 21:00:46,711:INFO:              mlflow: Not installed
2025-06-09 21:00:46,712:INFO:              gradio: Not installed
2025-06-09 21:00:46,712:INFO:             fastapi: Not installed
2025-06-09 21:00:46,712:INFO:             uvicorn: Not installed
2025-06-09 21:00:46,712:INFO:              m2cgen: Not installed
2025-06-09 21:00:46,712:INFO:           evidently: Not installed
2025-06-09 21:00:46,712:INFO:               fugue: Not installed
2025-06-09 21:00:46,712:INFO:           streamlit: 1.45.1
2025-06-09 21:00:46,712:INFO:             prophet: Not installed
2025-06-09 21:00:46,712:INFO:None
2025-06-09 21:00:46,712:INFO:Set up data.
2025-06-09 21:00:46,744:INFO:Set up folding strategy.
2025-06-09 21:00:46,744:INFO:Set up train/test split.
2025-06-09 21:12:48,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:12:48,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:12:48,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:12:48,334:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:42,746:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:42,746:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:42,746:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:42,746:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:44,366:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:44,366:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:44,366:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:18:44,366:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:20:31,222:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:20:31,222:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:20:31,222:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2025-06-09 21:20:31,223:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
